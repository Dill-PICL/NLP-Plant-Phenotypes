{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "\n",
    "- [Links of Interest](#links)\n",
    "\n",
    "- [Part 1. Loading and Filtering the Data](#part_1)\n",
    "    - [Reading in arguments](#args)\n",
    "    - [Setting input and output paths](#paths)\n",
    "    - [Reading in genes, annotations, and phenotype descriptions](#read_text_data)\n",
    "    - [Relating genes in this dataset to other biological datasets](#relating)\n",
    "    - [Reading in KEGG data](#kegg)\n",
    "    - [Reading in PlantCyc data](#plantcyc)\n",
    "    - [Reading in Lloyd and Meinke (2012) phenotype groupings](#subsets_and_classes)\n",
    "    - [Relating genes in this dataset to protein-protein associations](#edges)\n",
    "    - [Reading in Oellrich, Walls et al., (2015) EQ statements](#eqs)\n",
    "    - [Reading in protein associations from STRING](#string)\n",
    "    - [Reading in ortholog relationships from PANTHER](#panther)\n",
    "    - [Filtering the dataset to include relevant genes](#filtering)\n",
    "    - [Reading in dataset of paired phenotype descriptions](#phenotype_pairs)\n",
    "    - [Reading in the BIOSSES dataset](#biosses)\n",
    "    - [Selecting a dataset to use for the rest of the analysis](#selecting_a_dataset)\n",
    "    \n",
    "- [Part 2. Models](#part_2)\n",
    "    - [Word2Vec and Doc2Vec](#word2vec_doc2vec)\n",
    "    - [BERT and BioBERT](#bert_biobert)\n",
    "    - [Loading models](#load_models)\n",
    "\n",
    "- [Part 3. NLP Choices](#part_3)\n",
    "    - [Preprocessing descriptions](#preprocessing)\n",
    "    - [POS Tagging](#pos_tagging)\n",
    "    - [Reducing vocabulary size](#vocab)\n",
    "    - [Annotating with biological ontologies](#annotation)\n",
    "    - [Splitting into phene descriptions](#phenes)\n",
    "        \n",
    "- [Part 4. Generating Vectors and Distance Matrices](#part_4)\n",
    "    - [Defining methods to use](#methods)\n",
    "    - [Running all methods](#running)\n",
    "    - [Merging distances into an edgelist](#merging)\n",
    "      \n",
    "- [Part 5. Biological Questions](#part_5)\n",
    "    - [Using pathways as the objective](#pathway_objective)\n",
    "    - [Using phenotype subsets as the objective](#subset_objective)\n",
    "    - [Using protein associations as the objective](#association_objective)\n",
    "    - [Using orthology as the objective](#ortholog_objective)\n",
    "    - [Adding EQ similarity values](#eq_sim)\n",
    "    - [Noting whether gene pairs have curated data](#curated)\n",
    "    - [Noting whether gene pairs refer to the same species](#species)\n",
    "    - [Determining the number of genes and pairs involved in each question](#n_values)\n",
    "    - [Determining how similar the biological questions are to one another](#objective_similarities)\n",
    "    \n",
    "- [Part 6. Results](#part_6)\n",
    "    - [Distributions of distance values](#ks)\n",
    "    - [Within-group distance values](#within)\n",
    "    - [Predictions and AUC for shared pathways or interactions](#auc)\n",
    "    - [Tests for querying to recover related genes](#y)\n",
    "    - [Producing output summary table](#output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## Introduction: Text Mining Analysis of Phenotype Descriptions in Plants\n",
    "The purpose of this notebook is to evaluate what can be learned from a natural language processing approach to analyzing free-text descriptions of phenotype descriptions of plants. The approach is to generate pairwise distance matrices between a set of plant phenotype descriptions across different species, sourced from academic papers and online model species databases. These pairwise distance matrices can be constructed using any vectorization method that can be applied to natural language. In this notebook, we specifically evaluate the use of n-grams, bag-of-words, and topic modeling techniques, word and document embedding using Word2Vec and Doc2Vec, context-dependent word-embeddings using BERT and BioBERT, and ontology term annotations with automated annotation tools such as NOBLE Coder. We compare the performance of these approaches to using semantic similarity with manually annotated and experimentally validated ontology term annotations. \n",
    "\n",
    "<a id=\"links\"></a>\n",
    "## Relevant links of interest:\n",
    "- Paper describing comparison of NLP and ontology annotation approaches to curation: [Braun and Lawrence-Dill (2020)](https://doi.org/10.3389/fpls.2019.01629)\n",
    "- Paper describing results of manual phenotype description curation: [Oellrich, Walls et al. (2015](https://plantmethods.biomedcentral.com/articles/10.1186/s13007-015-0053-y)\n",
    "- Plant databases with phenotype description text data available: [TAIR](https://www.arabidopsis.org/), [SGN](https://solgenomics.net/), [MaizeGDB](https://www.maizegdb.org/)\n",
    "- Accompanying Python package for working with phenotype descriptions: [OATS](https://github.com/irbraun/oats)\n",
    "- Python package used for NLP functions and machine learning: [NLTK](https://www.nltk.org/), [Gensim](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
    "- Python package used for working with biological ontologies: [Pronto](https://pronto.readthedocs.io/en/latest/)\n",
    "- Python package for loading pretrained BERT models: [PyTorch Pretrained BERT](https://pypi.org/project/pytorch-pretrained-bert/)\n",
    "- For BERT Models pretrained on PubMed and PMC: [BioBERT Paper](https://arxiv.org/abs/1901.08746), [BioBERT Models](https://github.com/naver/biobert-pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 5.96863579750061 secs.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import argparse\n",
    "import shlex\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from itertools import combinations\n",
    "from scipy.special import comb\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp, hypergeom, pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, flatten, to_hms\n",
    "from oats.utils.utils import function_wrapper_with_duration, remove_duplicates_retain_order\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocab_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocab_connected_components, reduce_vocab_linares_pontes\n",
    "\n",
    "from _utils import Method\n",
    "\n",
    "# Some settings for how data is visualized in the notebook.\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running as a notebook\n"
     ]
    }
   ],
   "source": [
    "# Set a tag that specifies whether this is being run as a notebook or a script. Some sections are skipped when \n",
    "# running as a script, as this is intended to be run as a batch process on a cluster or something like that. This also\n",
    "# dictates whether arguments for running the full analysis pipeline will be taken from the command line arguments or \n",
    "# need to be taken from a string specified here in the notebook.\n",
    "script_name = os.path.basename(sys.argv[0])\n",
    "if script_name == \"ipykernel_launcher.py\":\n",
    "    NOTEBOOK = True\n",
    "    print(\"running as a notebook\")\n",
    "elif script_name == \"analysis.py\":\n",
    "    NOTEBOOK = False\n",
    "    print(\"running as a script\")\n",
    "else:\n",
    "    raise Exception(\"problem determining if this is being run as a notebook or a script.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_1\"></a>\n",
    "## Part 1. Loading and Filtering Data\n",
    "This section defines some constants which are used for creating a uniquely named directory to contain all the outputs from running this instance of this notebook. The naming scheme is based on the time that the notebook is run. All the input and output file paths for loading datasets or models are also contained within this section, so that if anything is moved the directories and file names should only have to be changed at this point and nowhere else further into the notebook. If additional files are added to the notebook cells they should be put here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"args\"></a>\n",
    "### Reading in arguments\n",
    "Command line arguments are used to define which subset of the approaches that are evaluated in this notebook are used during a given run. Because the pairwise distances matrices become very large when as the number of genes increases, the number of approaches used (which each generated one distance matrix) can be lowered if the script is using too much memory for datasets that contain many genes. Although there are differences in runtime for each approach where ones that generated larger vectors (n-grams) instead of small embeddings (Word2Vec) take longer, this is not significant compared to how long operations take on the resulting distance matrices, which are all the same size for any given approach, so it is the number of approaches used, not which ones, that matters in reducing the time and memory used for each run. In addition, arguments are also used here to pick which dataset should be used later in the notebook, and whether files should be created for using the results later for the dockerized app (those files are large, they shouldn't be created unless they'll be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(annotations=True, app=False, app_only=False, baseline=True, bert=True, bio_large=False, bio_small=True, biobert=True, collapsed=False, combined=True, dataset='plants', filter=True, ic=True, lda=True, learning=False, name='notebook', nmf=True, noblecoder=True, ratio=None, subset=600, vanilla=False, vocab=True)\n"
     ]
    }
   ],
   "source": [
    "# Creating the set of arguments that can be used to determine which approaches are run.\n",
    "DATASET_OPTIONS = [\"plants\",\"diseases\",\"snippets\",\"contexts\",\"biosses\",\"pairs\"]\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Arguments about how to name the output, what data to look at, and what to do with it.\n",
    "parser.add_argument(\"--name\", dest=\"name\", required=True, help=\"create a name for this run of the anaylsis, used in naming the output directory\")\n",
    "parser.add_argument(\"--dataset\", dest=\"dataset\", choices=DATASET_OPTIONS, required=True, help=\"name of the dataset the analysis pipeline should be run on\")\n",
    "parser.add_argument(\"--subset\", dest=\"subset\", type=int, required=False, help=\"randomly subset the data to only include this number of genes, used for testing\")\n",
    "parser.add_argument(\"--app\", dest=\"app\", required=False, action='store_true', help=\"use to have the script output objects needed to build the streamlit app\")\n",
    "parser.add_argument(\"--filter\", dest=\"filter\", required=False, action='store_true', help=\"remove genes that aren't mapped to any of the outside resources\")\n",
    "parser.add_argument(\"--ratio\", dest=\"ratio\", type=float, required=False, help=\"what should be the ratio be between the positive and negative classes\")\n",
    "\n",
    "# Arguments about which approaches to use.\n",
    "parser.add_argument(\"--learning\", dest=\"learning\", required=False, action='store_true', help=\"use the approaches that involve neural networks\")\n",
    "parser.add_argument(\"--bert\", dest=\"bert\", required=False, action='store_true', help=\"use the approaches that involve BERT\")\n",
    "parser.add_argument(\"--biobert\", dest=\"biobert\", required=False, action='store_true', help=\"use the approaches that involve BioBERT\")\n",
    "parser.add_argument(\"--bio_small\", dest=\"bio_small\", required=False, action='store_true', help=\"use the smaller bio nlp models\")\n",
    "parser.add_argument(\"--bio_large\", dest=\"bio_large\", required=False, action='store_true', help=\"use the larger bio nlp models\")\n",
    "parser.add_argument(\"--noblecoder\", dest=\"noblecoder\", required=False, action='store_true', help=\"use the approaches that involve computational annotation\")\n",
    "parser.add_argument(\"--lda\", dest=\"lda\", required=False, action='store_true', help=\"use the approaches that involve topic modeling\")\n",
    "parser.add_argument(\"--nmf\", dest=\"nmf\", required=False, action='store_true', help=\"use the approaches that involve topic modeling\")\n",
    "parser.add_argument(\"--vanilla\", dest=\"vanilla\", required=False, action='store_true', help=\"use the n-grams (bag-of-words) approach\")\n",
    "parser.add_argument(\"--vocab\", dest=\"vocab\", required=False, action='store_true', help=\"using the n-grams approach but with modified vocabularies\")\n",
    "parser.add_argument(\"--collapsed\", dest=\"collapsed\", required=False, action='store_true', help=\"using the n-grams approach but with collapsed vocabularies\")\n",
    "parser.add_argument(\"--annotations\", dest=\"annotations\", required=False, action='store_true', help=\"use the curated annotations\")\n",
    "parser.add_argument(\"--ic\", dest=\"ic\", required=False, action='store_true', help=\"use the curated annotations with information content\")\n",
    "parser.add_argument(\"--combined\", dest=\"combined\", required=False, action='store_true', help=\"use the methods that combine n-grams and embedding approaches\")\n",
    "parser.add_argument(\"--baseline\", dest=\"baseline\", required=True, action='store_true', help=\"use the methods that only check identity as a baseline approach\")\n",
    "parser.add_argument(\"--apponly\", dest=\"app_only\", required=False, action='store_true', help=\"list of methods needed only for the streamlit application\")\n",
    "\n",
    "\n",
    "# Specify the command line argument list here if running as a notebook instead.\n",
    "if NOTEBOOK:\n",
    "    arg_string = \"--name notebook --dataset plants --app --learning  --bert --noblecoder --lda --nmf --vanilla --vocab --annotations\"\n",
    "    arg_string = \"--name notebook --dataset plants --filter --subset 600 --ic --biobert --bio_small  --baseline --noblecoder --annotations --bert --combined --lda --nmf --vocab \"\n",
    "    args = parser.parse_args(shlex.split(arg_string))\n",
    "    print(args)\n",
    "else:\n",
    "    arg_string = \"--name script_pipeline --dataset plants --app --learning --bert --noblecoder --lda --nmf --vanilla --vocab --annotations\"\n",
    "    args = parser.parse_args(shlex.split(arg_string))\n",
    "    print(args)\n",
    "    print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"paths\"></a>\n",
    "### Defining the input file paths and creating output directory\n",
    "This section specifies the path to the base output directory, and creates all the subfolders inside of it that contain results that pertain to different parts of the analysis. Paths to all the files that are used by this notebook are specified in the subsequent cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and name an output directory according to when the notebooks or script was run.\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",\"{}_{}_{}\".format(args.name,datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'),random.randrange(1000,9999)))\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# Other subfolders that output files get organized into when this analysis notebook is run.\n",
    "STREAMLIT_DIR = \"app_resources\"\n",
    "GROUPINGS_DIR = \"gene_mappings\"\n",
    "APPROACHES_DIR = \"approaches_info\"\n",
    "VOCABULARIES_DIR = \"vocabularies\"\n",
    "METRICS_DIR = \"main_metrics\"\n",
    "QUESTIONS_DIR = \"questions_info\"\n",
    "PLOTS_DIR = \"distributions\"\n",
    "GROUP_DISTS_DIR = \"group_distances\"\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,STREAMLIT_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,GROUPINGS_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,APPROACHES_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,VOCABULARIES_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,METRICS_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,QUESTIONS_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,PLOTS_DIR))\n",
    "os.mkdir(os.path.join(OUTPUT_DIR,GROUP_DISTS_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to different datasets containing gene names, text descriptions, and/or ontology term annotations.\n",
    "plant_dataset_path = \"../../plant-data/genes_texts_annots.csv\"\n",
    "clinvar_dataset_path = \"../data/clinvar/clinvar_diseases.csv\"\n",
    "snpedia_snippets_dataset_path = \"../data/snpedia/snpedia_snippets.csv\"\n",
    "snpedia_contexts_dataset_path = \"../data/snpedia/snpedia_contexts.csv\"\n",
    "\n",
    "# Paths to datasets of sentence or description pairs.\n",
    "paired_phenotypes_path = \"../data/paired_sentences/plants/scored.csv\"\n",
    "biosses_datset_path = \"../data/paired_sentences/biosses/cleaned_by_me.csv\"\n",
    "\n",
    "# Paths to files for data about how genes can be grouped into biochemical pathways, etc.\n",
    "kegg_pathways_path = \"../../plant-data/reshaped_data/kegg_pathways.csv\" \n",
    "plantcyc_pathways_path = \"../../plant-data/reshaped_data/plantcyc_pathways.csv\" \n",
    "lloyd_meinke_subsets_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets.csv\" \n",
    "lloyd_meinke_classes_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes.csv\" \n",
    "\n",
    "# Paths files that contain mappings from the identifiers used by those groups to full name strings.\n",
    "kegg_pathways_names_path = \"../../plant-data/reshaped_data/kegg_pathways_name_map.csv\"\n",
    "plantcyc_pathways_names_path = \"../../plant-data/reshaped_data/plantcyc_pathways_name_map.csv\"\n",
    "lloyd_meinke_subsets_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets_name_map.csv\"\n",
    "lloyd_meinke_classes_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes_name_map.csv\"\n",
    "\n",
    "# Paths to other files including the ortholog edgelist from Panther, and cleaned files from the two papers.\n",
    "pppn_edgelist_path = \"../../plant-data/papers/oellrich_walls_et_al_2015/supplemental_files/13007_2015_53_MOESM9_ESM.txt\"\n",
    "ortholog_file_path = \"../../plant-data/databases/panther/PlantGenomeOrthologs_IRB_Modified.txt\"\n",
    "lloyd_function_hierarchy_path = \"../../plant-data/papers/lloyd_meinke_2012/versions_cleaned_by_me/192393Table_S1_Final.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text corpora paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathways to text corpora files that are used in this analysis.\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning model paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paths to pretrained or saved models used for embeddings with Word2Vec or Doc2vec.\n",
    "doc2vec_plants_path = \"../models/plants_dbow/doc2vec_ep100_both_dim200_a_s_min3.model\"\n",
    "doc2vec_wikipedia_path = \"../models/enwiki_dbow/doc2vec.bin\"\n",
    "word2vec_plants_path = \"../models/plants_sg/word2vec_both_dim200_500_a_s_min3_window8.model\"\n",
    "word2vec_wikipedia_path = \"../models/wiki_sg/word2vec.bin\"\n",
    "\n",
    "# Paths to BioBERT models.\n",
    "biobert_pmc_path = \"../models/biobert_v1.0_pmc/pytorch_model\"                                  \n",
    "biobert_pubmed_path = \"../models/biobert_v1.0_pubmed/pytorch_model\"                                 \n",
    "biobert_pubmed_pmc_path = \"../models/biobert_v1.0_pubmed_pmc/pytorch_model\"      \n",
    "\n",
    "# Word2Vec models available pretrained from Pyysalo et al.\n",
    "# http://bio.nlplab.org/#doc-tools\n",
    "# http://evexdb.org/pmresources/vec-space-models/\n",
    "word2vec_bio_pmc_path = \"../models/bio_nlp_lab/PMC-w2v.bin\"\n",
    "word2vec_bio_pubmed_path = \"../models/bio_nlp_lab/PubMed-w2v.bin\"\n",
    "word2vec_bio_pubmed_and_pmc_path = \"../models/bio_nlp_lab/PubMed-and-PMC-w2v.bin\"\n",
    "word2vec_bio_wikipedia_pubmed_and_pmc_path = \"../models/bio_nlp_lab/wikipedia-pubmed-and-PMC-w2v.bin\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ontology related paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path the jar file necessary for running NOBLE Coder.\n",
    "noblecoder_jarfile_path = \"../lib/NobleCoder-1.0.jar\"  \n",
    "\n",
    "# Paths to obo ontology files are used to create the ontology objects used.\n",
    "# If pickling these objects works, that might be better because building the larger ontology objects takes a long time.\n",
    "go_obo_path = \"../ontologies/go.obo\"                                                                \n",
    "po_obo_path = \"../ontologies/po.obo\"                                                             \n",
    "pato_obo_path = \"../ontologies/pato.obo\"\n",
    "go_pickle_path = \"../ontologies/go.pickle\"                                                                \n",
    "po_pickle_path = \"../ontologies/po.pickle\"                                                             \n",
    "pato_pickle_path = \"../ontologies/pato.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read_text_data\"></a>\n",
    "### Reading in the dataset of genes and their associated phenotype descriptions and annotations\n",
    "Every dataset that is relevant to this analysis or could be used is read in here and described. This is done even if additional arguments specified that the analysis should just focus on one of them. This is set up this way so that the analysis script will immediately fail if any of these datasets are missing, or if any of the paths are incorrect, this was useful when running locally before moving to a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6274</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7907</td>\n",
       "      <td>4841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                     6274                 3818\n",
       "1     gmx                       30                   23\n",
       "2     mtr                       37                   36\n",
       "3     osa                       92                   85\n",
       "4     sly                       69                   69\n",
       "5     zma                     1405                  810\n",
       "6   total                     7907                 4841"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the human dataset of concatenated disease names from ClinVar annotations.\n",
    "clinvar_dataset = Dataset(clinvar_dataset_path)\n",
    "clinvar_dataset.filter_has_description()\n",
    "clinvar_dataset.describe()\n",
    "\n",
    "# Loading the human dataset of concatenated SNP snippets for genes in SNPedia.\n",
    "snpedia_snippets_dataset = Dataset(snpedia_snippets_dataset_path)\n",
    "snpedia_snippets_dataset.filter_has_description()\n",
    "snpedia_snippets_dataset.describe()\n",
    "\n",
    "# Loading the human dataset of concatenated SNP context sentences for genes in SNPedia.\n",
    "snpedia_contexts_dataset = Dataset(snpedia_contexts_dataset_path)\n",
    "snpedia_contexts_dataset.filter_has_description()\n",
    "snpedia_contexts_dataset.describe()\n",
    "\n",
    "# Loading the plant dataset of phenotype descriptions.\n",
    "plant_dataset = Dataset(plant_dataset_path)\n",
    "plant_dataset.filter_has_description()\n",
    "plant_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6274</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7907</td>\n",
       "      <td>4841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                     6274                 3818\n",
       "1     gmx                       30                   23\n",
       "2     mtr                       37                   36\n",
       "3     osa                       92                   85\n",
       "4     sly                       69                   69\n",
       "5     zma                     1405                  810\n",
       "6   total                     7907                 4841"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which dataset should be used for the rest of the analysis? Useful for changing when running as a notebook.\n",
    "# This picks the dataset based on the dataset argument, if it's not one of the paired sentence ones. If it is,\n",
    "# then the plant dataset is used by default for this section just so that the notebook doesn't need to be changed\n",
    "# and all parts of the analysis still apply.\n",
    "dataset_arg_to_dataset_obj = {\n",
    "    \"plants\":plant_dataset,\n",
    "    \"diseases\":clinvar_dataset,\n",
    "    \"snippets\":snpedia_snippets_dataset,\n",
    "    \"contexts\":snpedia_contexts_dataset}\n",
    "dataset = dataset_arg_to_dataset_obj.get(args.dataset, plant_dataset)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"relating\"></a>\n",
    "### Relating the dataset of genes to the dataset of groups or categories\n",
    "This section generates tables that indicate how the genes present in the dataset were mapped to the defined pathways or groups. This includes a summary table that indicates how many genes by species were successfully  mapped to at least one pathway or group, as well as a more detailed table describing how many genes from each species were mapped to each particular pathway or group. Additionally, a pairwise group similarity matrix is also generated, where the similarity is given as the Jaccard similarity between two groups based on whether genes are shared by those groups or not. The function defined in this section returns a groupings object that can be used again, as well as the IDs of the genes in the full dataset that were found to be relevant to those particular groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_groupings_object_and_write_summary_tables(dataset, groupings_filename, group_name_mappings, name):\n",
    "\n",
    "    # Load the groupings object.\n",
    "    groups = Groupings(groupings_filename, group_name_mappings)\n",
    "    id_to_group_ids = groups.get_id_to_group_ids_dict(dataset.get_gene_dictionary())\n",
    "    group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "    group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "    groups.to_pandas().to_csv(os.path.join(OUTPUT_DIR,GROUPINGS_DIR,\"{}_groupings.csv\".format(name)))\n",
    "\n",
    "    # Generate a table describing how many of the genes input from each species map to atleast one group.\n",
    "    summary = defaultdict(dict)\n",
    "    species_dict = dataset.get_species_dictionary()\n",
    "    for species in dataset.get_species():\n",
    "        summary[species][\"input\"] = len([x for x in dataset.get_ids() if species_dict[x]==species])\n",
    "        summary[species][\"mapped\"] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "    table = pd.DataFrame(summary).transpose()\n",
    "    table.loc[\"total\"]= table.sum()\n",
    "    table[\"fraction\"] = table.apply(lambda row: \"{:0.4f}\".format(row[\"mapped\"]/row[\"input\"]), axis=1)\n",
    "    table = table.reset_index(inplace=False)\n",
    "    table = table.rename({\"index\":\"species\"}, axis=\"columns\")\n",
    "    table.to_csv(os.path.join(OUTPUT_DIR,GROUPINGS_DIR,\"{}_mappings_summary.csv\".format(name)), index=False)\n",
    "\n",
    "    \n",
    "    # Generate a table describing how many genes from each species map to which particular group.\n",
    "    if len(group_id_to_ids) != 0:\n",
    "        summary = defaultdict(dict)\n",
    "        for group_id,ids in group_id_to_ids.items():\n",
    "            summary[group_id].update({species:len([x for x in ids if species_dict[x]==species]) for species in dataset.get_species()})\n",
    "            summary[group_id][\"total\"] = len([x for x in ids])\n",
    "        table = pd.DataFrame(summary).transpose()\n",
    "        table = table.sort_values(by=\"total\", ascending=False)\n",
    "        table = table.reset_index(inplace=False)\n",
    "        table = table.rename({\"index\":\"pathway_id\"}, axis=\"columns\")\n",
    "        table[\"pathway_name\"] = table[\"pathway_id\"].map(groups.get_long_name)\n",
    "        table.loc[\"total\"] = table.sum()\n",
    "        table.loc[\"total\",\"pathway_id\"] = \"total\"\n",
    "        table.loc[\"total\",\"pathway_name\"] = \"total\"\n",
    "        table = table[table.columns.tolist()[-1:] + table.columns.tolist()[:-1]]\n",
    "        table.to_csv(os.path.join(OUTPUT_DIR,GROUPINGS_DIR,\"{}_mappings_by_group.csv\".format(name)), index=False)\n",
    "    \n",
    "    \n",
    "        # What are the similarities between the groups for the genes present in this dataset?\n",
    "        group_sims = defaultdict(dict)\n",
    "        for group_id_1,ids_1 in group_id_to_ids.items():\n",
    "            for group_id_2,ids_2 in group_id_to_ids.items():\n",
    "                jaccard_sim = len(set(ids_1).intersection(set(ids_2)))/len(set(ids_1).union(set(ids_2)))\n",
    "                group_sims[group_id_1][group_id_2] = jaccard_sim\n",
    "        table = pd.DataFrame(group_sims)\n",
    "    \n",
    "    \n",
    "        # Changing the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency, special case.\n",
    "        if name == \"subsets\":\n",
    "            lmtm_df = pd.read_csv(lloyd_function_hierarchy_path)    \n",
    "            subsets_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in table.columns]\n",
    "            table = table[subsets_in_order]\n",
    "            table = table.reindex(subsets_in_order)\n",
    "        \n",
    "        \n",
    "        # Formatting the column names for this table correctly and outputting to a file.\n",
    "        table = table.reset_index(drop=False).rename({\"index\":\"group\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)\n",
    "        table.to_csv(os.path.join(OUTPUT_DIR,GROUPINGS_DIR,\"{}_similarity_matrix.csv\".format(name)), index=False)\n",
    "    \n",
    "\n",
    "    # Returning the groupings object and the list of IDs for genes that were mapped to one or more groups.\n",
    "    return(groups, group_mapped_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"kegg\"></a>\n",
    "### Reading in and relating the pathways from KEGG\n",
    "See dataset description for what files were used to construct these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_ids</th>\n",
       "      <th>gene_identifiers</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>pathway_id_for_species</th>\n",
       "      <th>pathway_name</th>\n",
       "      <th>pathway_class</th>\n",
       "      <th>gene_names</th>\n",
       "      <th>ncbi_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>ko_number</th>\n",
       "      <th>ec_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>aas|pyridoxal phosphate (plp)-dependent transf...</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ath00901</td>\n",
       "      <td>Indole alkaloid biosynthesis - Arabidopsis tha...</td>\n",
       "      <td>Metabolism; Biosynthesis of other secondary me...</td>\n",
       "      <td>AAS|Pyridoxal phosphate (PLP)-dependent transf...</td>\n",
       "      <td>AT2G20340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K01593</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>calcium-dependent phosphotriesterase superfami...</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ath00901</td>\n",
       "      <td>Indole alkaloid biosynthesis - Arabidopsis tha...</td>\n",
       "      <td>Metabolism; Biosynthesis of other secondary me...</td>\n",
       "      <td>Calcium-dependent phosphotriesterase superfami...</td>\n",
       "      <td>AT1G74010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K01757</td>\n",
       "      <td>EC:4.3.3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ss2|strictosidine synthase 2|ncbi=at1g74020|at...</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ath00901</td>\n",
       "      <td>Indole alkaloid biosynthesis - Arabidopsis tha...</td>\n",
       "      <td>Metabolism; Biosynthesis of other secondary me...</td>\n",
       "      <td>SS2|strictosidine synthase 2</td>\n",
       "      <td>AT1G74020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K01757</td>\n",
       "      <td>EC:4.3.3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ss3|strictosidine synthase 3|ncbi=at1g74000|at...</td>\n",
       "      <td>ko00901</td>\n",
       "      <td>ath00901</td>\n",
       "      <td>Indole alkaloid biosynthesis - Arabidopsis tha...</td>\n",
       "      <td>Metabolism; Biosynthesis of other secondary me...</td>\n",
       "      <td>SS3|strictosidine synthase 3</td>\n",
       "      <td>AT1G74000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K01757</td>\n",
       "      <td>EC:4.3.3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>riba3|monofunctional riboflavin biosynthesis p...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>RIBA3|monofunctional riboflavin biosynthesis p...</td>\n",
       "      <td>AT5G59750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K14652</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>riba2|riboflavin biosynthesis protein  [ec:4.1...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>RIBA2|riboflavin biosynthesis protein  [EC:4.1...</td>\n",
       "      <td>AT2G22450</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K14652</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>gch|gtp cyclohydrolase ii  [ec:4.1.99.12 3.5.4...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>GCH|GTP cyclohydrolase II  [EC:4.1.99.12 3.5.4...</td>\n",
       "      <td>AT5G64300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K14652</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>pyrd|cytidine/deoxycytidylate deaminase family...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>PyrD|Cytidine/deoxycytidylate deaminase family...</td>\n",
       "      <td>AT4G20960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K11752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>phs1|cytidine/deoxycytidylate deaminase family...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>PHS1|cytidine/deoxycytidylate deaminase family...</td>\n",
       "      <td>AT3G47390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K11752</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>haloacid dehalogenase-like hydrolase (had) sup...</td>\n",
       "      <td>ko00740</td>\n",
       "      <td>ath00740</td>\n",
       "      <td>Riboflavin metabolism - Arabidopsis thaliana (...</td>\n",
       "      <td>Metabolism; Metabolism of cofactors and vitamins</td>\n",
       "      <td>Haloacid dehalogenase-like hydrolase (HAD) sup...</td>\n",
       "      <td>AT4G11570</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KO:K22912</td>\n",
       "      <td>EC:3.1.3.104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species group_ids                                   gene_identifiers pathway_id pathway_id_for_species                                       pathway_name                                      pathway_class                                         gene_names    ncbi_id  uniprot_id  ko_number     ec_number\n",
       "0     ath   ko00901  aas|pyridoxal phosphate (plp)-dependent transf...    ko00901               ath00901  Indole alkaloid biosynthesis - Arabidopsis tha...  Metabolism; Biosynthesis of other secondary me...  AAS|Pyridoxal phosphate (PLP)-dependent transf...  AT2G20340         NaN  KO:K01593           NaN\n",
       "1     ath   ko00901  calcium-dependent phosphotriesterase superfami...    ko00901               ath00901  Indole alkaloid biosynthesis - Arabidopsis tha...  Metabolism; Biosynthesis of other secondary me...  Calcium-dependent phosphotriesterase superfami...  AT1G74010         NaN  KO:K01757    EC:4.3.3.2\n",
       "2     ath   ko00901  ss2|strictosidine synthase 2|ncbi=at1g74020|at...    ko00901               ath00901  Indole alkaloid biosynthesis - Arabidopsis tha...  Metabolism; Biosynthesis of other secondary me...                       SS2|strictosidine synthase 2  AT1G74020         NaN  KO:K01757    EC:4.3.3.2\n",
       "3     ath   ko00901  ss3|strictosidine synthase 3|ncbi=at1g74000|at...    ko00901               ath00901  Indole alkaloid biosynthesis - Arabidopsis tha...  Metabolism; Biosynthesis of other secondary me...                       SS3|strictosidine synthase 3  AT1G74000         NaN  KO:K01757    EC:4.3.3.2\n",
       "4     ath   ko00740  riba3|monofunctional riboflavin biosynthesis p...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  RIBA3|monofunctional riboflavin biosynthesis p...  AT5G59750         NaN  KO:K14652           NaN\n",
       "5     ath   ko00740  riba2|riboflavin biosynthesis protein  [ec:4.1...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  RIBA2|riboflavin biosynthesis protein  [EC:4.1...  AT2G22450         NaN  KO:K14652           NaN\n",
       "6     ath   ko00740  gch|gtp cyclohydrolase ii  [ec:4.1.99.12 3.5.4...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  GCH|GTP cyclohydrolase II  [EC:4.1.99.12 3.5.4...  AT5G64300         NaN  KO:K14652           NaN\n",
       "7     ath   ko00740  pyrd|cytidine/deoxycytidylate deaminase family...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  PyrD|Cytidine/deoxycytidylate deaminase family...  AT4G20960         NaN  KO:K11752           NaN\n",
       "8     ath   ko00740  phs1|cytidine/deoxycytidylate deaminase family...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  PHS1|cytidine/deoxycytidylate deaminase family...  AT3G47390         NaN  KO:K11752           NaN\n",
       "9     ath   ko00740  haloacid dehalogenase-like hydrolase (had) sup...    ko00740               ath00740  Riboflavin metabolism - Arabidopsis thaliana (...   Metabolism; Metabolism of cofactors and vitamins  Haloacid dehalogenase-like hydrolase (HAD) sup...  AT4G11570         NaN  KO:K22912  EC:3.1.3.104"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the dataset of groupings for pathways in KEGG.\n",
    "kegg_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(kegg_pathways_names_path).itertuples()}\n",
    "kegg_groups, kegg_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, kegg_pathways_path, kegg_name_mapping, \"kegg_pathways\")\n",
    "kegg_groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plantcyc\"></a>\n",
    "### Reading in and relating the pathways from PlantCyc\n",
    "See dataset description for what files were used to construct these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_ids</th>\n",
       "      <th>gene_identifiers</th>\n",
       "      <th>pathway_id</th>\n",
       "      <th>pathway_name</th>\n",
       "      <th>ec_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at1g52400-monomer|abscisic acid glucose ester ...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-3.2.1.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at4g15550-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at4g15260-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at3g21790-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at3g21760-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at2g23210-monomer|abscisate &amp;beta;-glucosyltra...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at1g05530-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at1g05560-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at4g34138-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>at2g23250-monomer|abscisic acid glucosyltransf...</td>\n",
       "      <td>PWY-5272</td>\n",
       "      <td>abscisic acid degradation by glucosylation</td>\n",
       "      <td>EC-2.4.1.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species group_ids                                   gene_identifiers pathway_id                                pathway_name     ec_number\n",
       "0     ath  PWY-5272  at1g52400-monomer|abscisic acid glucose ester ...   PWY-5272  abscisic acid degradation by glucosylation  EC-3.2.1.175\n",
       "1     ath  PWY-5272  at4g15550-monomer|abscisate &beta;-glucosyltra...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "2     ath  PWY-5272  at4g15260-monomer|abscisate &beta;-glucosyltra...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "3     ath  PWY-5272  at3g21790-monomer|abscisate &beta;-glucosyltra...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "4     ath  PWY-5272  at3g21760-monomer|abscisate &beta;-glucosyltra...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "5     ath  PWY-5272  at2g23210-monomer|abscisate &beta;-glucosyltra...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "6     ath  PWY-5272  at1g05530-monomer|abscisic acid glucosyltransf...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "7     ath  PWY-5272  at1g05560-monomer|abscisic acid glucosyltransf...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "8     ath  PWY-5272  at4g34138-monomer|abscisic acid glucosyltransf...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263\n",
       "9     ath  PWY-5272  at2g23250-monomer|abscisic acid glucosyltransf...   PWY-5272  abscisic acid degradation by glucosylation  EC-2.4.1.263"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the dataset of groupings for pathways in PlantCyc.\n",
    "plantcyc_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(plantcyc_pathways_names_path).itertuples()}\n",
    "pmn_groups, pmn_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, plantcyc_pathways_path, plantcyc_name_mapping, \"plantcyc_pathways\")\n",
    "pmn_groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chlorotic leaf lesion. Necrotic leaf lesions. Premature leaf senescence. Premature plant death. Leaves have round lesions. Leaves have elliptical lesions. Chlorotic-necrotic leaf lesions. Early leaf senescence. Premature plant death, Medium-sized round to elliptical lesions, often with concentric rings of dead and dying tissue. Start as small chlorotic flecks near the tip of the first leaf at the 2- to 4-leaf stage. These subsequently enlarge, become necrotic, and coalesce to produce early leaf senescence and often plant death. Some plants survive to produce pollen but no ears. Expression of lls1 is cell autonomous. Lesion. Necrotic or chlorotic lesions resembling those formed by fungal infection of susceptible tissue.\n",
      "\n",
      "Necrotic lesions. Develops lesions resembling those produced during a pathogen-induced rapid cell death response (the hypersensitive response, HR). Unable to control the rate and extent of cell death after exposure to a variety of stimuli that induce senescence responses. Substantial ion leakage as an indicator of the loss of integrity of cell menbranes and cell death. Rapid and spreading necrotic responses to both virulent and avirulent Pseudomonas syringae pv. Maculicola or pv. Tomato pathogens and to ethylene. Develops necrotic lesions with aging. Sensitive to mechanical stress in a developmentally controlled manner. Susceptible to opportunistic pathogens and exhibits decreased growth inhibition of a heterologous pathogen of bean. In constant darkness chlorophyll is retained resulting in a stay-green phenotype. This mutant develops a lesion-mimic phenotype on the leaves similar to that of several acd1 mutants. The lesions occur in a development-related, light-dependent fashion, coinciding with the initiation of leaf senescence in the wild-type. Premature cell death was observed in the flowers. Sepals and petals started to degrade earlier than that of wild-type, resulting in flowers that fail to fully open. 40% of seeds aborts at an early stage of development.\n",
      "\n",
      "Necrotic lesions. Accumulation of red chlorophyll catabolites (RCC) and RCC-like compounds during senescence. Detached leaves maintained in permanent darkness accumulate a faint red-orange coloration after 5 to 7 days, which differs from wild type where a progressive yellowing of the leaves is observed. This red coloration is not observed under normal senescence. Development-related lesion-mimic phenotype, occuring at a presenescent stage. Develops spontaneous lesions resembling those produced during a pathogen-induced rapid cell death response (the hypersensitive response, HR). Typical HR characteristics occur both within the necrotic tissue and within the healthy part of the plant, including: modification of plant cell walls, resistance to bacterial pathogens and accumulation of defense-related gene transcripts, the signal molecule salicylic acid and an antimicrobial compound. No cell death symptoms or alterations in senescence behavior compared to wild type.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_to_pmn_group_ids, pmn_group_id_to_ids = pmn_groups.get_groupings_for_dataset(dataset)\n",
    "pmn_group_id_to_ids\n",
    "texts = dataset.get_description_dictionary()\n",
    "for i in pmn_group_id_to_ids[\"PWY-5098\"]:\n",
    "    #PWY-762\n",
    "    #PWY-3982\n",
    "    print(texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subsets_and_classes\"></a>\n",
    "###  Reading in and relating the phenotype classes and subsets from Lloyd and Meinke (2012)\n",
    "See dataset description for what files were used to construct these mappings, and links there and above to the related paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_ids</th>\n",
       "      <th>gene_identifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>FSM</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB|FSM|OVP|SRF</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>CDR|LIT</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>IST|WAT</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>OVP|SRF</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>EMB</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>CHS</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>NLS|GRS|IST</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>LEF|FSM</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species                        group_ids                                   gene_identifiers\n",
       "0     ath                              FSM                         at1g01030|nga3|top1|ngatha\n",
       "1     ath                  EMB|FSM|OVP|SRF    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2     ath                          CDR|LIT             at1g01060|lhy|late elongated hypocotyl\n",
       "3     ath                          IST|WAT   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4     ath                          OVP|SRF                 at1g01280|cyp703a2|cytochrome p450\n",
       "5     ath                              EMB        at1g01370|cenh3|centromere-specific histone\n",
       "6     ath                              CHS  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7     ath                      NLS|GRS|IST  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8     ath                          LEF|FSM                          at1g01510|an|angustifolia\n",
       "9     ath  SRL|ROT|LEF|MSL|STT|RTH|TCM|TMP                              at1g01550|bps1|bypass"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the datasets of phenotype subset classifications from the Lloyd, Meinke 2012 paper.\n",
    "lloyd_meinke_subsets_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(lloyd_meinke_subsets_names_path).itertuples()}\n",
    "phe_subsets_groups, subsets_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, lloyd_meinke_subsets_path, lloyd_meinke_subsets_name_mapping, \"oellrich_walls_subsets\")\n",
    "phe_subsets_groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>group_ids</th>\n",
       "      <th>gene_identifiers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g01030|nga3|top1|ngatha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>S</td>\n",
       "      <td>at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>T</td>\n",
       "      <td>at1g01060|lhy|late elongated hypocotyl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01120|kcs1|3-ketoacyl-coa synthase defective</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>R</td>\n",
       "      <td>at1g01280|cyp703a2|cytochrome p450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>S</td>\n",
       "      <td>at1g01370|cenh3|centromere-specific histone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>H</td>\n",
       "      <td>at1g01460|pipk11|phosphatidylinositol phosphat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01480|acs2|aminocyclopropane carboxylate s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>V</td>\n",
       "      <td>at1g01510|an|angustifolia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>L</td>\n",
       "      <td>at1g01550|bps1|bypass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species group_ids                                   gene_identifiers\n",
       "0     ath         R                         at1g01030|nga3|top1|ngatha\n",
       "1     ath         S    at1g01040|sus1|dcl1|sin1|caf|abnormal suspensor\n",
       "2     ath         T             at1g01060|lhy|late elongated hypocotyl\n",
       "3     ath         V   at1g01120|kcs1|3-ketoacyl-coa synthase defective\n",
       "4     ath         R                 at1g01280|cyp703a2|cytochrome p450\n",
       "5     ath         S        at1g01370|cenh3|centromere-specific histone\n",
       "6     ath         H  at1g01460|pipk11|phosphatidylinositol phosphat...\n",
       "7     ath         V  at1g01480|acs2|aminocyclopropane carboxylate s...\n",
       "8     ath         V                          at1g01510|an|angustifolia\n",
       "9     ath         L                              at1g01550|bps1|bypass"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in the datasets of phenotype class classifications from the Lloyd, Meinke 2012 paper.\n",
    "lloyd_meinke_classes_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(lloyd_meinke_classes_names_path).itertuples()}\n",
    "phe_classes_groups, classes_mapped_ids = read_in_groupings_object_and_write_summary_tables(dataset, lloyd_meinke_classes_path, lloyd_meinke_classes_name_mapping, \"oellrich_walls_classes\")\n",
    "phe_classes_groups.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"edges\"></a>\n",
    "### Relating pairs of genes to information about network edges\n",
    "This is done to only include genes and their corresponding phenotype descriptions and annotations that are useful for the current analysis. In this case we want to only retain genes that are mentioned at least one time in the STRING database for a given species. If a gene is not mentioned at all in STRING, there is no information available for whether or not it associates with any other proteins in the dataset so choose to not include it in the analysis. Only genes that have at least one true positive are included because these are the only ones for which the missing information (negatives) is meaningful. These sections obtain these edgelists and lists of IDs for both the edges specified in Oellrich, Walls et al., (2015) which are similarity values between genes based on semantic similarity between their EQ statement representations, and also from STRING, which are protein-protein association scores with a variety of subscores depending on the type of evidence using in determining that association. We also look at the edgelist and list of gene IDs that can be related to PANTHER, where those edges indicate ortholog relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eqs\"></a>\n",
    "### EQ-based similarities from Oellrich, Walls et al., (2015)\n",
    "See dataset description for what files were used to construct these mappings, and links there and above to the related paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1809</td>\n",
       "      <td>1774</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1809</td>\n",
       "      <td>1815</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1809</td>\n",
       "      <td>440</td>\n",
       "      <td>0.926471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1809</td>\n",
       "      <td>1742</td>\n",
       "      <td>0.516393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1809</td>\n",
       "      <td>1813</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1809</td>\n",
       "      <td>2008</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1809</td>\n",
       "      <td>2032</td>\n",
       "      <td>0.417219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1809</td>\n",
       "      <td>1770</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1809</td>\n",
       "      <td>1869</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1809</td>\n",
       "      <td>501</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to     value\n",
       "0  1809  1774  1.000000\n",
       "1  1809  1815  1.000000\n",
       "2  1809   440  0.926471\n",
       "3  1809  1742  0.516393\n",
       "4  1809  1813  1.000000\n",
       "5  1809  2008  0.954545\n",
       "6  1809  2032  0.417219\n",
       "7  1809  1770  1.000000\n",
       "8  1809  1869  1.000000\n",
       "9  1809   501  0.900000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ow_edgelist = AnyInteractions(dataset.get_name_to_id_dictionary(), pppn_edgelist_path)\n",
    "ow_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1008075, 3)\n",
      "(1038612, 3)\n"
     ]
    }
   ],
   "source": [
    "# The edgelist that is returned has some duplicate lines with respect a single gene pair in this dataset.\n",
    "# This would add duplications later when merging with this dataframe of gene pairs. \n",
    "# Fix this issue by first collapsing duplicate rows by STRING by taking the max association for all duplicates.\n",
    "eq_edgelist_collapsed = ow_edgelist.df.copy(deep=True)\n",
    "eq_edgelist_collapsed = eq_edgelist_collapsed.groupby([\"from\",\"to\"], as_index=False)[\"value\"].max() \n",
    "print(eq_edgelist_collapsed.shape)\n",
    "print(ow_edgelist.df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"string\"></a>\n",
    "### Protein-Protein Associations from the STRING database\n",
    "See dataset description for what files were used to construct these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>known_associations</th>\n",
       "      <th>predicted_associations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>73.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>73.0</td>\n",
       "      <td>1859.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4884.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4462.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>73.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4241.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>73.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>73.0</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>73.0</td>\n",
       "      <td>5762.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>73.0</td>\n",
       "      <td>4524.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     from      to  known_associations  predicted_associations\n",
       "346  73.0    49.0                 161                       0\n",
       "348  73.0  1859.0                   0                       0\n",
       "351  73.0  4884.0                   0                       0\n",
       "352  73.0  4462.0                   0                       0\n",
       "353  73.0   131.0                   0                       0\n",
       "355  73.0  4241.0                   0                       0\n",
       "357  73.0   469.0                   0                       0\n",
       "358  73.0  5411.0                   0                       0\n",
       "359  73.0  5762.0                   0                       0\n",
       "360  73.0  4524.0                   0                       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naming_file = \"../../plant-data/databases/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../../plant-data/databases/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis\n",
    "    \"../../plant-data/databases/string/4577.protein.links.detailed.v11.0.txt\", # Maize\n",
    "    \"../../plant-data/databases/string/4530.protein.links.detailed.v11.0.txt\", # Tomato \n",
    "    \"../../plant-data/databases/string/4081.protein.links.detailed.v11.0.txt\", # Medicago\n",
    "    \"../../plant-data/databases/string/3880.protein.links.detailed.v11.0.txt\", # Rice \n",
    "    \"../../plant-data/databases/string/3847.protein.links.detailed.v11.0.txt\", # Soybean\n",
    "    \"../../plant-data/databases/string/9606.protein.links.detailed.v11.0.txt\", # Human\n",
    "]\n",
    "genes = dataset.get_gene_dictionary()\n",
    "string_edgelist = ProteinInteractions(genes, naming_file, *interaction_files)\n",
    "string_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(893386, 4)\n",
      "(1022938, 4)\n"
     ]
    }
   ],
   "source": [
    "# The edgelist that is returned has some duplicate lines with respect a single gene pair in this dataset.\n",
    "# This would add duplications later when merging with this dataframe of gene pairs. \n",
    "# Fix this issue by first collapsing duplicate rows by STRING by taking the max association for all duplicates.\n",
    "string_edgelist_collapsed = string_edgelist.df.copy(deep=True)\n",
    "string_edgelist_collapsed = string_edgelist_collapsed.groupby([\"from\",\"to\"], as_index=False)[\"known_associations\",\"predicted_associations\"].max() \n",
    "print(string_edgelist_collapsed.shape)\n",
    "print(string_edgelist.df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"panther\"></a>\n",
    "### Orthologous genes from PANTHER\n",
    "See dataset description for what files were used to construct these mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>564.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>1074.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107380</th>\n",
       "      <td>2947.0</td>\n",
       "      <td>2621.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179180</th>\n",
       "      <td>1873.0</td>\n",
       "      <td>1873.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190004</th>\n",
       "      <td>3002.0</td>\n",
       "      <td>2379.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192404</th>\n",
       "      <td>1662.0</td>\n",
       "      <td>1842.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198820</th>\n",
       "      <td>3828.0</td>\n",
       "      <td>4678.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413820</th>\n",
       "      <td>3843.0</td>\n",
       "      <td>425.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415020</th>\n",
       "      <td>2529.0</td>\n",
       "      <td>1141.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431292</th>\n",
       "      <td>3616.0</td>\n",
       "      <td>2196.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          from      to  value\n",
       "692      564.0    23.0    1.0\n",
       "788     1074.0    23.0    1.0\n",
       "107380  2947.0  2621.0    1.0\n",
       "179180  1873.0  1873.0    1.0\n",
       "190004  3002.0  2379.0    1.0\n",
       "192404  1662.0  1842.0    1.0\n",
       "198820  3828.0  4678.0    1.0\n",
       "413820  3843.0   425.0    1.0\n",
       "415020  2529.0  1141.0    1.0\n",
       "431292  3616.0  2196.0    1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panther_edgelist = AnyInteractions(dataset.get_name_to_id_dictionary(), ortholog_file_path)\n",
    "panther_edgelist.df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>\n",
    "### Subsetting the dataset to include only genes with relevance to any of the biological questions\n",
    "This is done to only include genes (and the corresponding phenotype descriptions and annotations) which are useful for the current analysis. In this case we want to only retain genes that are mapped to at least one pathway in whatever the source of pathway membership we are using is (KEGG, Plant Metabolic Network, etc). This is because for genes other than these genes, it will be impossible to correctly predict their pathway membership, and we have no evidence that they belong or do not belong in certain pathways so they can not be identified as being true or false negatives in any case. This step is necessary because the datasets used with this analysis consist of all the genes that we were able to obtain a free text phenotype description for, but this set of genes might include genes that are not mapped to any of the other biological resources we are using the evaluate different NLP approaches with, so they have to be discounted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of all the IDs in this dataset that have any relevant mapping at all to the biological questions.\n",
    "ids_with_any_mapping = list(set(flatten([\n",
    "    kegg_mapped_ids,\n",
    "    pmn_mapped_ids,\n",
    "    subsets_mapped_ids,\n",
    "    classes_mapped_ids,\n",
    "    string_edgelist.ids,\n",
    "    panther_edgelist.ids\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2602\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all the IDs in this dataset that have all of types of curated values we want to look at. \n",
    "go_mapped_ids = list(dataset.get_annotations_dictionary(ontology_name=\"GO\").keys())\n",
    "po_mapped_ids = list(dataset.get_annotations_dictionary(ontology_name=\"PO\").keys())\n",
    "ids_with_all_annotations = set(go_mapped_ids).intersection(set(po_mapped_ids)).intersection(set(ow_edgelist.ids))\n",
    "ids_with_all_annotations = list(ids_with_all_annotations)\n",
    "print(len(ids_with_all_annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>4385</td>\n",
       "      <td>3456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>466</td>\n",
       "      <td>353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>4985</td>\n",
       "      <td>3935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                     4385                 3456\n",
       "1     gmx                        5                    4\n",
       "2     mtr                       14                   14\n",
       "3     osa                       88                   81\n",
       "4     sly                       27                   27\n",
       "5     zma                      466                  353\n",
       "6   total                     4985                 3935"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.filter:\n",
    "    dataset.filter_with_ids(ids_with_any_mapping)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>523</td>\n",
       "      <td>467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mtr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>osa</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sly</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zma</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>total</td>\n",
       "      <td>600</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                      523                  467\n",
       "1     mtr                        1                    1\n",
       "2     osa                       13                   13\n",
       "3     sly                        2                    2\n",
       "4     zma                       61                   53\n",
       "5   total                      600                  536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if args.subset:\n",
    "    dataset.filter_random_k(args.subset)\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"phenotype_pairs\"></a>\n",
    "### Reading in the descriptions from hand-picked dataset of plant phenotype pairs\n",
    "See the other notebook for the creation of this dataset. This is included in this notebook instead of a separated notebook because we want the treatment of the individual phenotype text instances to be the same as is done for the descriptions from the real dataset of plant phenotypes. The list of computational approaches being evaluated for this task is the same in both cases so all of the cells between the point where the descriptions are read in and when the distance matrices are found using all those methods are the same for this task as any of the biological questions that this notebook is focused on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phenotype 1</th>\n",
       "      <th>Phenotype 2</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long hypocotyl under far-red light. partially ...</td>\n",
       "      <td>sensitive to uv-b light</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>embryo defective. seedling defective</td>\n",
       "      <td>embryo defective-cotyledon. embryo defective. ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>increased lateral root number. late flowering</td>\n",
       "      <td>insensitive to cytokinin. short roots</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embryo defective-globular. embryo defective. g...</td>\n",
       "      <td>embryo defective-transition. embryo defective....</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abnormal microtubule development. abnormal pav...</td>\n",
       "      <td>albino seedlings. pale green seedlings</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>albino. dwarf</td>\n",
       "      <td>homozygous plants have small and hyponastic co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pale green leaves. reduced rosette size</td>\n",
       "      <td>blunt siliques. short inflorescence stems</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pale green seedlings. pale green seeds</td>\n",
       "      <td>anthocyanin accumulation. dark-grown seedlings...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>embryo defective. seedling defective</td>\n",
       "      <td>abnormal rosette leaf morphology. decreased le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>large rosette leaves</td>\n",
       "      <td>abnormal leaf morphology. dwarf</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Phenotype 1                                        Phenotype 2  Score\n",
       "0  long hypocotyl under far-red light. partially ...                            sensitive to uv-b light      1\n",
       "1               embryo defective. seedling defective  embryo defective-cotyledon. embryo defective. ...      2\n",
       "2      increased lateral root number. late flowering              insensitive to cytokinin. short roots      1\n",
       "3  embryo defective-globular. embryo defective. g...  embryo defective-transition. embryo defective....      2\n",
       "4  abnormal microtubule development. abnormal pav...             albino seedlings. pale green seedlings      0\n",
       "5                                      albino. dwarf  homozygous plants have small and hyponastic co...      0\n",
       "6            pale green leaves. reduced rosette size          blunt siliques. short inflorescence stems      1\n",
       "7             pale green seedlings. pale green seeds  anthocyanin accumulation. dark-grown seedlings...      2\n",
       "8               embryo defective. seedling defective  abnormal rosette leaf morphology. decreased le...      0\n",
       "9                               large rosette leaves                    abnormal leaf morphology. dwarf      2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the table of similarity scored phenotype pairs that was prepared from random selection.\n",
    "num_pairs = 50\n",
    "mupdata = pd.read_csv(paired_phenotypes_path)\n",
    "assert num_pairs == mupdata.shape[0]\n",
    "\n",
    "# TODO do this in the dataset preprocessing rather than in this analysis notebook.\n",
    "mupdata[\"Phenotype 1\"] = mupdata[\"Phenotype 1\"].map(lambda x: x.replace(\";\", \".\"))\n",
    "mupdata[\"Phenotype 2\"] = mupdata[\"Phenotype 2\"].map(lambda x: x.replace(\";\", \".\"))\n",
    "paired_phenotypes = mupdata[\"Phenotype 1\"].values.tolist()\n",
    "paired_phenotypes.extend(mupdata[\"Phenotype 2\"].values.tolist())\n",
    "first_paired_id = 0\n",
    "paired_phenotypes = {i:description for i,description in enumerate(paired_phenotypes, first_paired_id)}\n",
    "pair_to_score = {(i,i+num_pairs):s for i,s in enumerate(mupdata[\"Score\"].values, first_paired_id)}\n",
    "paired_phenotype_ids = list(paired_phenotypes.keys())\n",
    "mupdata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"biosses\"></a>\n",
    "### Reading in a dataset of sentence pairs from the BIOSSES dataset\n",
    "The dataset that is loaded here is the set of a hundred sentence pairs that were scored for similarity by annotators, and the scores were averaged, from the BIOSSES paper. See the BIOSSES paper for how this dataset was constructed and what the similarity scores for the pairs of sentences mean. This cell sets the descriptions dictionary to contain these sentences, and creates other dictionaries for mapping each pair to itself and for mapping pairs to the scores that were assigned to them by annotators. This will be overwritten if running the notebook automatically as a script, and only matters if looking at this dataset by running this as an interactive notebook. For the analysis, this dataset was used as a means of comparing different hyperparameters that could be used over the plant (testing) data. This includes things like how many encoder layers of BERT to use for phenotype description embeddings, or how whether token vectors from Word2Vec should be combined using mean or max to yield document vectors.<a id=\"filtering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pair Number</th>\n",
       "      <th>Sentence 1</th>\n",
       "      <th>Sentence 2</th>\n",
       "      <th>Annotator A</th>\n",
       "      <th>Annotator B</th>\n",
       "      <th>Annotator C</th>\n",
       "      <th>Annotator D</th>\n",
       "      <th>Annotator E</th>\n",
       "      <th>Annotator Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>It has recently been shown that Craf is essent...</td>\n",
       "      <td>It has recently become evident that Craf is es...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Bcl-2 inhibitor ABT-737 induces regression...</td>\n",
       "      <td>Recently, it has been reported that ABT-737 is...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Previous studies demonstrated that the decreas...</td>\n",
       "      <td>In addition, genetic and functional studies su...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>More recently, IDH mutations and resultant 2-h...</td>\n",
       "      <td>It has also been recently reported that mutati...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Recent in vitro studies using shRNA-based appr...</td>\n",
       "      <td>Two recent studies used RNAi-mediated Tet2 kno...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Recently, it was reported that expression of I...</td>\n",
       "      <td>This large-scale study showed that IDH1/IDH2 m...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Recently, it was reported that expression of I...</td>\n",
       "      <td>the mechanism was clarified by yet another gen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>expression of an activated form of Ras protein...</td>\n",
       "      <td>When expressed alone in primary cells however,...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>A high percentage of tumor cells that take on ...</td>\n",
       "      <td>As a serine/threonine protein kinase, AKT func...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>In view of the evidence that many tumors occur...</td>\n",
       "      <td>The phenomena of neoplastic development and ne...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pair Number                                         Sentence 1                                         Sentence 2  Annotator A  Annotator B  Annotator C  Annotator D  Annotator E  Annotator Mean\n",
       "0            1  It has recently been shown that Craf is essent...  It has recently become evident that Craf is es...            4            4            4            4            4             4.0\n",
       "1            2  The Bcl-2 inhibitor ABT-737 induces regression...  Recently, it has been reported that ABT-737 is...            3            3            3            3            3             3.0\n",
       "2            3  Previous studies demonstrated that the decreas...  In addition, genetic and functional studies su...            2            2            3            2            2             2.2\n",
       "3            4  More recently, IDH mutations and resultant 2-h...  It has also been recently reported that mutati...            3            3            4            3            3             3.2\n",
       "4            5  Recent in vitro studies using shRNA-based appr...  Two recent studies used RNAi-mediated Tet2 kno...            3            3            4            3            3             3.2\n",
       "5            6  Recently, it was reported that expression of I...  This large-scale study showed that IDH1/IDH2 m...            3            3            4            3            3             3.2\n",
       "6            7  Recently, it was reported that expression of I...  the mechanism was clarified by yet another gen...            1            1            3            2            1             1.6\n",
       "7            8  expression of an activated form of Ras protein...  When expressed alone in primary cells however,...            3            3            3            3            3             3.0\n",
       "8            9  A high percentage of tumor cells that take on ...  As a serine/threonine protein kinase, AKT func...            2            1            1            2            1             1.4\n",
       "9           10  In view of the evidence that many tumors occur...  The phenomena of neoplastic development and ne...            3            3            3            3            3             3.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dataset of paired sentences from a dataset like the BIOSSES set of sentences pairs.\n",
    "num_pairs = 100\n",
    "mupdata = pd.read_csv(biosses_datset_path)\n",
    "assert num_pairs == mupdata.shape[0]\n",
    "\n",
    "biosses_sentences = mupdata[\"Sentence 1\"].values.tolist()\n",
    "biosses_sentences.extend(mupdata[\"Sentence 2\"].values.tolist())\n",
    "first_paired_id = 0\n",
    "biosses_sentences = {i:description for i,description in enumerate(biosses_sentences, first_paired_id)}\n",
    "pair_to_score = {(i,i+num_pairs):s for i,s in enumerate(mupdata[\"Annotator Mean\"].values, first_paired_id)}\n",
    "biosses_ids = list(biosses_sentences.keys())\n",
    "mupdata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"selecting_a_dataset\"></a>\n",
    "### Selecting which dataset should be used to proceed with the analysis\n",
    "The analysis is run over different datasets using this same notebook to avoid including lots of redundant code in the project. Therefore the dataset to use is set here within the notebook, even though some of the previous sections only apply to the main phenotypes dataset, which are the ones that aren't sentence pairs. The options here should match the datasets argument that can be specified when running the notebook here or as a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# Obtain a mapping between IDs and the raw text descriptions associated with that ID from the dataset.\n",
    "# The choice made in this cell determines what the dataset used for building the distance matrices is.\n",
    "# These two variables are referenced repeatedly throughout the notebook. If using the small datasets of\n",
    "# paired sentences instead, make the descriptions variable equal one of those, and ids to use as well.\n",
    "dataset_choice_to_descriptions_and_ids_to_use = {\n",
    "    \"plants\": (dataset.get_description_dictionary(), dataset.get_ids()),\n",
    "    \"diseases\": (clinvar_dataset.get_description_dictionary(), clinvar_dataset.get_ids()),\n",
    "    \"snippets\": (snpedia_snippets_dataset.get_description_dictionary(), snpedia_snippets_dataset.get_ids()),\n",
    "    \"contexts\": (snpedia_contexts_dataset.get_description_dictionary(), snpedia_contexts_dataset.get_ids()),\n",
    "    \"biosses\": (biosses_sentences, biosses_ids),\n",
    "    \"pairs\": (paired_phenotypes, paired_phenotype_ids),\n",
    "}\n",
    "descriptions, ids_to_use = dataset_choice_to_descriptions_and_ids_to_use[args.dataset]\n",
    "print(len(descriptions))\n",
    "print(len(ids_to_use))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_2\"></a>\n",
    "# Part 2. NLP Models\n",
    "\n",
    "\n",
    "<a id=\"word2vec_doc2vec\"></a>\n",
    "### Word2Vec and Doc2Vec\n",
    "Word2Vec is a word embedding technique using a neural network trained on a so-called *false task*, namely either predicting a missing word from within a sequence of context words drawn from a sentence or phrase, or predicting which contexts words surround some given input word drawn from a sentence or phrase. Each of these tasks are supervised (the correct answer is fixed and known), but can be generated from unlabeled text data such as a collection of books or Wikipedia articles, meaning that even though the task itself is supervised the training data can be generated automatically, enabling the creation of enormous training sets. The internal representation for particular words learned during the training process contain semantically informative features related to that given word, and can therefore be used as embeddings used downstream for tasks such as finding similarity between words or as input into additional models. Doc2Vec is an extension of this technique that determines vector embeddings for entire documents (strings containing multiple words, could be sentences, paragraphs, or documents).\n",
    "\n",
    "\n",
    "<a id=\"bert_biobert\"></a>\n",
    "### BERT and BioBERT\n",
    "BERT ('Bidirectional Encoder Representations from Transformers') is another neural network-based model trained on two different false tasks, namely predicting whether two sentences are consecutive, or predicting the identity of a set of words masked from an input sentence. Like Word2Vec, this architecture can be used to generate vector embeddings for a particular input word by extracting values from a subset of the encoder layers that correspond to that input word. Practically, a major difference is that because the input word is input in the context of its surrounding sentence, the embedding reflects the meaning of a particular word in a particular context (such as the difference in the meaning of *root* in the phrases *plant root* and *root of the problem*. BioBERT refers to a set of BERT models which have been finetuned on the PubMed and PMC corpora. See the list of relevant links for the publications and pages associated with these models.\n",
    "\n",
    "<a id=\"load_models\"></a>\n",
    "### Loading trained and saved models\n",
    "Versions of the architectures discussed above which have been saved as trained models are loaded here. Some of these models are loaded as pretrained models from the work of other groups, and some were trained on data specific to this notebook and loaded here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epochs = []\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "        self.deltas = []\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 1:\n",
    "            delta = loss\n",
    "        else:\n",
    "            delta = loss- self.loss_previous_step\n",
    "        self.loss_previous_step=loss\n",
    "        self.losses.append(loss)\n",
    "        self.epochs.append(self.epoch)\n",
    "        self.epoch += 1\n",
    "        self.deltas.append(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always load these Word2Vec models because they're needed for a bunch of different approaches.\n",
    "# Word2Vec models that were trained on corpora that range from most general to most domain specific.\n",
    "word2vec_wiki_model = gensim.models.Word2Vec.load(word2vec_wikipedia_path)\n",
    "word2vec_pubmed_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_bio_pubmed_path, binary=True)\n",
    "word2vec_plants_model = gensim.models.Word2Vec.load(word2vec_plants_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.bio_small or args.combined:\n",
    "    word2vec_bio_pmc_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_bio_pmc_path, binary=True)\n",
    "#if args.bio_large or args.combined:\n",
    "       #word2vec_bio_pubmed_and_pmc_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_bio_pubmed_and_pmc_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Doc2Vec models that were trained on English Wikipedia or from out plant phenotypes corpus.\n",
    "if args.learning:\n",
    "    doc2vec_wiki_model = gensim.models.Doc2Vec.load(doc2vec_wikipedia_path)\n",
    "    doc2vec_plants_model= gensim.models.Doc2Vec.load(doc2vec_plants_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.bert or args.biobert:\n",
    "    # Reading in BERT tokenizers that correspond to particular models.\n",
    "    bert_tokenizer_base = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    bert_tokenizer_pmc = BertTokenizer.from_pretrained(biobert_pmc_path)\n",
    "    bert_tokenizer_pubmed = BertTokenizer.from_pretrained(biobert_pubmed_path)\n",
    "    bert_tokenizer_pubmed_pmc = BertTokenizer.from_pretrained(biobert_pubmed_pmc_path)\n",
    "\n",
    "    # Reading in the BERT models themselves.\n",
    "    bert_model_base = BertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_model_pmc = BertModel.from_pretrained(biobert_pmc_path)\n",
    "    bert_model_pubmed = BertModel.from_pretrained(biobert_pubmed_path)\n",
    "    bert_model_pubmed_pmc = BertModel.from_pretrained(biobert_pubmed_pmc_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_3\"></a>\n",
    "# Part 3. NLP Choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a mapping between gene IDs and lists of some other type of ID that references a single object that was \n",
    "# somehow extracted from descriptions or annotations associated with that gene.\n",
    "gene_id_to_unique_ids_mappings = defaultdict(lambda: defaultdict(list))\n",
    "unique_id_to_gene_ids_mappings = defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to unique text strings for whole genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a mapping between a new unique identifier and unique description strings that are not sentence tokenized.\n",
    "unique_id_to_unique_text = {i:text for i,text in enumerate(list(set(descriptions.values())))}\n",
    "_reverse_of_that_mapping = {text:i for i,text in unique_id_to_unique_text.items()}\n",
    "\n",
    "# Get a mapping between the original gene IDs from this dataset and the corresponding ID for unique text strings.\n",
    "gene_id_to_unique_ids_mappings[\"whole_texts\"] = {i:[_reverse_of_that_mapping[text]] for i,text in descriptions.items()}\n",
    "whole_unique_ids = list(unique_id_to_unique_text.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping to unique text strings that have been tokenized by sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokenized_descriptions = {i:sent_tokenize(d) for i,d in descriptions.items()}\n",
    "unique_sents = list(set(flatten(sent_tokenized_descriptions.values())))\n",
    "largest_whole_unique_id = max(whole_unique_ids)\n",
    "unique_id_to_unique_sent = {i:text for i,text in enumerate(unique_sents,largest_whole_unique_id+1)}\n",
    "_reverse_of_that_mapping = {text:i for i,text in unique_id_to_unique_sent.items()}\n",
    "\n",
    "for i, sent_list in sent_tokenized_descriptions.items():\n",
    "    for sent in sent_list:\n",
    "        gene_id_to_unique_ids_mappings[\"sent_tokens\"][i].append(_reverse_of_that_mapping[sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establishing which dictionaries will be used for preprocessing text next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What should 'descriptions' be for the sake of doing batch pre-processing?\n",
    "# These are no longer whole phenotype descriptions strings, but rather a collection of strings that are considered\n",
    "# unique that can sent to the larger calculation step in order to make the realized pairwise distance matrices as \n",
    "# small as possible.\n",
    "descriptions = {}\n",
    "descriptions.update(unique_id_to_unique_text)\n",
    "descriptions.update(unique_id_to_unique_sent)\n",
    "assert len(descriptions) == len(unique_id_to_unique_text) + len(unique_id_to_unique_sent)\n",
    "\n",
    "# Which of the IDs that were created for unique text strings are for whole descriptions, and which are for sentences?\n",
    "unique_whole_ids = list(unique_id_to_unique_text.keys())\n",
    "unique_tokenized_ids = list(unique_id_to_unique_sent.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "### Preprocessing text descriptions\n",
    "The preprocessing methods applied to the phenotype descriptions are a choice which impacts the subsequent vectorization and similarity methods which construct the pairwise distance matrix from each of these descriptions. The preprocessing methods that make sense are also highly dependent on the vectorization method or embedding method that is to be applied. For example, stemming (which is part of the full preprocessing done below using the Gensim preprocessing function) is useful for the n-grams and bag-of-words methods but not for the document embeddings methods which need each token to be in the vocabulary that was constructed and used when the model was trained. For this reason, embedding methods with pretrained models where the vocabulary is fixed should have a lighter degree of preprocessing not involving stemming or lemmatization but should involve things like removal of non-alphanumerics and normalizing case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying canned preprocessing approaches to the descriptions.\n",
    "processed = defaultdict(dict)\n",
    "processed[\"simple\"] = {i:\" \".join(simple_preprocess(d)) for i,d in descriptions.items()}\n",
    "processed[\"simple_no_stops\"] = {i:remove_stopwords(\" \".join(simple_preprocess(d))) for i,d in descriptions.items()}\n",
    "processed[\"full\"] = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of stopwords, used later for checking it tokens in a list are stopwords or not.\n",
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pos_tagging\"></a>\n",
    "### POS tagging the phenotype descriptions for nouns and adjectives\n",
    "Note that preprocessing of the descriptions should be done after part-of-speech tagging, because tokens that are removed during preprocessing before n-gram analysis contain information that the parser needs to accurately call parts-of-speech. This step should be done on the raw descriptions and then the resulting bags of words can be subset using additional preprocessing steps before input in one of the vectorization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pos_tokens = lambda text,pos: \" \".join([t[0] for t in nltk.pos_tag(word_tokenize(text)) if t[1].lower()==pos.lower()])\n",
    "processed[\"nouns\"] =  {i:get_pos_tokens(d,\"NN\") for i,d in descriptions.items()}\n",
    "processed[\"nouns_full\"] = {i:\" \".join(preprocess_string(d)) for i,d in processed[\"nouns\"].items()}\n",
    "processed[\"nouns_simple\"] = {i:\" \".join(simple_preprocess(d)) for i,d in processed[\"nouns\"].items()}\n",
    "processed[\"adjectives\"] =  {i:get_pos_tokens(d,\"JJ\") for i,d in descriptions.items()}\n",
    "processed[\"adjectives_full\"] = {i:\" \".join(preprocess_string(d)) for i,d in processed[\"adjectives\"].items()}\n",
    "processed[\"adjectives_simple\"] = {i:\" \".join(simple_preprocess(d)) for i,d in processed[\"adjectives\"].items()}\n",
    "processed[\"nouns_adjectives\"] = {i:\"{} {}\".format(processed[\"nouns\"][i],processed[\"adjectives\"][i]) for i in descriptions.keys()}\n",
    "processed[\"nouns_adjectives_full\"] = {i:\"{} {}\".format(processed[\"nouns_full\"][i],processed[\"adjectives_full\"][i]) for i in descriptions.keys()}\n",
    "processed[\"nouns_adjectives_simple\"] = {i:\"{} {}\".format(processed[\"nouns_simple\"][i],processed[\"adjectives_simple\"][i]) for i in descriptions.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing vocabulary size based on identifying important words\n",
    "These approaches for reducing the vocabulary size of the dataset work by identifying which words in the descriptions are likely to be the most important for identifying differences between the phenotypes and meaning of the descriptions. One approach is to determine which words occur at a higher rate in text of interest such as articles about plant phenotypes as compared to their rates in more general texts such as a corpus of news articles. These approaches do not create modified versions of the descriptions but rather provide vocabulary objects that can be passed to the sklearn vectorizer or constructors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ontology objects for all the biological ontologies being used.\n",
    "# Or skip that step and load them as pickled python objects instead, which is much faster.\n",
    "pato = Ontology(pato_obo_path)\n",
    "po = Ontology(po_obo_path)\n",
    "go = Ontology(go_obo_path)\n",
    "#pato = load_from_pickle(pato_pickle_path)\n",
    "#po = load_from_pickle(po_pickle_path)\n",
    "#go = load_from_pickle(go_pickle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sets of tokens that are part of bio ontology term labels or synonyms.\n",
    "bio_ontology_tokens = list(set(po.tokens()).union(set(go.tokens())).union(set(pato.tokens())))\n",
    "bio_ontology_tokens = [t for t in bio_ontology_tokens if t not in stop_words]\n",
    "bio_ontology_tokens_simple = flatten([simple_preprocess(t) for t in bio_ontology_tokens])\n",
    "bio_ontology_tokens_full = flatten([preprocess_string(t) for t in bio_ontology_tokens])\n",
    "with open(os.path.join(OUTPUT_DIR, VOCABULARIES_DIR, \"bio_ontology_vocab_size_{}.txt\".format(len(bio_ontology_tokens))),\"w\") as f:\n",
    "    f.write(\" \".join(bio_ontology_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting sets of tokens that are overrepresented in plant phenotype papers as compared to some background corpus.\n",
    "#maximum_number_of_tokens = 5000\n",
    "background_corpus = open(background_corpus_filename,\"r\").read()\n",
    "phenotypes_corpus = open(phenotypes_corpus_filename,\"r\").read()\n",
    "ppp_overrepresented_tokens,_ = get_overrepresented_tokens(phenotypes_corpus, background_corpus, min_ratio=2)\n",
    "ppp_overrepresented_tokens = [t for t in ppp_overrepresented_tokens if t not in stop_words]\n",
    "ppp_overrepresented_tokens_simple = flatten([simple_preprocess(t) for t in ppp_overrepresented_tokens])\n",
    "ppp_overrepresented_tokens_full = flatten([preprocess_string(t) for t in ppp_overrepresented_tokens])\n",
    "with open(os.path.join(OUTPUT_DIR, VOCABULARIES_DIR, \"plant_phenotype_vocab_size_{}.txt\".format(len(ppp_overrepresented_tokens))), \"w\") as f:\n",
    "    f.write(\" \".join(ppp_overrepresented_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating processed description entries by subsetting tokens to only include ones from these vocabularies.\n",
    "ppp_overrepresented_tokens_full_set = set(ppp_overrepresented_tokens_full)\n",
    "bio_ontology_tokens_full_set = set(bio_ontology_tokens_full)\n",
    "processed[\"plant_overrepresented_tokens\"] = {i:\" \".join([token for token in word_tokenize(text) if token in ppp_overrepresented_tokens_full_set]) for i,text in processed[\"full\"].items()}\n",
    "processed[\"bio_ontology_tokens\"] = {i:\" \".join([token for token in word_tokenize(text) if token in bio_ontology_tokens_full_set]) for i,text in processed[\"full\"].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vocab\"></a>\n",
    "### Reducing the vocabulary size using a word distance matrix\n",
    "These approaches for reducing the vocabulary size of the dataset work by replacing multiple words that occur throughout the dataset of descriptions with an identical word that is representative of this larger group of words. The total number of unique words across all descriptions is therefore reduced, and when observing n-gram overlaps between vector representations of these descriptions, overlaps will now occur between descriptions that included different but similar words. These methods work by actually generating versions of these descriptions that have the word replacements present. The returned objects for these methods are the revised description dictionary, a dictionary mapping tokens in the full vocabulary to tokens in the reduced vocabulary, and a dictionary mapping tokens in the reduced vocabulary to a list of tokens in the full vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a pairwise distance matrix object using the oats subpackage, and create an appropriately shaped matrix,\n",
    "# making sure that the tokens list is in the same order as the indices representing each word in the distance matrix.\n",
    "# This is currently trivial because the IDs that are used are ordered integers 0 to n, but this might not always be\n",
    "# the case so it's not directly assumed here.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in processed[\"simple\"].values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.with_word2vec(word2vec_wiki_model, tokens_dict, \"cosine\")\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "\n",
    "# Now we have a list of tokens of length n, and the corresponding n by n distance matrix for looking up distances.\n",
    "# The other argument that the Linares Pontes algorithm needs is a value for n, see paper or description above\n",
    "# for an explanation of what that value is in the algorithm, and why values near 3 were a good fit.\n",
    "n = 3\n",
    "processed[\"linares_pontes_wikipedia\"], reduce_lp, unreduce_lp = reduce_vocab_linares_pontes(processed[\"simple\"], tokens, distance_matrix, n)\n",
    "\n",
    "\n",
    "# For the combined approach that yields semantic vectors.\n",
    "for_combined_distance_matrix_wiki = distance_matrix\n",
    "for_combined_tokens_wiki = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating to create the descriptions for collapsed vocabulary but using the embeddings trained on PubMed.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in processed[\"simple\"].values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.with_word2vec(word2vec_pubmed_model, tokens_dict, \"cosine\")\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "processed[\"linares_pontes_pubmed\"], reduce_lp, unreduce_lp = reduce_vocab_linares_pontes(processed[\"simple\"], tokens, distance_matrix, n)\n",
    "# For the combined approach that yields semantic vectors.\n",
    "for_combined_distance_matrix_pubmed = distance_matrix\n",
    "for_combined_tokens_pubmed = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating to create the descriptions for collapsed vocabulary but using the embeddings trained on our plant data.\n",
    "# NOTE that these use the fully preprocessed tokens not the simply preprocessed ones.\n",
    "tokens = list(set([w for w in flatten(d.split() for d in processed[\"full\"].values())]))\n",
    "tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "graph = pw.with_word2vec(word2vec_plants_model, tokens_dict, \"cosine\")\n",
    "distance_matrix = graph.array\n",
    "tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "n = 3\n",
    "processed[\"linares_pontes_plants\"], reduce_lp, unreduce_lp = reduce_vocab_linares_pontes(processed[\"full\"], tokens, distance_matrix, n)\n",
    "# For the combined approach that yields semantic vectors.\n",
    "for_combined_distance_matrix_plants = distance_matrix\n",
    "for_combined_tokens_plants = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the pairwise distance matrices for tokens for the combined approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the larger set of similarity matrices for the combined methods that use the simply preprocessed tokens.\n",
    "#tokens = list(set([w for w in flatten(d.split() for d in processed[\"simple\"].values())]))\n",
    "#tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "#for_combined_distance_matrix_wiki = pw.with_word2vec(word2vec_wiki_model, tokens_dict, \"cosine\").array\n",
    "#for_combined_distance_matrix_pubmed = pw.with_word2vec(word2vec_pubmed_model, tokens_dict, \"cosine\").array\n",
    "#for_combined_simple_tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "\n",
    "\n",
    "# Preparing the larger set of similarity matrices for the combined methods that use the fully preprocessed tokens.\n",
    "#tokens = list(set([w for w in flatten(d.split() for d in processed[\"full\"].values())]))\n",
    "#tokens_dict = {i:w for i,w in enumerate(tokens)}\n",
    "#for_combined_distance_matrix_wiki = pw.with_word2vec(word2vec_wiki_model, tokens_dict, \"cosine\").array\n",
    "#for_combined_distance_matrix_pubmed = pw.with_word2vec(word2vec_pubmed_model, tokens_dict, \"cosine\").array\n",
    "#for_combined_simple_tokens = [tokens_dict[graph.index_to_id[index]] for index in np.arange(distance_matrix.shape[0])]\n",
    "\n",
    "\n",
    "# Now we have a list of tokens of length n, and the corresponding n by n distance matrix for looking up distances.\n",
    "# Add the other distance matrices that you want there.\n",
    "# Use the processed[\"simple\"] descriptions for those methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"annotation\"></a>\n",
    "### Annotating descriptions with ontology terms\n",
    "This section generates dictionaries that map gene IDs from the dataset to lists of strings, where those strings are ontology term IDs. How the term IDs are found for each gene entry with its corresponding phenotype description depends on the cell below. Firstly, the terms are found by using the NOBLE Coder annotation tool through these wrapper functions to identify the terms by looking for instances of the term's label or synonyms in the actual text of the phenotype descriptions. Secondly, the next cell just draws the terms directly from the dataset itself. In this case, these are high-confidence annotations done by curators for a comparison against what can be accomplished through computational analysis of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "precise-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "go\n",
      "temp_output/nc_stdout.txt\n",
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "partial-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "go\n",
      "temp_output/nc_stdout.txt\n",
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "precise-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "po\n",
      "temp_output/nc_stdout.txt\n",
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "partial-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "po\n",
      "temp_output/nc_stdout.txt\n",
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "precise-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "pato\n",
      "temp_output/nc_stdout.txt\n",
      "temp_output/RESULTS.tsv\n",
      "temp_textfiles\n",
      "temp_output\n",
      "partial-match\n",
      "../lib/NobleCoder-1.0.jar\n",
      "pato\n",
      "temp_output/nc_stdout.txt\n"
     ]
    }
   ],
   "source": [
    "# Run the NOBLE Coder annotator over the raw input text descriptions, which handles things like case normalization.\n",
    "direct_annots_nc_go_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"go\", precise=1)\n",
    "direct_annots_nc_go_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"go\", precise=0)\n",
    "direct_annots_nc_po_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"po\", precise=1)\n",
    "direct_annots_nc_po_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"po\", precise=0)\n",
    "direct_annots_nc_pato_precise = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"pato\", precise=1)\n",
    "direct_annots_nc_pato_partial = annotate_using_noble_coder(descriptions, noblecoder_jarfile_path, \"pato\", precise=0)\n",
    "\n",
    "# Use the ontology hierarchies to add terms that are inherited by the terms that were annotated to the text.\n",
    "inherited_annots_nc_go_precise = {i:go.inherited(term_id_list) for i,term_id_list in direct_annots_nc_go_precise.items()}\n",
    "inherited_annots_nc_go_partial = {i:go.inherited(term_id_list) for i,term_id_list in direct_annots_nc_go_partial.items()}\n",
    "inherited_annots_nc_po_precise = {i:po.inherited(term_id_list) for i,term_id_list in direct_annots_nc_po_precise.items()}\n",
    "inherited_annots_nc_po_partial = {i:po.inherited(term_id_list) for i,term_id_list in direct_annots_nc_po_partial.items()}\n",
    "inherited_annots_nc_pato_precise = {i:pato.inherited(term_id_list) for i,term_id_list in direct_annots_nc_pato_precise.items()}\n",
    "inherited_annots_nc_pato_partial = {i:pato.inherited(term_id_list) for i,term_id_list in direct_annots_nc_pato_partial.items()}\n",
    "\n",
    "# Merge the ontology term annotations for each descritpion into a single dictionary for the precise and partial levels.\n",
    "all_precise_annotations = {i:flatten([inherited_annots_nc_go_precise[i],inherited_annots_nc_po_precise[i],inherited_annots_nc_pato_precise[i]]) for i in descriptions.keys()}\n",
    "all_partial_annotations = {i:flatten([inherited_annots_nc_go_partial[i],inherited_annots_nc_po_partial[i],inherited_annots_nc_pato_partial[i]]) for i in descriptions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating these sets of inherited ontology terms as tokens so that they can be used as n-grams.\n",
    "processed[\"precise_annotations\"] = {i:\" \".join(annots) for i,annots in all_precise_annotations.items()}\n",
    "processed[\"partial_annotations\"] = {i:\" \".join(annots) for i,annots in all_partial_annotations.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create description strings with all ontology term annotations concatenated to the end of the descriptions.\n",
    "processed[\"simple_plus_precise_annotations\"] = {i:\" \".join(flatten([text,all_precise_annotations[i]])) for i,text in processed[\"simple\"].items()}\n",
    "processed[\"simple_plus_partial_annotations\"] = {i:\" \".join(flatten([text,all_partial_annotations[i]])) for i,text in processed[\"simple\"].items()}\n",
    "processed[\"full_plus_precise_annotations\"] = {i:\" \".join(flatten([text,all_precise_annotations[i]])) for i,text in processed[\"full\"].items()}\n",
    "processed[\"full_plus_partial_annotations\"] = {i:\" \".join(flatten([text,all_partial_annotations[i]])) for i,text in processed[\"full\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ontology term annotations dictionaries for all the high confidence annotations present in the dataset.\n",
    "curated_go_annotations = dataset.get_annotations_dictionary(\"GO\")\n",
    "curated_po_annotations = dataset.get_annotations_dictionary(\"PO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize GO and PO curator annotated ontology terms and map from those to gene identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a mapping between GO term IDs (like GO:0001234) and the list of gene IDs in this dataset they were annotated to.\n",
    "go_term_to_gene_ids = defaultdict(list)\n",
    "for gene_id, term_list, in curated_go_annotations.items():\n",
    "    for term in term_list: \n",
    "        go_term_to_gene_ids[term].append(gene_id)\n",
    "        \n",
    "# Create a mapping between a new unique identifier for each unique term used and a list with one item, the given term.\n",
    "individual_curated_go_terms = {i:[t] for i,t in enumerate(go_term_to_gene_ids.keys())}  \n",
    "_reverse_mapping = {t[0]:i for i,t in individual_curated_go_terms.items()}\n",
    "gene_id_to_unique_ids_mappings[\"go_terms\"] = {i:[_reverse_mapping[t] for t in terms] for i,terms in curated_go_annotations.items()}\n",
    "\n",
    "# What about genes that don't have any GO terms annotated to them by a curator? That should be accounted for.\n",
    "if len(individual_curated_go_terms.keys()) > 0:\n",
    "    unique_id_for_emtpy_annotation_list = max(list(individual_curated_go_terms.keys()))+1\n",
    "else:\n",
    "    unique_id_for_emtpy_annotation_list = 0\n",
    "    \n",
    "individual_curated_go_terms[unique_id_for_emtpy_annotation_list] = []\n",
    "for gene_id,uid_list in gene_id_to_unique_ids_mappings[\"go_terms\"].items():\n",
    "    if len(uid_list) == 0:\n",
    "        gene_id_to_unique_ids_mappings[\"go_terms\"][gene_id].append(unique_id_for_emtpy_annotation_list)\n",
    "        \n",
    "        \n",
    "# Make the dictionary reflect inherited terms as well and be a string not a list.\n",
    "individual_curated_go_term_strings = {i:\" \".join(go.inherited(terms)) for i,terms in individual_curated_go_terms.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a mapping between PO term IDs (like PO:0001234) and the list of gene IDs in this dataset they were annotated to.\n",
    "po_term_to_gene_ids = defaultdict(list)\n",
    "for gene_id, term_list, in curated_po_annotations.items():\n",
    "    for term in term_list: \n",
    "        po_term_to_gene_ids[term].append(gene_id)\n",
    "        \n",
    "# Create a mapping between a new unique identifer for each unique term used and a list with one item, the given term.\n",
    "individual_curated_po_terms = {i:[t] for i,t in enumerate(po_term_to_gene_ids.keys())}  \n",
    "_reverse_mapping = {t[0]:i for i,t in individual_curated_po_terms.items()}\n",
    "gene_id_to_unique_ids_mappings[\"po_terms\"] = {i:[_reverse_mapping[t] for t in terms] for i,terms in curated_po_annotations.items()}\n",
    "\n",
    "# What about genes that don't have any GO terms annotated to them by a curator? That should be accounted for.\n",
    "if len(individual_curated_po_terms.keys()) > 0:\n",
    "    unique_id_for_emtpy_annotation_list = max(list(individual_curated_po_terms.keys()))+1\n",
    "else:\n",
    "    unique_id_for_emtpy_annotation_list = 0\n",
    "    \n",
    "individual_curated_po_terms[unique_id_for_emtpy_annotation_list] = []\n",
    "for gene_id,uid_list in gene_id_to_unique_ids_mappings[\"po_terms\"].items():\n",
    "    if len(uid_list) == 0:\n",
    "        gene_id_to_unique_ids_mappings[\"po_terms\"][gene_id].append(unique_id_for_emtpy_annotation_list)\n",
    "\n",
    "        \n",
    "# Make the dictionary reflect inherited terms as well and be a string not a list.\n",
    "individual_curated_po_term_strings = {i:\" \".join(po.inherited(terms)) for i,terms in individual_curated_po_terms.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about for the union set of GO and PO terms that were annotated by curators?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is obtain the set of unique term sets, with a mapping from/back to gene IDs, to avoid redundancy.\n",
    "curated_go_annotation_strings_sorted = {i:\" \".join(sorted(go.inherited(terms))) for i,terms in curated_go_annotations.items()}\n",
    "unique_go_annotation_set_strings = [s for s in list(set(curated_go_annotation_strings_sorted.values()))]\n",
    "unique_id_to_unique_go_annotation_strings = {i:s for i,s in enumerate(unique_go_annotation_set_strings)}\n",
    "_reverse_mapping = {s:i for i,s in unique_id_to_unique_go_annotation_strings.items()}\n",
    "gene_id_to_unique_ids_mappings[\"go_term_sets\"] = {i:[_reverse_mapping[s]] for i,s in curated_go_annotation_strings_sorted.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal here is to obtain the set of unique term sets, with a mapping from/back to gene IDs, to avoid redundancy.\n",
    "curated_po_annotation_strings_sorted = {i:\" \".join(sorted(po.inherited(terms))) for i,terms in curated_po_annotations.items()}\n",
    "unique_po_annotation_set_strings = [s for s in list(set(curated_po_annotation_strings_sorted.values()))]\n",
    "unique_id_to_unique_po_annotation_strings = {i:s for i,s in enumerate(unique_po_annotation_set_strings)}\n",
    "_reverse_mapping = {s:i for i,s in unique_id_to_unique_po_annotation_strings.items()}\n",
    "gene_id_to_unique_ids_mappings[\"po_term_sets\"] = {i:[_reverse_mapping[s]] for i,s in curated_po_annotation_strings_sorted.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"todo\"></a>\n",
    "### Splitting dictionaries back into phenotype and phene specific dictionaries\n",
    "As a preprocessing step, split into a new set of descriptions that's larger. Note that phenotypes are split into phenes, and the phenes that are identical are retained as separate entries in the dataset. This makes the distance matrix calculation more needlessly expensive, because vectors need to be found for the same string more than once, but it simplifies converting the edgelist back to having IDs that reference the genes (full phenotypes) instead of the smaller phenes. If anything, that problem should be addressed in the pairwise functions, not here. (The package should handle it, not when creating input data for those methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dictionaries that refer just to either unique raw whole texts, or unique raw sentences tokenized out.\n",
    "descriptions = unique_id_to_unique_text\n",
    "phenes = unique_id_to_unique_sent\n",
    "\n",
    "# Create the processed text dictionaries that have the same keys are those two, named accordingly for each.\n",
    "processes = list(processed.keys())\n",
    "unmerged = defaultdict(dict)\n",
    "for process,di in processed.items():\n",
    "    unmerged[process] = {i:text for i,text in di.items() if i in unique_whole_ids}\n",
    "    unmerged[\"{}_phenes\".format(process)] = {i:text for i,text in di.items() if i in unique_tokenized_ids}\n",
    "processed = unmerged\n",
    "\n",
    "# Checking to make sure the size of each dictionary is as expected.\n",
    "for process in processes:\n",
    "    assert len(unique_whole_ids) == len(processed[process].keys())\n",
    "    assert len(unique_tokenized_ids) == len(processed[\"{}_phenes\".format(process)].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These should be to sets not lists, don't need the duplicate references.\n",
    "for dtype,mapping in gene_id_to_unique_ids_mappings.items():\n",
    "    for gene_id,unique_ids in mapping.items():\n",
    "        gene_id_to_unique_ids_mappings[dtype][gene_id] = list(set(unique_ids))\n",
    "\n",
    "\n",
    "# What about the mapping from unique IDs of all kinds back to the gene IDs they came from?\n",
    "for dtype,mapping in gene_id_to_unique_ids_mappings.items():\n",
    "    for gene_id,unique_ids in mapping.items():\n",
    "        for unique_id in unique_ids:\n",
    "            unique_id_to_gene_ids_mappings[dtype][unique_id].append(gene_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the gene IDs should map to a list of exactly one ID referencing to a unique whole text, or set of terms.\n",
    "assert all([len(unique_ids)==1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"whole_texts\"].items()])\n",
    "assert all([len(unique_ids)==1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"go_term_sets\"].items()])\n",
    "assert all([len(unique_ids)==1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"po_term_sets\"].items()])\n",
    "\n",
    "# For the IDs that reference individual unique sentence tokens or ontology terms, a gene can map to one or more.\n",
    "assert all([len(unique_ids)>=1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"sent_tokens\"].items()])\n",
    "assert all([len(unique_ids)>=1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"go_terms\"].items()])\n",
    "assert all([len(unique_ids)>=1 for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"po_terms\"].items()])\n",
    "\n",
    "# In those cases, the list of IDs referencing unique terms of strings shouldn't contain any duplicates.\n",
    "assert all([len(unique_ids)==len(set(unique_ids)) for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"sent_tokens\"].items()])\n",
    "assert all([len(unique_ids)==len(set(unique_ids)) for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"go_terms\"].items()])\n",
    "assert all([len(unique_ids)==len(set(unique_ids)) for gene_id,unique_ids in gene_id_to_unique_ids_mappings[\"po_terms\"].items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_4\"></a>\n",
    "# Part 4. Generating vector representations and pairwise distances matrices\n",
    "This section uses the text descriptions, preprocessed text descriptions, or ontology term annotations created or read in the previous sections to generate a vector representation for each gene and build a pairwise distance matrix for the whole dataset. Each method specified is a unique combination of a method of vectorization (bag-of-words, n-grams, document embedding model, etc) and distance metric (Euclidean, Jaccard, cosine, etc) applied to those vectors in constructing the pairwise matrix. The method of vectorization here is equivalent to feature selection, so the task is to figure out which type of vectors will encode features that are useful (n-grams, full words, only words from a certain vocabulary, etc).\n",
    "\n",
    "<a id=\"methods\"></a>\n",
    "### Specifying a list of NLP methods to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['something in the dataset three times',\n",
       " 'something in the dataset three times',\n",
       " 'something in the dataset three times',\n",
       " 'something in the dataset only once']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of texts, this is necessary for weighting because inverse document frequency won't make sense\n",
    "# unless the texts that appear more than once in the actual dataset are actually account for, rather than treating\n",
    "# them as just one unique text (which is what is done as far as the distance matrix is concerned, in order to save\n",
    "# memory for the really large datasets like sentence tokens).\n",
    "def get_raw_texts_for_term_weighting(documents, unique_id_to_real_ids):\n",
    "    texts = flatten([[text]*len(unique_id_to_real_ids[i]) for i,text in documents.items()])\n",
    "    return(texts)\n",
    "\n",
    "# Quick test for the above method.\n",
    "test_unique_id_to_real_ids = {1:[1,2345,34564], 2:[1332]}\n",
    "test_documents = {1:\"something in the dataset three times\", 2:\"something in the dataset only once\"}\n",
    "get_raw_texts_for_term_weighting(test_documents, test_unique_id_to_real_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_and_word2vec_approaches = []\n",
    "if args.learning: doc2vec_and_word2vec_approaches.extend([\n",
    "    # Set of six approaches that all use the Word2Vec or Doc2Vec models trained on English Wikipedia.\n",
    "    Method(\"Doc2Vec\",\"Wikipedia,Size=300\",\"NLP\",11, pw.with_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Word2Vec\",\"Wikipedia,Size=300,Mean\",\"NLP\",12, pw.with_word2vec, {\"model\":word2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"Wikipedia,Size=300,Max\",\"NLP\",33 ,pw.with_word2vec, {\"model\":word2vec_wiki_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Doc2Vec\",\"Tokenization,Wikipedia,Size=300\",\"NLP\",1011, pw.with_doc2vec, {\"model\":doc2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,Wikipedia,Size=300,Mean\",\"NLP\",1012, pw.with_word2vec, {\"model\":word2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,Wikipedia,Size=300,Max\",\"NLP\",1013, pw.with_word2vec, {\"model\":word2vec_wiki_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    \n",
    "    # Another set of six approaches that all use the Word2Vec or Doc2Vec models trained on a plant phenotype corpus.\n",
    "    Method(\"Doc2Vec\",\"Plants,Size=300\",\"NLP\",1, pw.with_doc2vec, {\"model\":doc2vec_plants_model, \"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Word2Vec\",\"Plants,Size=300,Mean\",\"NLP\",2, pw.with_word2vec, {\"model\":word2vec_plants_model, \"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"Plants,Size=300,Max\",\"NLP\",3 ,pw.with_word2vec, {\"model\":word2vec_plants_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Doc2Vec\",\"Tokenization,Plants,Size=300\",\"NLP\",4, pw.with_doc2vec, {\"model\":doc2vec_plants_model, \"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,Plants,Size=300,Mean\",\"NLP\",5, pw.with_word2vec, {\"model\":word2vec_plants_model, \"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,Plants,Size=300,Max\",\"NLP\",6, pw.with_word2vec, {\"model\":word2vec_plants_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_nlp_approaches_small = []\n",
    "if args.bio_small: bio_nlp_approaches_small.extend([\n",
    "    # Set of six approaches that all use the Word2Vec or Doc2Vec models trained on English Wikipedia.\n",
    "    #Method(\"Word2Vec\",\"PMC,Size=200,Mean\",\"NLP\",14, pw.with_word2vec, {\"model\":word2vec_bio_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"PMC,Size=200,Max\",\"NLP\",15 ,pw.with_word2vec, {\"model\":word2vec_bio_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,PMC,Size=200,Mean\",\"NLP\",1014, pw.with_word2vec, {\"model\":word2vec_bio_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,PMC,Size=200,Max\",\"NLP\",1015, pw.with_word2vec, {\"model\":word2vec_bio_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    \n",
    "    # Another set of six approaches that all use the Word2Vec or Doc2Vec models trained on a plant phenotype corpus.\n",
    "    Method(\"Word2Vec\",\"PubMed,Size=200,Mean\",\"NLP\",16, pw.with_word2vec, {\"model\":word2vec_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"PubMed,Size=200,Max\",\"NLP\",17 ,pw.with_word2vec, {\"model\":word2vec_bio_pubmed_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,PubMed,Size=200,Mean\",\"NLP\",1016, pw.with_word2vec, {\"model\":word2vec_pubmed_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,PubMed,Size=200,Max\",\"NLP\",1017, pw.with_word2vec, {\"model\":word2vec_bio_pubmed_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_utils.Method at 0x15008a8256a0>, <_utils.Method at 0x15008a8258d0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_nlp_approaches_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_nlp_approaches_large = []\n",
    "if args.bio_large: bio_nlp_approaches_large.extend([\n",
    "    # Set of six approaches that all use the Word2Vec or Doc2Vec models trained on English Wikipedia.\n",
    "    Method(\"Word2Vec\",\"PubMed_PMC,Size=300,Mean\",\"NLP\",18, pw.with_word2vec, {\"model\":word2vec_bio_pubmed_and_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"PubMed_PMC,Size=300,Max\",\"NLP\",19 ,pw.with_word2vec, {\"model\":word2vec_bio_pubmed_and_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,PubMed_PMC,Size=200,Mean\",\"NLP\",1018, pw.with_word2vec, {\"model\":word2vec_bio_pubmed_and_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,PubMed_PMC,Size=200,Max\",\"NLP\",1019, pw.with_word2vec, {\"model\":word2vec_bio_pubmed_and_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    \n",
    "    # Another set of six approaches that all use the Word2Vec or Doc2Vec models trained on a plant phenotype corpus.\n",
    "    Method(\"Word2Vec\",\"Wiki_PubMed_PMC,Size=200,Mean\",\"NLP\",20, pw.with_word2vec, {\"model\":word2vec_bio_wikipedia_pubmed_and_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"Word2Vec\",\"Wiki_PubMed_PMC,Size=200,Max\",\"NLP\",21 ,pw.with_word2vec, {\"model\":word2vec_bio_wikipedia_pubmed_and_pmc_model, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,Wiki_PubMed_PMC,Size=200,Mean\",\"NLP\",1020, pw.with_word2vec, {\"model\":word2vec_bio_wikipedia_pubmed_and_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"Word2Vec\",\"Tokenization,Wiki_PubMed_PMC,Size=200,Max\",\"NLP\",1021, pw.with_word2vec, {\"model\":word2vec_bio_wikipedia_pubmed_and_pmc_model, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"max\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_nlp_approaches_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_approaches = []\n",
    "if args.combined: combined_approaches.extend([ \n",
    "    Method(\"Combined\",\"Wikipedia\",\"NLP\",43, pw.with_similarities, {\"ids_to_texts\":processed[\"simple\"],\"vocab_tokens\":for_combined_tokens_wiki,\"vocab_matrix\":for_combined_distance_matrix_wiki,\"model\":word2vec_wiki_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Combined\",\"PubMed\",\"NLP\",44, pw.with_similarities, {\"ids_to_texts\":processed[\"simple\"],\"vocab_tokens\":for_combined_tokens_pubmed,\"vocab_matrix\":for_combined_distance_matrix_pubmed,\"model\":word2vec_pubmed_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Combined\",\"Plants\",\"NLP\",45, pw.with_similarities, {\"ids_to_texts\":processed[\"full\"],\"vocab_tokens\":for_combined_tokens_plants,\"vocab_matrix\":for_combined_distance_matrix_plants,\"model\":word2vec_plants_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Combined\",\"Tokenization,Wikipedia\",\"NLP\",1043, pw.with_similarities, {\"ids_to_texts\":processed[\"simple_phenes\"],\"vocab_tokens\":for_combined_tokens_wiki,\"vocab_matrix\":for_combined_distance_matrix_wiki,\"model\":word2vec_wiki_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Combined\",\"Tokenization,PubMed\",\"NLP\",1044, pw.with_similarities, {\"ids_to_texts\":processed[\"simple_phenes\"],\"vocab_tokens\":for_combined_tokens_pubmed,\"vocab_matrix\":for_combined_distance_matrix_pubmed,\"model\":word2vec_pubmed_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Combined\",\"Tokenization,Plants\",\"NLP\",1045, pw.with_similarities, {\"ids_to_texts\":processed[\"full_phenes\"],\"vocab_tokens\":for_combined_tokens_plants,\"vocab_matrix\":for_combined_distance_matrix_plants,\"model\":word2vec_plants_model,\"metric\":\"cosine\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_utils.Method at 0x15008a940c50>,\n",
       " <_utils.Method at 0x15008a820e10>,\n",
       " <_utils.Method at 0x15008a820550>,\n",
       " <_utils.Method at 0x15008a820630>,\n",
       " <_utils.Method at 0x15008a820da0>,\n",
       " <_utils.Method at 0x15008a820978>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_approaches = []\n",
    "if args.baseline: baseline_approaches.extend([ \n",
    "    Method(\"Baseline\",\"Identity\",\"NLP\",0, pw.with_identity, {\"ids_to_texts\":descriptions}, None, tag=\"whole_texts\"),\n",
    "    Method(\"Baseline\",\"Tokenization,Identity\",\"NLP\",1000, pw.with_identity, {\"ids_to_texts\":phenes}, None, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_utils.Method at 0x15008a8236a0>, <_utils.Method at 0x15008a823898>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_approaches = []\n",
    "if args.bert: bert_approaches.extend([ \n",
    "    #Method(\"BERT\", \"Base,Layers=2,Concatenated\",\"NLP\",22, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"BERT\", \"Base,Layers=3,Concatenated\",\"NLP\",23, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BERT\", \"Base,Layers=4,Concatenated\",\"NLP\",24, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BERT\", \"Base,Layers=2,Summed\",\"NLP\",25, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BERT\", \"Base,Layers=3,Summed\",\"NLP\",26, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BERT\", \"Base,Layers=4,Summed\",\"NLP\",27, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "\n",
    "    #Method(\"BERT\", \"Tokenization,Base:Layers=2,Concatenated\",\"NLP\",2022, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BERT\", \"Tokenization,Base:Layers=3,Concatenated\",\"NLP\",2023, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BERT\", \"Tokenization,Base:Layers=4,Concatenated\",\"NLP\",2024, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BERT\", \"Tokenization,Base:Layers=2,Summed\",\"NLP\",2025, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BERT\", \"Tokenization,Base:Layers=3,Summed\",\"NLP\",2026, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"BERT\", \"Tokenization,Base:Layers=4,Summed\",\"NLP\",1027, pw.with_bert, {\"model\":bert_model_base, \"tokenizer\":bert_tokenizer_base, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_utils.Method at 0x15008a7d29e8>, <_utils.Method at 0x15008a7d2978>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_approaches = []\n",
    "if args.biobert: biobert_approaches.extend([ \n",
    "    Method(\"BioBERT\", \"PubMed,PMC,Layers=2,Concatenated\",\"NLP\",28, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=3,Concatenated\",\"NLP\",29, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Concatenated\",\"NLP\",30, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=2,Summed\",\"NLP\",31, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=3,Summed\",\"NLP\",32, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"BioBERT\", \"PubMed,PMC,Layers=4,Summed\",\"NLP\",33, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":descriptions, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    " \n",
    "    #Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=2,Concatenated\",\"NLP\",1028, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":2}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=3,Concatenated\",\"NLP\",1029, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":3}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=4,Concatenated\",\"NLP\",1030, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"concat\", \"layers\":4}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=2,Summed\",\"NLP\",1031, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":2}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=3,Summed\",\"NLP\",1032, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":3}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"BioBERT\", \"Tokenization,PubMed,PMC,Layers=4,Summed\",\"NLP\",1033, pw.with_bert, {\"model\":bert_model_pubmed_pmc, \"tokenizer\":bert_tokenizer_pubmed_pmc, \"ids_to_texts\":phenes, \"metric\":\"cosine\", \"method\":\"sum\", \"layers\":4}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_annotation_approaches = []\n",
    "if args.noblecoder: automated_annotation_approaches.extend([    \n",
    "    Method(\"NOBLE Coder\",\"Precise,TFIDF\",\"NLP\",36, pw.with_ngrams, {\"ids_to_texts\":processed[\"precise_annotations\"], \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"metric\":\"cosine\", \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"precise_annotations\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"NOBLE Coder\",\"Partial,TFIDF\",\"NLP\",37, pw.with_ngrams, {\"ids_to_texts\":processed[\"partial_annotations\"], \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"metric\":\"cosine\", \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"partial_annotations\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),    \n",
    "    Method(\"NOBLE Coder\",\"Tokenization,Precise,TFIDF\",\"NLP\",1036, pw.with_ngrams, {\"ids_to_texts\":processed[\"precise_annotations_phenes\"], \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"metric\":\"cosine\", \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"precise_annotations_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"NOBLE Coder\",\"Tokenization,Partial,TFIDF\",\"NLP\",1037, pw.with_ngrams, {\"ids_to_texts\":processed[\"partial_annotations_phenes\"], \"binary\":True, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"metric\":\"cosine\", \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"partial_annotations_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_topic_modeling_approaches = []\n",
    "if args.nmf: nmf_topic_modeling_approaches.extend([\n",
    "    Method(\"Topic Modeling\",\"NMF,Full,Topics=50\",\"NLP\",9, pw.with_topic_model, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"nmf\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Topic Modeling\",\"NMF,Full,Topics=100\",\"NLP\",10, pw.with_topic_model, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"nmf\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Topic Modeling\",\"Tokenization,NMF,Full,Topics=50\",\"NLP\",1009, pw.with_topic_model, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"nmf\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Topic Modeling\",\"Tokenization,NMF,Full,Topics=100\",\"NLP\",1010,  pw.with_topic_model, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"nmf\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topic_modeling_approaches = []\n",
    "if args.lda: lda_topic_modeling_approaches.extend([\n",
    "    Method(\"Topic Modeling\", \"LDA,Full,Topics=50\",\"NLP\",7, pw.with_topic_model, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"lda\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Topic Modeling\", \"LDA,Full,Topics=100\",\"NLP\",8, pw.with_topic_model, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"lda\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"Topic Modeling\", \"Tokenization,LDA,Full,Topics=50\",\"NLP\",1007, pw.with_topic_model, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"num_topics\":50, \"algorithm\":\"lda\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Topic Modeling\", \"Tokenization,LDA,Full,Topics=100\",\"NLP\",1008, pw.with_topic_model, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"num_topics\":100, \"algorithm\":\"lda\", \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_ngrams_approaches = []\n",
    "if args.vanilla: vanilla_ngrams_approaches.extend([\n",
    "    Method(\"N-Grams\",\"Full,Words,1-grams,TFIDF\",\"NLP\",1,pw.with_ngrams, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Full,Words,1-grams,2-grams,TFIDF\",\"NLP\",2, pw.with_ngrams, {\"ids_to_texts\":processed[\"full\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Full,Words,1-grams,TFIDF\",\"NLP\",1001, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Full,Words,1-grams,2-grams,TFIDF\",\"NLP\",1002, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,2), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "collapsed_approaches = []\n",
    "if args.collapsed: collapsed_approaches.extend([\n",
    "    Method(\"N-Grams\",\"Linares_Pontes,Wikipedia,Words,1-grams\",\"NLP\",40, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_wikipedia\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_wikipedia\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Linares_Pontes,PubMed,Words,1-grams\",\"NLP\",41, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_pubmed\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1),\"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_pubmed\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Linares_Pontes,Plants,Words,1-grams\",\"NLP\",42, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_plants\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_plants\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Linares_Pontes,Wikipedia,Words,1-grams\",\"NLP\",1040, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_wikipedia_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1),  \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_wikipedia_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Linares_Pontes,PubMed,Words,1-grams\",\"NLP\",1041, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_pubmed_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_pubmed_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Linares_Pontes,Plants,Words,1-grams\",\"NLP\",1042, pw.with_ngrams, {\"ids_to_texts\":processed[\"linares_pontes_plants_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1),  \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"linares_pontes_plants_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_vocab_approaches = []\n",
    "if args.vocab: modified_vocab_approaches.extend([\n",
    "    #Method(\"N-Grams\",\"Full,Nouns,Adjectives,1-grams\",\"NLP\",3, pw.with_ngrams, {\"ids_to_texts\":processed[\"nouns_adjectives_full\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"nouns_adjectives_full\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    Method(\"N-Grams\",\"Full,Plant Overrepresented Tokens,1-grams\",\"NLP\",4, pw.with_ngrams, {\"ids_to_texts\":processed[\"plant_overrepresented_tokens\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"plant_overrepresented_tokens\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"N-Grams\",\"Full,Bio Ontology Tokens,1-grams\",\"NLP\",5, pw.with_ngrams, {\"ids_to_texts\":processed[\"bio_ontology_tokens\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"bio_ontology_tokens\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    " \n",
    "    #Method(\"N-Grams\",\"Tokenization,Full,Nouns,Adjectives,1-grams\",\"NLP\",1003, pw.with_ngrams, {\"ids_to_texts\":processed[\"nouns_adjectives_full_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True,  \"training_texts\":get_raw_texts_for_term_weighting(processed[\"nouns_adjectives_full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"N-Grams\",\"Tokenization,Full,Plant Overrepresented Tokens,1-grams\",\"NLP\",1004, pw.with_ngrams, {\"ids_to_texts\":processed[\"plant_overrepresented_tokens_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"plant_overrepresented_tokens_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"N-Grams\",\"Tokenization,Full,Bio Ontology Tokens,1-grams\",\"NLP\",1005, pw.with_ngrams, {\"ids_to_texts\":processed[\"bio_ontology_tokens_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"bio_ontology_tokens_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    \n",
    "    #Method(\"N-Grams\",\"Full,Precise_Annotations,Words,1-grams\",\"NLP\",38, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_plus_precise_annotations\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_plus_precise_annotations\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"N-Grams\",\"Full,Partial_Annotations,Words,1-grams\",\"NLP\",39, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_plus_partial_annotations\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_plus_partial_annotations\"], unique_id_to_gene_ids_mappings[\"whole_texts\"])}, spatial.distance.cosine, tag=\"whole_texts\"),\n",
    "    #Method(\"N-Grams\",\"Tokenization,Full,Precise_Annotations,Words,1-grams\",\"NLP\",1038, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_plus_precise_annotations_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_plus_precise_annotations_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    #Method(\"N-Grams\",\"Tokenization,Full,Partial_Annotations,Words,1-grams\",\"NLP\",1039, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_plus_partial_annotations_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_plus_partial_annotations_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done splitting by aspect\n"
     ]
    }
   ],
   "source": [
    "# Subsetting annotations to get aspect specific scores.\n",
    "cc_id = \"GO:0005575\"\n",
    "bp_id = \"GO:0008150\"\n",
    "mf_id = \"GO:0003674\"\n",
    "individual_curated_go_term_strings_cc = {}\n",
    "individual_curated_go_term_strings_bp = {}\n",
    "individual_curated_go_term_strings_mf = {}\n",
    "for k,v in individual_curated_go_term_strings.items():\n",
    "    individual_curated_go_term_strings_cc[k] = \" \".join([x for x in v.split() if cc_id in go.inherited(x)])\n",
    "    individual_curated_go_term_strings_bp[k] = \" \".join([x for x in v.split() if bp_id in go.inherited(x)])\n",
    "    individual_curated_go_term_strings_mf[k] = \" \".join([x for x in v.split() if mf_id in go.inherited(x)])\n",
    "     \n",
    "unique_id_to_unique_go_annotation_strings_cc = {}\n",
    "unique_id_to_unique_go_annotation_strings_bp = {}\n",
    "unique_id_to_unique_go_annotation_strings_mf = {}\n",
    "for k,v in individual_curated_go_term_strings.items():\n",
    "    unique_id_to_unique_go_annotation_strings_cc[k] = \" \".join([x for x in v.split() if cc_id in go.inherited(x)])\n",
    "    unique_id_to_unique_go_annotation_strings_bp[k] = \" \".join([x for x in v.split() if bp_id in go.inherited(x)])\n",
    "    unique_id_to_unique_go_annotation_strings_mf[k] = \" \".join([x for x in v.split() if mf_id in go.inherited(x)])\n",
    "print(\"done splitting by aspect\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "go_lists = {k:v.split() for k,v in unique_id_to_unique_go_annotation_strings.items()}\n",
    "go_cc_lists = {k:v.split() for k,v in unique_id_to_unique_go_annotation_strings_cc.items()}\n",
    "go_bp_lists = {k:v.split() for k,v in unique_id_to_unique_go_annotation_strings_bp.items()}\n",
    "go_mf_lists = {k:v.split() for k,v in unique_id_to_unique_go_annotation_strings_mf.items()}\n",
    "po_lists = {k:v.split() for k,v in unique_id_to_unique_po_annotation_strings.items()}\n",
    "\n",
    "ic_annotation_approaches = []\n",
    "if args.ic: ic_annotation_approaches.extend([\n",
    "    Method(\"GO\",\"IC\",\"Curated\",9001, pw.with_annotations_special_case, {\"ids_to_annotations\":go_lists, \"ontology\":go, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    #Method(\"GO CC\",\"IC\",\"Curated\",9002, pw.with_annotations_special_case, {\"ids_to_annotations\":go_cc_lists, \"ontology\":go, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    #Method(\"GO BP\",\"IC\",\"Curated\",9003, pw.with_annotations_special_case, {\"ids_to_annotations\":go_bp_lists, \"ontology\":go, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    #Method(\"GO MF\",\"IC\",\"Curated\",9004, pw.with_annotations_special_case, {\"ids_to_annotations\":go_mf_lists, \"ontology\":go, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    Method(\"PO\",\"IC\",\"Curated\",9005, pw.with_annotations_special_case, {\"ids_to_annotations\":po_lists, \"ontology\":po, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":False}, spatial.distance.cosine, tag=\"po_term_sets\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_annotation_approaches = []\n",
    "if args.annotations: manual_annotation_approaches.extend([\n",
    "    \n",
    "    Method(\"GO\",\"Union\",\"Curated\",2001, pw.with_ngrams, {\"ids_to_texts\":unique_id_to_unique_go_annotation_strings,  \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(unique_id_to_unique_go_annotation_strings, unique_id_to_gene_ids_mappings[\"go_term_sets\"])}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    Method(\"PO\",\"Union\",\"Curated\",2002, pw.with_ngrams, {\"ids_to_texts\":unique_id_to_unique_po_annotation_strings, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(unique_id_to_unique_po_annotation_strings, unique_id_to_gene_ids_mappings[\"po_term_sets\"])}, spatial.distance.cosine, tag=\"po_term_sets\"),\n",
    "    Method(\"GO\",\"Minimum\",\"Curated\",2003, pw.with_ngrams, {\"ids_to_texts\":individual_curated_go_term_strings, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(individual_curated_go_term_strings, unique_id_to_gene_ids_mappings[\"go_terms\"])}, spatial.distance.cosine, tag=\"go_terms\"),\n",
    "    Method(\"PO\",\"Minimum\",\"Curated\",2004, pw.with_ngrams, {\"ids_to_texts\":individual_curated_po_term_strings, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(individual_curated_po_term_strings, unique_id_to_gene_ids_mappings[\"po_terms\"])},spatial.distance.cosine, tag=\"po_terms\"),\n",
    "    \n",
    "    Method(\"GO CC\",\"Union\",\"Curated\",5001, pw.with_ngrams, {\"ids_to_texts\":unique_id_to_unique_go_annotation_strings_cc,  \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(unique_id_to_unique_go_annotation_strings, unique_id_to_gene_ids_mappings[\"go_term_sets\"])}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    Method(\"GO BP\",\"Union\",\"Curated\",5002, pw.with_ngrams, {\"ids_to_texts\":unique_id_to_unique_go_annotation_strings_bp,  \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(unique_id_to_unique_go_annotation_strings, unique_id_to_gene_ids_mappings[\"go_term_sets\"])}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    Method(\"GO MF\",\"Union\",\"Curated\",5003, pw.with_ngrams, {\"ids_to_texts\":unique_id_to_unique_go_annotation_strings_mf,  \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(unique_id_to_unique_go_annotation_strings, unique_id_to_gene_ids_mappings[\"go_term_sets\"])}, spatial.distance.cosine, tag=\"go_term_sets\"),\n",
    "    \n",
    "    Method(\"GO CC\",\"Minimum\",\"Curated\",6001, pw.with_ngrams, {\"ids_to_texts\":individual_curated_go_term_strings_cc, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(individual_curated_go_term_strings, unique_id_to_gene_ids_mappings[\"go_terms\"])}, spatial.distance.cosine, tag=\"go_terms\"),\n",
    "    Method(\"GO BP\",\"Minimum\",\"Curated\",6002, pw.with_ngrams, {\"ids_to_texts\":individual_curated_go_term_strings_bp, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(individual_curated_go_term_strings, unique_id_to_gene_ids_mappings[\"go_terms\"])}, spatial.distance.cosine, tag=\"go_terms\"),\n",
    "    Method(\"GO MF\",\"Minimum\",\"Curated\",6003, pw.with_ngrams, {\"ids_to_texts\":individual_curated_go_term_strings_mf, \"metric\":\"cosine\", \"min_df\":2, \"max_df\":0.9, \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(individual_curated_go_term_strings, unique_id_to_gene_ids_mappings[\"go_terms\"])}, spatial.distance.cosine, tag=\"go_terms\"),\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_app_approaches = []\n",
    "if args.app_only: for_app_approaches.extend([\n",
    "    Method(\"N-Grams\",\"Tokenization,Full,Words,1-grams,TFIDF\",\"NLP\",1001, pw.with_ngrams, {\"ids_to_texts\":processed[\"full_phenes\"], \"metric\":\"cosine\", \"binary\":False, \"analyzer\":\"word\", \"ngram_range\":(1,1), \"max_features\":10000, \"min_df\":2, \"max_df\":0.9, \"tfidf\":True, \"training_texts\":get_raw_texts_for_term_weighting(processed[\"full_phenes\"], unique_id_to_gene_ids_mappings[\"sent_tokens\"])}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "    Method(\"Word2Vec\",\"Tokenization,Plants,Size=300,Mean\",\"NLP\",5, pw.with_word2vec, {\"model\":word2vec_plants_model, \"ids_to_texts\":processed[\"linares_pontes_plants_phenes\"], \"metric\":\"cosine\", \"method\":\"mean\"}, spatial.distance.cosine, tag=\"sent_tokens\"),\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding lists of approaches to the complete set to be run, this is useful when running the notebook as a script.\n",
    "methods = []\n",
    "if args.combined: methods.extend(combined_approaches)\n",
    "if args.baseline: methods.extend(baseline_approaches)\n",
    "if args.learning: methods.extend(doc2vec_and_word2vec_approaches)\n",
    "if args.bert: methods.extend(bert_approaches)\n",
    "if args.biobert: methods.extend(biobert_approaches)\n",
    "if args.bio_small: methods.extend(bio_nlp_approaches_small)\n",
    "if args.bio_large: methods.extend(bio_nlp_approaches_large)\n",
    "if args.noblecoder: methods.extend(automated_annotation_approaches)\n",
    "if args.nmf: methods.extend(nmf_topic_modeling_approaches)\n",
    "if args.lda: methods.extend(lda_topic_modeling_approaches)\n",
    "if args.vanilla: methods.extend(vanilla_ngrams_approaches)\n",
    "if args.vocab: methods.extend(modified_vocab_approaches)\n",
    "if args.collapsed: methods.extend(collapsed_approaches) \n",
    "if args.annotations: methods.extend(manual_annotation_approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding lists of approaches to the complete set to be run, this is useful when running the notebook as a script.\n",
    "methods = []\n",
    "methods.extend(combined_approaches)\n",
    "methods.extend(baseline_approaches)\n",
    "methods.extend(doc2vec_and_word2vec_approaches)\n",
    "methods.extend(bert_approaches)\n",
    "methods.extend(biobert_approaches)\n",
    "methods.extend(bio_nlp_approaches_small)\n",
    "methods.extend(bio_nlp_approaches_large)\n",
    "methods.extend(automated_annotation_approaches)\n",
    "methods.extend(nmf_topic_modeling_approaches)\n",
    "methods.extend(lda_topic_modeling_approaches)\n",
    "methods.extend(vanilla_ngrams_approaches)\n",
    "methods.extend(modified_vocab_approaches)\n",
    "methods.extend(collapsed_approaches) \n",
    "methods.extend(manual_annotation_approaches)\n",
    "methods.extend(ic_annotation_approaches)\n",
    "methods.extend(for_app_approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<_utils.Method at 0x15008a940c50>,\n",
       " <_utils.Method at 0x15008a820e10>,\n",
       " <_utils.Method at 0x15008a820550>,\n",
       " <_utils.Method at 0x15008a820630>,\n",
       " <_utils.Method at 0x15008a820da0>,\n",
       " <_utils.Method at 0x15008a820978>,\n",
       " <_utils.Method at 0x15008a8236a0>,\n",
       " <_utils.Method at 0x15008a823898>,\n",
       " <_utils.Method at 0x15008a7d29e8>,\n",
       " <_utils.Method at 0x15008a7d2978>,\n",
       " <_utils.Method at 0x15008a7d2208>,\n",
       " <_utils.Method at 0x15008a7d2518>,\n",
       " <_utils.Method at 0x15008a8256a0>,\n",
       " <_utils.Method at 0x15008a8258d0>,\n",
       " <_utils.Method at 0x15008a889940>,\n",
       " <_utils.Method at 0x15008a889fd0>,\n",
       " <_utils.Method at 0x15008a889710>,\n",
       " <_utils.Method at 0x15008a889dd8>,\n",
       " <_utils.Method at 0x15008a812240>,\n",
       " <_utils.Method at 0x15008a823198>,\n",
       " <_utils.Method at 0x15008a823eb8>,\n",
       " <_utils.Method at 0x15008a823dd8>,\n",
       " <_utils.Method at 0x15008a8237f0>,\n",
       " <_utils.Method at 0x15008a823da0>,\n",
       " <_utils.Method at 0x15008a823a90>,\n",
       " <_utils.Method at 0x15008a8233c8>,\n",
       " <_utils.Method at 0x15008a887c18>,\n",
       " <_utils.Method at 0x15008a887470>,\n",
       " <_utils.Method at 0x15008a68c2b0>,\n",
       " <_utils.Method at 0x15008a68c710>,\n",
       " <_utils.Method at 0x15008a68cda0>,\n",
       " <_utils.Method at 0x15008a812278>,\n",
       " <_utils.Method at 0x15008a68c080>,\n",
       " <_utils.Method at 0x15008a68cfd0>,\n",
       " <_utils.Method at 0x15008a68c898>,\n",
       " <_utils.Method at 0x15008a68c828>,\n",
       " <_utils.Method at 0x15008a68ceb8>,\n",
       " <_utils.Method at 0x15008a84cc88>,\n",
       " <_utils.Method at 0x15008a8477f0>,\n",
       " <_utils.Method at 0x15008a847240>]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"running\"></a>\n",
    "### Running all of the methods to generate distance matrices\n",
    "Notes- Instead of passing in similarity function like cosine distance that will get evaluated for every possible i,j pair of vectors that are created (this is very big when splitting by phenes), don't use a specific similarity function, but instead let the object use a KNN classifier. pass in some limit for k like 100. then the object uses some more efficient (not brute force) algorithm to set the similarity of some vector v to its 100 nearest neighbors as those 100 probabilities, and sets everything else to 0. This would need to be implemented as a matching but separate function from the get_square_matrix_from_vectors thing. And then this would need to be noted in the similarity function that was used for these in the big table of methods. This won't work because the faster (not brute force algorithms) are not for sparse vectors like n-grams, and the non-sparse embeddings aren't really the problem here because those vectors are relatively much short, even when concatenating BERT encoder layers thats only up to around length of ~1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined__wikipedia                                                    00:01:55         3307        536\n",
      "combined__pubmed                                                       00:03:43         3307        536\n",
      "combined__plants                                                       00:00:20         2378        536\n",
      "combined__tokenization_wikipedia                                       00:07:42         3307       2921\n",
      "combined__tokenization_pubmed                                          00:17:46         3307       2921\n",
      "combined__tokenization_plants                                          00:00:31         2378       2921\n",
      "baseline__identity                                                     00:00:00            0        536\n",
      "baseline__tokenization_identity                                        00:00:00            0       2921\n",
      "bert__base_layers_3_concatenated                                       00:27:52         2304        536\n",
      "bert__tokenization_base_layers_4_summed                                00:28:15          768       2921\n",
      "biobert__pubmed_pmc_layers_2_concatenated                              00:33:03         1536        536\n",
      "biobert__tokenization_pubmed_pmc_layers_4_summed                       00:21:58          768       2921\n",
      "word2vec__pubmed_size_200_mean                                         00:00:00          200        536\n",
      "word2vec__tokenization_pubmed_size_200_mean                            00:00:03          200       2921\n",
      "noble_coder__precise_tfidf                                             00:00:00          940        536\n",
      "noble_coder__partial_tfidf                                             00:00:01        10000        536\n",
      "noble_coder__tokenization_precise_tfidf                                00:00:03         1019       2921\n",
      "noble_coder__tokenization_partial_tfidf                                00:00:29        10000       2921\n",
      "topic_modeling__nmf_full_topics_50                                     00:00:02           50        536\n",
      "topic_modeling__nmf_full_topics_100                                    00:00:06          100        536\n",
      "topic_modeling__tokenization_nmf_full_topics_50                        00:00:06           50       2921\n",
      "topic_modeling__tokenization_nmf_full_topics_100                       00:00:11          100       2921\n",
      "topic_modeling__lda_full_topics_50                                     00:00:00           50        536\n",
      "topic_modeling__lda_full_topics_100                                    00:00:00          100        536\n",
      "topic_modeling__tokenization_lda_full_topics_50                        00:00:04           50       2921\n",
      "topic_modeling__tokenization_lda_full_topics_100                       00:00:04          100       2921\n",
      "n_grams__full_plant_overrepresented_tokens_1_grams                     00:00:00          835        536\n",
      "n_grams__tokenization_full_plant_overrepresented_tokens_1_grams        00:00:03          917       2921\n",
      "go__union                                                              00:00:00         1716        493\n",
      "po__union                                                              00:00:00          275        418\n",
      "go__minimum                                                            00:00:01         1797       1508\n",
      "po__minimum                                                            00:00:00          284        270\n",
      "go_cc__union                                                           00:00:01         1716       1508\n",
      "go_bp__union                                                           00:00:01         1716       1508\n",
      "go_mf__union                                                           00:00:01         1716       1508\n",
      "go_cc__minimum                                                         00:00:01         1797       1508\n",
      "go_bp__minimum                                                         00:00:01         1797       1508\n",
      "go_mf__minimum                                                         00:00:01         1797       1508\n",
      "go__ic                                                                 00:00:01         1714        493\n",
      "po__ic                                                                 00:00:02          274        418\n"
     ]
    }
   ],
   "source": [
    "# Generate all the pairwise distance matrices but not in parallel.  \n",
    "graphs = {}\n",
    "method_col_names = []\n",
    "method_name_to_method_obj = {}\n",
    "durations = []\n",
    "vector_lengths = []\n",
    "array_lengths = []\n",
    "for method in methods:\n",
    "    graph,duration = function_wrapper_with_duration(function=method.function, args=method.kwargs)\n",
    "    graph.edgelist = None\n",
    "    graphs[method.name_with_hyperparameters] = graph\n",
    "    method_col_names.append(method.name_with_hyperparameters)\n",
    "    method_name_to_method_obj[method.name_with_hyperparameters] = method\n",
    "    durations.append(to_hms(duration))\n",
    "    vector_length = len(list(graph.vector_dictionary.values())[0])    \n",
    "    array_length = graph.array.shape[0]\n",
    "    vector_lengths.append(vector_length)\n",
    "    array_lengths.append(array_length)\n",
    "    print(\"{:70} {:10} {:10} {:10}\".format(method.name_with_hyperparameters, to_hms(duration), vector_length, array_length))\n",
    "\n",
    "    # Saving the graph objects to pickles that could be loaded later to access the same vector embeddings.\n",
    "    if args.app:\n",
    "        filters = [lambda x: x.lower(), strip_punctuation]\n",
    "        filename = \"_\".join(preprocess_string(method.name_with_hyperparameters, filters))\n",
    "            \n",
    "        # New version here that splits the two biggest components to make pickle file sizes smaller.\n",
    "        path_to_dists_object = os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"dists_with_{}.pickle\".format(filename))\n",
    "        path_to_vectors_dictionary = os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"vectors_with_{}.pickle\".format(filename))\n",
    "        save_to_pickle(path=path_to_vectors_dictionary, obj=graph.vector_dictionary)\n",
    "        # Remove the large components from the graph before pickling it.\n",
    "        saved_vectors_dict = graph.vector_dictionary.copy()\n",
    "        saved_array = np.copy(graph.array)\n",
    "        graph.vector_dictionary = None\n",
    "        graph.array = None\n",
    "        save_to_pickle(path=path_to_dists_object, obj=graph)\n",
    "        # Add them back in for use in this script.\n",
    "        graph.vector_dictionary = saved_vectors_dict\n",
    "        graph.array = saved_array\n",
    "    \n",
    "    \n",
    "# Save a file that details what was constructed with each approach, and how long it took.\n",
    "approaches_df = pd.DataFrame({\"method\":method_col_names, \"duration\":durations, \"vector_length\":vector_lengths, \"arr_length\":array_lengths})\n",
    "approaches_df[\"name_key\"] = approaches_df[\"method\"]\n",
    "approaches_df.to_csv(os.path.join(OUTPUT_DIR, APPROACHES_DIR, \"approaches.csv\"), index=False)\n",
    "\n",
    "# Save a copy of the dataset that includes just the genes that were looked at here.\n",
    "# This way this dataset will correspond exactly to the distance matrices created and saved as well.\n",
    "if args.app:\n",
    "    path = os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"genes_texts_annots.csv\")\n",
    "    dataset.to_pandas().to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These IDs should either be the IDs picked from the dataset that represent actual genes, or the paired sentence IDs.\n",
    "# This depends on which one was set as the dataset to use previously.\n",
    "ids = ids_to_use\n",
    "from_to_id_pairs = [(i,j) for (i,j) in itertools.combinations(ids, 2)]\n",
    "df = pd.DataFrame(from_to_id_pairs, columns=[\"from\",\"to\"])\n",
    "\n",
    "# The number of rows in the dataframe should the number of possible ways to pick two genes without replacement, where\n",
    "# the order doesn't matter. These are the gene pairs that we want to look at.\n",
    "expected_number_of_rows = ((len(ids)**2)-len(ids))/2\n",
    "assert df.shape[0] == expected_number_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When multiple indices within the array could be part of the data for one particular gene (sentence tokenized).\n",
    "def get_min_of_distances(gene_id_1, gene_id_2, gene_id_to_uids, uid_to_array_index, array):\n",
    "    uids_list_1 = gene_id_to_uids[gene_id_1]\n",
    "    uids_list_2 = gene_id_to_uids[gene_id_2]\n",
    "    possible_uid_combinations = itertools.product(uids_list_1, uids_list_2)\n",
    "    distance = min([array[uid_to_array_index[i],uid_to_array_index[j]] for (i,j) in possible_uid_combinations])\n",
    "    return(distance)\n",
    "\n",
    "\n",
    "# When a single index within the array has to represent entirely the data for one gene (not sentence tokenized).\n",
    "def lookup_distance(gene_id_1, gene_id_2, gene_id_to_uids, uid_to_array_index, array):\n",
    "    assert len(gene_id_to_uids[gene_id_1]) == 1\n",
    "    assert len(gene_id_to_uids[gene_id_2]) == 1\n",
    "    uid_1 = gene_id_to_uids[gene_id_1][0]\n",
    "    uid_2 = gene_id_to_uids[gene_id_2][0]\n",
    "    distance = array[uid_to_array_index[uid_1],uid_to_array_index[uid_2]]\n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined__wikipedia\n",
      "combined__pubmed\n",
      "combined__plants\n",
      "combined__tokenization_wikipedia\n",
      "combined__tokenization_pubmed\n",
      "combined__tokenization_plants\n",
      "baseline__identity\n",
      "baseline__tokenization_identity\n",
      "bert__base_layers_3_concatenated\n",
      "bert__tokenization_base_layers_4_summed\n",
      "biobert__pubmed_pmc_layers_2_concatenated\n",
      "biobert__tokenization_pubmed_pmc_layers_4_summed\n",
      "word2vec__pubmed_size_200_mean\n",
      "word2vec__tokenization_pubmed_size_200_mean\n",
      "noble_coder__precise_tfidf\n",
      "noble_coder__partial_tfidf\n",
      "noble_coder__tokenization_precise_tfidf\n",
      "noble_coder__tokenization_partial_tfidf\n",
      "topic_modeling__nmf_full_topics_50\n",
      "topic_modeling__nmf_full_topics_100\n",
      "topic_modeling__tokenization_nmf_full_topics_50\n",
      "topic_modeling__tokenization_nmf_full_topics_100\n",
      "topic_modeling__lda_full_topics_50\n",
      "topic_modeling__lda_full_topics_100\n",
      "topic_modeling__tokenization_lda_full_topics_50\n",
      "topic_modeling__tokenization_lda_full_topics_100\n",
      "n_grams__full_plant_overrepresented_tokens_1_grams\n",
      "n_grams__tokenization_full_plant_overrepresented_tokens_1_grams\n",
      "go__union\n",
      "po__union\n",
      "go__minimum\n",
      "po__minimum\n",
      "go_cc__union\n",
      "go_bp__union\n",
      "go_mf__union\n",
      "go_cc__minimum\n",
      "go_bp__minimum\n",
      "go_mf__minimum\n",
      "go__ic\n",
      "po__ic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Depending on what the IDs in the dictionaries for each approach were referencing, the distance values in the\n",
    "# arrays that were returned mean different things. In some cases, the IDs might refer to unique text strings parsed\n",
    "# from the whole descriptions, or tokenized strings referring to single sentences, or they might be referring to \n",
    "# particular unique gene ontology terms that were used in the curated annotations, or to unique whole sets of terms\n",
    "# that were used in the annotations. This dictionary maps tags associated with each approach to which function and \n",
    "# dictionary for translating between ID types in order to handle each approach appropriately.\n",
    "\n",
    "function_and_mapping_to_use = {\n",
    "    \"sent_tokens\":(get_min_of_distances, gene_id_to_unique_ids_mappings[\"sent_tokens\"]),\n",
    "    \"whole_texts\":(lookup_distance, gene_id_to_unique_ids_mappings[\"whole_texts\"]),\n",
    "    \"go_term_sets\":(lookup_distance, gene_id_to_unique_ids_mappings[\"go_term_sets\"]),\n",
    "    \"po_term_sets\":(lookup_distance, gene_id_to_unique_ids_mappings[\"po_term_sets\"]),\n",
    "    \"go_terms\":(get_min_of_distances, gene_id_to_unique_ids_mappings[\"go_terms\"]),\n",
    "    \"po_terms\":(get_min_of_distances, gene_id_to_unique_ids_mappings[\"po_terms\"])}\n",
    "\n",
    "\n",
    "\n",
    "# Writing those dictionaries to pickles so that we can use them in the app with the distance objects.\n",
    "if args.app:\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_sent_tokens.pickle\"), obj=gene_id_to_unique_ids_mappings[\"sent_tokens\"])\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_whole_texts.pickle\"), obj=gene_id_to_unique_ids_mappings[\"whole_texts\"])\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_go_term_sets.pickle\"), obj=gene_id_to_unique_ids_mappings[\"go_term_sets\"])\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_po_term_sets.pickle\"), obj=gene_id_to_unique_ids_mappings[\"po_term_sets\"])\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_go_terms.pickle\"), obj=gene_id_to_unique_ids_mappings[\"go_terms\"])\n",
    "    save_to_pickle(path=os.path.join(OUTPUT_DIR, STREAMLIT_DIR, \"gene_id_to_unique_ids_po_terms.pickle\"), obj=gene_id_to_unique_ids_mappings[\"po_terms\"])\n",
    "\n",
    "\n",
    "# Create one new column in the edge list dataframe for each of the approaches that were used.\n",
    "for name,graph in graphs.items():\n",
    "    print(name)\n",
    "    function, mapping = function_and_mapping_to_use[method_name_to_method_obj[name].tag]\n",
    "    df[name] = df.apply(lambda x: function(x[\"from\"], x[\"to\"], mapping, graph.id_to_index, graph.array), axis=1)\n",
    "        \n",
    "\n",
    "# Memory cleanup for the extremely large objects returned by the distance matrix generating functions.\n",
    "graphs = None\n",
    "\n",
    "\n",
    "# Because cosine similarity and distance functions are used, vectors with all zeroes will have undefined similarity\n",
    "# to other vectors. This results when empty strings or empty lists are passed to the methods that generate vectors\n",
    "# and distances matrices over this dataset. Therefore those NaNs that result are replaced here with the maximum\n",
    "# distance value, which is set 1 because the range of all the distance functions used is 0 to 1.\n",
    "df.fillna(value=1.000, inplace=True)\n",
    "\n",
    "\n",
    "# Make sure that the edge list contains the expected data types before moving forward.\n",
    "df[\"from\"] = df[\"from\"].astype(\"int64\")\n",
    "df[\"to\"] = df[\"to\"].astype(\"int64\")\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.app:\n",
    "    print(stophere_because_we_only_need_file_for_the_app_not_anything_else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column for the EQ statement similarity at this point. This way, it will be present when the dataframe that is\n",
    "# here is used to create the standard set of numpy arrays.\n",
    "if (args.dataset == \"plants\") and (args.annotations == True or args.ic == True):\n",
    "    eqs_method = Method(name=\"eqs\", hyperparameters=\"no hyperparams\", group=\"curated\", number=2005)\n",
    "    eqs_method.name_with_hyperparameters\n",
    "\n",
    "    # Add a column that indicates whether or not both genes of the pair are mapped to all the curation types.\n",
    "    relevant_ids = set(ow_edgelist.ids)\n",
    "    df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "    df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "    df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "    # Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "    df[eqs_method.name_with_hyperparameters] = -1\n",
    "    df = df.merge(right=eq_edgelist_collapsed, how=\"left\", on=[\"from\",\"to\"])\n",
    "    df[\"value\"].fillna(value=0, inplace=True)\n",
    "    df.loc[(df[\"pair_is_valid\"]==True),eqs_method.name_with_hyperparameters] = 1-df[\"value\"]\n",
    "    df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\",\"value\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    # Also, add the curated EQ approach to the list of column names that reference approaches to be evaluated.\n",
    "    methods.append(eqs_method)\n",
    "\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"merging\"></a>\n",
    "### Merging all of the distance matrices into a single dataframe specifying edges\n",
    "This section also handles replacing IDs from the individual methods that are references individual phenes that are part of a larger phenotype, and replacing those IDs with IDs referencing the full phenotypes (one-to-one relationship between phenotypes and genes). In this case, the minimum distance found between any two phenes from those two phenotypes represents the distance between that pair of phenotypes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "### Combining multiple distances measurements into summarizing distance values\n",
    "The purpose of this section is to iteratively train models on subsections of the dataset using simple regression or machine learning approaches to predict a value from zero to one indicating how likely is it that two genes share at least one of the specified groups in common. The information input to these models is the distance scores provided by each method in some set of all the methods used in this notebook. The purpose is to see whether or not a function of these similarity scores specifically trained to the task of predicting common groupings is better able to used the distance metric information to report a score for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average distance percentile as a means of combining multiple scores.\n",
    "#mean_method = Method(name=\"mean\", hyperparameters=\"no hyperparameters\", group=\"nlp\", number=300)\n",
    "#col_names_to_use_for_mean = [method.name_with_hyperparameters for method in methods if (\"go:\" not in method.name_with_hyperparameters) and (\"po:\" not in method.name_with_hyperparameters)]\n",
    "#df[mean_method.name_with_hyperparameters] = df[col_names_to_use_for_mean].rank(pct=True).mean(axis=1)\n",
    "#methods.append(mean_method)\n",
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined__wikipedia\n",
      "combined__pubmed\n",
      "combined__plants\n",
      "combined__tokenization_wikipedia\n",
      "combined__tokenization_pubmed\n",
      "combined__tokenization_plants\n",
      "baseline__identity\n",
      "baseline__tokenization_identity\n",
      "bert__base_layers_3_concatenated\n",
      "bert__tokenization_base_layers_4_summed\n",
      "biobert__pubmed_pmc_layers_2_concatenated\n",
      "biobert__tokenization_pubmed_pmc_layers_4_summed\n",
      "word2vec__pubmed_size_200_mean\n",
      "word2vec__tokenization_pubmed_size_200_mean\n",
      "noble_coder__precise_tfidf\n",
      "noble_coder__partial_tfidf\n",
      "noble_coder__tokenization_precise_tfidf\n",
      "noble_coder__tokenization_partial_tfidf\n",
      "topic_modeling__nmf_full_topics_50\n",
      "topic_modeling__nmf_full_topics_100\n",
      "topic_modeling__tokenization_nmf_full_topics_50\n",
      "topic_modeling__tokenization_nmf_full_topics_100\n",
      "topic_modeling__lda_full_topics_50\n",
      "topic_modeling__lda_full_topics_100\n",
      "topic_modeling__tokenization_lda_full_topics_50\n",
      "topic_modeling__tokenization_lda_full_topics_100\n",
      "n_grams__full_plant_overrepresented_tokens_1_grams\n",
      "n_grams__tokenization_full_plant_overrepresented_tokens_1_grams\n",
      "go__union\n",
      "po__union\n",
      "go__minimum\n",
      "po__minimum\n",
      "go_cc__union\n",
      "go_bp__union\n",
      "go_mf__union\n",
      "go_cc__minimum\n",
      "go_bp__minimum\n",
      "go_mf__minimum\n",
      "go__ic\n",
      "po__ic\n",
      "eqs__no_hyperparams\n"
     ]
    }
   ],
   "source": [
    "# Normalizing all of the array representations of the graphs so they can be combined. Then this version of the arrays\n",
    "# should be used by any other cells that need all of the arrays, rather than the arrays accessed from the graph\n",
    "# objects. This is necessary for this analysis because the distances matrices created and put in the graph objects use\n",
    "# IDs that don't actually reference the genes like the IDs used as nodes in the edgelist dataframe do, they reference \n",
    "# other types of subsets of that data which are smaller for that processing step. This section is included just to \n",
    "# produce a standardized list of arrays which exactly represent the data in the edgelist dataframe. It is redundant,\n",
    "# and could be removed later if necessary for memory constraints, but it is useful to be able to reference this\n",
    "# information sometimes using numpy instead of pandas only.\n",
    "name_to_array = {}\n",
    "\n",
    "# Picked based of what the dataset being looked at is. Theres either referencing genes or could also be from\n",
    "# the files that have paired sentences like BIOSSES or the dataset of score plant-related sentences.\n",
    "ids = ids_to_use\n",
    "\n",
    "n = len(ids)\n",
    "id_to_array_index = {i:idx for idx,i in enumerate(ids)}\n",
    "array_index_to_id = {idx:i for i,idx in id_to_array_index.items()}\n",
    "for method in methods:\n",
    "    name = method.name_with_hyperparameters\n",
    "    print(name)\n",
    "    idx = list(df.columns).index(name)+1\n",
    "    arr = np.ones((n, n))\n",
    "    for row in df.itertuples():\n",
    "        arr[id_to_array_index[row[1]]][id_to_array_index[row[2]]] = row[idx]\n",
    "        arr[id_to_array_index[row[2]]][id_to_array_index[row[1]]] = row[idx]\n",
    "    np.fill_diagonal(arr, 0.000) \n",
    "    name_to_array[name] = arr    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding correlations between human and computational approaches for hand-picked phenotype pairs\n",
    "This is only meant to be run in the context of the notebook, and should never be run automatically in the script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset in (\"biosses\", \"pairs\"):\n",
    "    print(\"Helo\")\n",
    "    small_table = defaultdict(dict)\n",
    "    for method in methods:\n",
    "        name = method.name_with_hyperparameters\n",
    "        values = []\n",
    "        scores = []\n",
    "        for tup,score in pair_to_score.items():\n",
    "            i = id_to_array_index[tup[0]]\n",
    "            j = id_to_array_index[tup[1]]\n",
    "            value = 1 - name_to_array[name][i,j]\n",
    "            values.append(value)\n",
    "            scores.append(score)\n",
    "        rho,pval = spearmanr(values,scores)\n",
    "        small_table[name] = {\"rho\":rho,\"pval\":pval}\n",
    "    correlations_df = pd.DataFrame(small_table).transpose()\n",
    "    correlations_df.reset_index(drop=False, inplace=True)\n",
    "    correlations_df.rename({\"index\":\"approach\"}, inplace=True, axis=\"columns\")\n",
    "    correlations_df[\"name_key\"] = correlations_df[\"approach\"]\n",
    "    correlations_df.to_csv(os.path.join(OUTPUT_DIR,METRICS_DIR,\"correlations.csv\"), index=False)\n",
    "    print(correlations_df)\n",
    "    #print(stop_here_gibberish_notavariable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_5\"></a>\n",
    "# Part 5. Biological Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"species\"></a>\n",
    "### Checking whether gene pairs are intraspecies or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0  4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1  4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2  4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3  4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4  4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5  4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6  4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7  4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8  4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9  4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "\n",
       "   n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  \n",
       "0                                           1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True  \n",
       "1                                           1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True  \n",
       "2                                           0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True  \n",
       "3                                           1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False  \n",
       "4                                           1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True  \n",
       "5                                           1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False  \n",
       "6                                           0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False  \n",
       "7                                           1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False  \n",
       "8                                           0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True  \n",
       "9                                           1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "species_dict = dataset.get_species_dictionary()\n",
    "df[\"same\"] = df[[\"from\",\"to\"]].apply(lambda x: species_dict[x[\"from\"]]==species_dict[x[\"to\"]],axis=1)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pathway_objective\"></a>\n",
    "### Using shared pathway membership (PlantCyc and KEGG) as the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair mapped to a pathway resource.\n",
    "pathway_mapped_ids = set(kegg_mapped_ids+pmn_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"pathways\"] = -1\n",
    "id_to_kegg_group_ids, kegg_group_id_to_ids = kegg_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_pmn_group_ids, pmn_group_id_to_ids = pmn_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_group_ids = {i:flatten([id_to_kegg_group_ids[i],id_to_pmn_group_ids[i]]) for i in dataset.get_ids()}\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"pathways\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair mapped to a pathway resource.\n",
    "pathway_mapped_ids = set(kegg_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"kegg_only\"] = -1\n",
    "id_to_kegg_group_ids, kegg_group_id_to_ids = kegg_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_pmn_group_ids, pmn_group_id_to_ids = pmn_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_group_ids = {i:flatten([id_to_kegg_group_ids[i],id_to_pmn_group_ids[i]]) for i in dataset.get_ids()}\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"kegg_only\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "      <th>pathways</th>\n",
       "      <th>kegg_only</th>\n",
       "      <th>pmn_only</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  pathways  kegg_only  pmn_only  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True        -1         -1        -1  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False        -1         -1        -1  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False        -1         -1        -1  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0   True        -1         -1        -1  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0   True        -1         -1        -1  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0   True        -1         -1        -1  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0   True        -1         -1        -1  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair mapped to a pathway resource.\n",
    "pathway_mapped_ids = set(pmn_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in pathway_mapped_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"pmn_only\"] = -1\n",
    "id_to_kegg_group_ids, kegg_group_id_to_ids = kegg_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_pmn_group_ids, pmn_group_id_to_ids = pmn_groups.get_groupings_for_dataset(dataset)\n",
    "id_to_group_ids = {i:flatten([id_to_kegg_group_ids[i],id_to_pmn_group_ids[i]]) for i in dataset.get_ids()}\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"pmn_only\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"subset_objective\"></a>\n",
    "### Using shared phenotype classification (Lloyd and Meinke et al., 2012) as the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "      <th>pathways</th>\n",
       "      <th>kegg_only</th>\n",
       "      <th>pmn_only</th>\n",
       "      <th>subsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  pathways  kegg_only  pmn_only  subsets  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True        -1         -1        -1       -1  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False        -1         -1        -1       -1  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0   True        -1         -1        -1       -1  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0   True        -1         -1        -1       -1  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0   True        -1         -1        -1       -1  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0   True        -1         -1        -1       -1  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(subsets_mapped_ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"subsets\"] = -1\n",
    "id_to_group_ids,_ = phe_subsets_groups.get_groupings_for_dataset(dataset)\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"subsets\"] = df[[\"from\",\"to\"]].apply(lambda x: len(set(id_to_group_ids[x[\"from\"]]).intersection(set(id_to_group_ids[x[\"to\"]])))>0, axis=1)*1\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\", \"pair_is_valid\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"association_objective\"></a>\n",
    "### Using protein associations (STRING) as the objective "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "      <th>pathways</th>\n",
       "      <th>kegg_only</th>\n",
       "      <th>pmn_only</th>\n",
       "      <th>subsets</th>\n",
       "      <th>known</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  pathways  kegg_only  pmn_only  subsets  known  predicted  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0   True        -1         -1        -1       -1    0.0        0.0  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0   True        -1         -1        -1       -1    0.0        0.0  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(string_edgelist.ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]*df[\"same\"]\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"known\"] = -1\n",
    "df[\"predicted\"] = -1\n",
    "df = df.merge(right=string_edgelist_collapsed, how=\"left\", on=[\"from\",\"to\"])\n",
    "df[\"known_associations\"].fillna(value=0, inplace=True)\n",
    "df[\"predicted_associations\"].fillna(value=0, inplace=True)\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"known\"] = df[\"known_associations\"]\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"predicted\"] = df[\"predicted_associations\"]\n",
    "\n",
    "# Convert all the positive values from string on range 0 to arbitrary n to be equal to 1.\n",
    "df.loc[df[\"known\"] >= 1, \"known\"] = 1 \n",
    "df.loc[df[\"predicted\"] >= 1, \"predicted\"] = 1 \n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\",\"known_associations\",\"predicted_associations\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ortholog_objective\"></a>\n",
    "### Using orthology between genes (PANTHER) as the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "      <th>pathways</th>\n",
       "      <th>kegg_only</th>\n",
       "      <th>pmn_only</th>\n",
       "      <th>subsets</th>\n",
       "      <th>known</th>\n",
       "      <th>predicted</th>\n",
       "      <th>orthologs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4093</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.018418</td>\n",
       "      <td>0.006368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059684</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.383153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.694824</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.770424</td>\n",
       "      <td>0.633595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962687</td>\n",
       "      <td>0.897528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.664674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218734</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465605</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4093</td>\n",
       "      <td>3802</td>\n",
       "      <td>0.011135</td>\n",
       "      <td>0.020086</td>\n",
       "      <td>0.008257</td>\n",
       "      <td>0.008003</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>0.258992</td>\n",
       "      <td>0.069424</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>0.699850</td>\n",
       "      <td>0.673029</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.961618</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927193</td>\n",
       "      <td>0.582024</td>\n",
       "      <td>0.848397</td>\n",
       "      <td>0.975622</td>\n",
       "      <td>0.986262</td>\n",
       "      <td>0.963858</td>\n",
       "      <td>0.979015</td>\n",
       "      <td>0.965783</td>\n",
       "      <td>0.909578</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4093</td>\n",
       "      <td>5699</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.016023</td>\n",
       "      <td>0.006050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.336467</td>\n",
       "      <td>0.287461</td>\n",
       "      <td>0.090416</td>\n",
       "      <td>0.104246</td>\n",
       "      <td>0.710369</td>\n",
       "      <td>0.707974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996138</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.985575</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983363</td>\n",
       "      <td>0.989410</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.986543</td>\n",
       "      <td>0.967103</td>\n",
       "      <td>0.983108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.648034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.396415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4093</td>\n",
       "      <td>4683</td>\n",
       "      <td>0.019272</td>\n",
       "      <td>0.032516</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.011083</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.005179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.254270</td>\n",
       "      <td>0.210932</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.087029</td>\n",
       "      <td>0.604017</td>\n",
       "      <td>0.640941</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.956194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999604</td>\n",
       "      <td>0.963454</td>\n",
       "      <td>0.986627</td>\n",
       "      <td>0.956023</td>\n",
       "      <td>0.973806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.829385</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697589</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4093</td>\n",
       "      <td>2562</td>\n",
       "      <td>0.018161</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.010590</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>0.012179</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.253616</td>\n",
       "      <td>0.229981</td>\n",
       "      <td>0.070291</td>\n",
       "      <td>0.080497</td>\n",
       "      <td>0.694555</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.977579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984528</td>\n",
       "      <td>0.989584</td>\n",
       "      <td>0.970331</td>\n",
       "      <td>0.985063</td>\n",
       "      <td>0.953709</td>\n",
       "      <td>0.971426</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.402415</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4093</td>\n",
       "      <td>5456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.015029</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324941</td>\n",
       "      <td>0.243613</td>\n",
       "      <td>0.079001</td>\n",
       "      <td>0.077653</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>0.622279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902194</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.891180</td>\n",
       "      <td>0.873198</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.032472</td>\n",
       "      <td>0.663096</td>\n",
       "      <td>0.971562</td>\n",
       "      <td>0.980641</td>\n",
       "      <td>0.964676</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>0.877704</td>\n",
       "      <td>0.840332</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033982</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.433008</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4093</td>\n",
       "      <td>1309</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>0.012660</td>\n",
       "      <td>0.020760</td>\n",
       "      <td>0.005141</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.283842</td>\n",
       "      <td>0.262463</td>\n",
       "      <td>0.107087</td>\n",
       "      <td>0.120944</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991928</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952906</td>\n",
       "      <td>0.982786</td>\n",
       "      <td>0.953181</td>\n",
       "      <td>0.976016</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4093</td>\n",
       "      <td>2601</td>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031397</td>\n",
       "      <td>0.011583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.197014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.602459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.796782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.758868</td>\n",
       "      <td>0.936757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.968980</td>\n",
       "      <td>0.984762</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.850362</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.227354</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4093</td>\n",
       "      <td>5421</td>\n",
       "      <td>0.013957</td>\n",
       "      <td>0.025509</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>0.019362</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286934</td>\n",
       "      <td>0.233266</td>\n",
       "      <td>0.087192</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.763259</td>\n",
       "      <td>0.798480</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.976970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.967407</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.972346</td>\n",
       "      <td>0.978541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.701905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.486953</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4093</td>\n",
       "      <td>5722</td>\n",
       "      <td>0.024181</td>\n",
       "      <td>0.030026</td>\n",
       "      <td>0.010292</td>\n",
       "      <td>0.018895</td>\n",
       "      <td>0.021731</td>\n",
       "      <td>0.006675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.257455</td>\n",
       "      <td>0.238730</td>\n",
       "      <td>0.091272</td>\n",
       "      <td>0.104687</td>\n",
       "      <td>0.739517</td>\n",
       "      <td>0.767785</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.828361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.741569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985349</td>\n",
       "      <td>0.971167</td>\n",
       "      <td>0.982011</td>\n",
       "      <td>0.951851</td>\n",
       "      <td>0.967182</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.456987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.386041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0   4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1   4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2   4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3   4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4   4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5   4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6   4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7   4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8   4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9   4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "10  4093  4787             0.010419          0.018418          0.006368                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.214887                                 0.000000                                   0.059684                                          0.000000                        0.383153                                     0.000000                         1.0                    0.694824                                      1.0                                 0.000000                            0.770424                             0.633595                                         0.000000                                          0.000000                            0.962687                             0.897528                                         0.000000                                          0.000000   \n",
       "11  4093  3802             0.011135          0.020086          0.008257                          0.008003                       0.013123                       0.004813                 1.0                              1.0                          0.219075                                 0.258992                                   0.069424                                          0.088994                        0.699850                                     0.673029                         1.0                    0.961618                                      1.0                                 0.952124                            1.000000                             0.927193                                         0.582024                                          0.848397                            0.975622                             0.986262                                         0.963858                                          0.979015   \n",
       "12  4093  5699             0.012889          0.016655          0.006834                          0.012889                       0.016023                       0.006050                 1.0                              1.0                          0.336467                                 0.287461                                   0.090416                                          0.104246                        0.710369                                     0.707974                         1.0                    0.996138                                      1.0                                 0.985575                            1.000000                             1.000000                                         0.983363                                          0.989410                            0.973457                             0.986543                                         0.967103                                          0.983108   \n",
       "13  4093  4683             0.019272          0.032516          0.011884                          0.011083                       0.018895                       0.005179                 1.0                              1.0                          0.254270                                 0.210932                                   0.074321                                          0.087029                        0.604017                                     0.640941                         1.0                    0.983262                                      1.0                                 0.956194                            1.000000                             1.000000                                         1.000000                                          0.999604                            0.963454                             0.986627                                         0.956023                                          0.973806   \n",
       "14  4093  2562             0.018161          0.028812          0.010590                          0.010650                       0.012179                       0.004967                 1.0                              1.0                          0.253616                                 0.229981                                   0.070291                                          0.080497                        0.694555                                     0.538078                         1.0                    0.994331                                      1.0                                 0.977579                            1.000000                             1.000000                                         0.984528                                          0.989584                            0.970331                             0.985063                                         0.953709                                          0.971426   \n",
       "15  4093  5456             0.009518          0.015029          0.004295                          0.009518                       0.015029                       0.004295                 1.0                              1.0                          0.324941                                 0.243613                                   0.079001                                          0.077653                        0.622279                                     0.622279                         1.0                    0.902194                                      1.0                                 0.891180                            0.873198                             0.625715                                         0.032472                                          0.663096                            0.971562                             0.980641                                         0.964676                                          0.983373   \n",
       "16  4093  1309             0.012660          0.020760          0.005141                          0.012660                       0.020760                       0.005141                 1.0                              1.0                          0.283842                                 0.262463                                   0.107087                                          0.120944                        0.714719                                     0.714719                         1.0                    1.000000                                      1.0                                 0.991928                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.952906                             0.982786                                         0.953181                                          0.976016   \n",
       "17  4093  2601             0.021228          0.031397          0.011583                          0.000000                       0.000000                       0.000000                 1.0                              0.0                          0.197014                                 0.000000                                   0.058670                                          0.000000                        0.602459                                     0.000000                         1.0                    0.796782                                      1.0                                 0.000000                            0.758868                             0.936757                                         0.000000                                          0.000000                            0.968980                             0.984762                                         0.000000                                          0.000000   \n",
       "18  4093  5421             0.013957          0.025509          0.007548                          0.010868                       0.019362                       0.005355                 1.0                              1.0                          0.286934                                 0.233266                                   0.087192                                          0.114324                        0.763259                                     0.798480                         1.0                    0.975895                                      1.0                                 0.976970                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.967407                             0.977273                                         0.972346                                          0.978541   \n",
       "19  4093  5722             0.024181          0.030026          0.010292                          0.018895                       0.021731                       0.006675                 1.0                              1.0                          0.257455                                 0.238730                                   0.091272                                          0.104687                        0.739517                                     0.767785                         1.0                    0.828361                                      1.0                                 0.741569                            1.000000                             1.000000                                         1.000000                                          0.985349                            0.971167                             0.982011                                         0.951851                                          0.967182   \n",
       "\n",
       "    n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  pathways  kegg_only  pmn_only  subsets  known  predicted  orthologs  \n",
       "0                                            1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "1                                            1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "2                                            0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "3                                            1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0  \n",
       "4                                            1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "5                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0  \n",
       "6                                            0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0  \n",
       "7                                            1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0  \n",
       "8                                            0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "9                                            1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "10                                           0.664674                                            0.000000                      1.0   0.218734          1.0     0.000000           1.0      0.465605           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "11                                           0.965783                                            0.909578                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0  \n",
       "12                                           1.000000                                            1.000000                      1.0   0.648034          1.0     0.000000           1.0      0.396415           1.0             1.0             1.0             1.0     1.0  0.700000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "13                                           1.000000                                            1.000000                      1.0   0.829385          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.697589                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "14                                           1.000000                                            1.000000                      1.0   0.560322          1.0     0.000000           1.0      0.402415           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "15                                           0.877704                                            0.840332                      1.0   0.033982          1.0     0.000000           1.0      0.433008           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "16                                           1.000000                                            1.000000                      1.0   0.109449          1.0     0.000000           1.0      0.361407           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "17                                           0.850362                                            0.000000                      1.0   0.227354          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "18                                           1.000000                                            1.000000                      1.0   0.701905          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.486953                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  \n",
       "19                                           1.000000                                            1.000000                      1.0   0.456987          1.0     0.000000           1.0      0.386041           1.0             1.0             1.0             1.0     1.0  0.500000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to a phenotype classification.\n",
    "relevant_ids = set(panther_edgelist.ids)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]*~df[\"same\"]\n",
    "\n",
    "# Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "df[\"orthologs\"] = -1\n",
    "df = df.merge(right=panther_edgelist.df, how=\"left\", on=[\"from\",\"to\"])\n",
    "df[\"value\"].fillna(value=0, inplace=True)\n",
    "df.loc[(df[\"pair_is_valid\"]==True),\"orthologs\"] = df[\"value\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\",\"value\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(20)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"eq_sim\"></a>\n",
    "### Curator-derived similarity values from Oellrich, Walls et al., 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was moved up to make sure that the EQs as a method is present in the dataframe when arrays get created.\n",
    "#if args.dataset == \"plants\":\n",
    "#    eqs_method = Method(name=\"eqs\", hyperparameters=\"no hyperparams\", group=\"curated\", number=2005)\n",
    "#    eqs_method.name_with_hyperparameters\n",
    "#\n",
    "#    # Add a column that indicates whether or not both genes of the pair are mapped to all the curation types.\n",
    "#    relevant_ids = set(ow_edgelist.ids)\n",
    "#    df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "#    df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "#    df[\"pair_is_valid\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "\n",
    "#    # Add a column giving the actual target output value for this biological task, with -1 for the irrelevant rows.\n",
    "#    df[eqs_method.name_with_hyperparameters] = -1\n",
    "#    df = df.merge(right=eq_edgelist_collapsed, how=\"left\", on=[\"from\",\"to\"])\n",
    "#    df[\"value\"].fillna(value=0, inplace=True)\n",
    "#    df.loc[(df[\"pair_is_valid\"]==True),eqs_method.name_with_hyperparameters] = 1-df[\"value\"]\n",
    "#    df.drop(labels=[\"from_is_valid\",\"to_is_valid\",\"pair_is_valid\",\"value\"], axis=\"columns\", inplace=True)\n",
    "\n",
    "    # Also, add the curated EQ approach to the list of column names that reference approaches to be evaluated.\n",
    "#    methods.append(eqs_method)\n",
    "\n",
    "#assert df.shape[0] == expected_number_of_rows\n",
    "#df.head(20)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"curated\"></a>\n",
    "### Checking whether gene pairs are considered curated or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>combined__wikipedia</th>\n",
       "      <th>combined__pubmed</th>\n",
       "      <th>combined__plants</th>\n",
       "      <th>combined__tokenization_wikipedia</th>\n",
       "      <th>combined__tokenization_pubmed</th>\n",
       "      <th>combined__tokenization_plants</th>\n",
       "      <th>baseline__identity</th>\n",
       "      <th>baseline__tokenization_identity</th>\n",
       "      <th>bert__base_layers_3_concatenated</th>\n",
       "      <th>bert__tokenization_base_layers_4_summed</th>\n",
       "      <th>biobert__pubmed_pmc_layers_2_concatenated</th>\n",
       "      <th>biobert__tokenization_pubmed_pmc_layers_4_summed</th>\n",
       "      <th>word2vec__pubmed_size_200_mean</th>\n",
       "      <th>word2vec__tokenization_pubmed_size_200_mean</th>\n",
       "      <th>noble_coder__precise_tfidf</th>\n",
       "      <th>noble_coder__partial_tfidf</th>\n",
       "      <th>noble_coder__tokenization_precise_tfidf</th>\n",
       "      <th>noble_coder__tokenization_partial_tfidf</th>\n",
       "      <th>topic_modeling__nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_nmf_full_topics_100</th>\n",
       "      <th>topic_modeling__lda_full_topics_50</th>\n",
       "      <th>topic_modeling__lda_full_topics_100</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_50</th>\n",
       "      <th>topic_modeling__tokenization_lda_full_topics_100</th>\n",
       "      <th>n_grams__full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>n_grams__tokenization_full_plant_overrepresented_tokens_1_grams</th>\n",
       "      <th>go__union</th>\n",
       "      <th>po__union</th>\n",
       "      <th>go__minimum</th>\n",
       "      <th>po__minimum</th>\n",
       "      <th>go_cc__union</th>\n",
       "      <th>go_bp__union</th>\n",
       "      <th>go_mf__union</th>\n",
       "      <th>go_cc__minimum</th>\n",
       "      <th>go_bp__minimum</th>\n",
       "      <th>go_mf__minimum</th>\n",
       "      <th>go__ic</th>\n",
       "      <th>po__ic</th>\n",
       "      <th>eqs__no_hyperparams</th>\n",
       "      <th>same</th>\n",
       "      <th>pathways</th>\n",
       "      <th>kegg_only</th>\n",
       "      <th>pmn_only</th>\n",
       "      <th>subsets</th>\n",
       "      <th>known</th>\n",
       "      <th>predicted</th>\n",
       "      <th>orthologs</th>\n",
       "      <th>curated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4093</td>\n",
       "      <td>5757</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.028582</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.014158</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286487</td>\n",
       "      <td>0.215179</td>\n",
       "      <td>0.073477</td>\n",
       "      <td>0.090136</td>\n",
       "      <td>0.730418</td>\n",
       "      <td>0.710201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.987947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.951162</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.978262</td>\n",
       "      <td>0.948471</td>\n",
       "      <td>0.972212</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.765446</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4093</td>\n",
       "      <td>849</td>\n",
       "      <td>0.026466</td>\n",
       "      <td>0.038164</td>\n",
       "      <td>0.016437</td>\n",
       "      <td>0.010039</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220043</td>\n",
       "      <td>0.203800</td>\n",
       "      <td>0.063275</td>\n",
       "      <td>0.071212</td>\n",
       "      <td>0.596440</td>\n",
       "      <td>0.480467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872203</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.682345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.780742</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>0.964242</td>\n",
       "      <td>0.984735</td>\n",
       "      <td>0.764925</td>\n",
       "      <td>0.000572</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.361739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4093</td>\n",
       "      <td>5017</td>\n",
       "      <td>0.015389</td>\n",
       "      <td>0.022860</td>\n",
       "      <td>0.008063</td>\n",
       "      <td>0.011845</td>\n",
       "      <td>0.015611</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.307953</td>\n",
       "      <td>0.240655</td>\n",
       "      <td>0.078245</td>\n",
       "      <td>0.086225</td>\n",
       "      <td>0.564886</td>\n",
       "      <td>0.557720</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.920055</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.924027</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>0.189937</td>\n",
       "      <td>0.591749</td>\n",
       "      <td>0.972625</td>\n",
       "      <td>0.982359</td>\n",
       "      <td>0.966798</td>\n",
       "      <td>0.983229</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.877296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.329672</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4093</td>\n",
       "      <td>3731</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>0.015332</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324839</td>\n",
       "      <td>0.316868</td>\n",
       "      <td>0.104704</td>\n",
       "      <td>0.084486</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>0.691245</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966900</td>\n",
       "      <td>0.983170</td>\n",
       "      <td>0.966964</td>\n",
       "      <td>0.983203</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4093</td>\n",
       "      <td>1402</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>0.010233</td>\n",
       "      <td>0.016574</td>\n",
       "      <td>0.005421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.358581</td>\n",
       "      <td>0.219964</td>\n",
       "      <td>0.119103</td>\n",
       "      <td>0.108093</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>0.672983</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.949656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944617</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966020</td>\n",
       "      <td>0.982716</td>\n",
       "      <td>0.956615</td>\n",
       "      <td>0.975953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4093</td>\n",
       "      <td>2326</td>\n",
       "      <td>0.017643</td>\n",
       "      <td>0.024863</td>\n",
       "      <td>0.006968</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>0.013521</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234614</td>\n",
       "      <td>0.236156</td>\n",
       "      <td>0.076470</td>\n",
       "      <td>0.092160</td>\n",
       "      <td>0.663932</td>\n",
       "      <td>0.606153</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971710</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.945347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959371</td>\n",
       "      <td>0.965316</td>\n",
       "      <td>0.982731</td>\n",
       "      <td>0.956947</td>\n",
       "      <td>0.971609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4093</td>\n",
       "      <td>1222</td>\n",
       "      <td>0.026355</td>\n",
       "      <td>0.043397</td>\n",
       "      <td>0.017755</td>\n",
       "      <td>0.012693</td>\n",
       "      <td>0.015659</td>\n",
       "      <td>0.004525</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.297935</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.071276</td>\n",
       "      <td>0.090697</td>\n",
       "      <td>0.697745</td>\n",
       "      <td>0.692346</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.967403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>0.994719</td>\n",
       "      <td>0.985315</td>\n",
       "      <td>0.049944</td>\n",
       "      <td>0.596927</td>\n",
       "      <td>0.976384</td>\n",
       "      <td>0.989671</td>\n",
       "      <td>0.053443</td>\n",
       "      <td>0.969937</td>\n",
       "      <td>0.979949</td>\n",
       "      <td>0.880080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.898544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.484201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880323</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4093</td>\n",
       "      <td>3194</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>0.027005</td>\n",
       "      <td>0.008353</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225783</td>\n",
       "      <td>0.249967</td>\n",
       "      <td>0.063061</td>\n",
       "      <td>0.086642</td>\n",
       "      <td>0.617088</td>\n",
       "      <td>0.597901</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974629</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.959209</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984949</td>\n",
       "      <td>0.986308</td>\n",
       "      <td>0.971549</td>\n",
       "      <td>0.983987</td>\n",
       "      <td>0.948541</td>\n",
       "      <td>0.973554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4093</td>\n",
       "      <td>1085</td>\n",
       "      <td>0.016977</td>\n",
       "      <td>0.027115</td>\n",
       "      <td>0.010153</td>\n",
       "      <td>0.014969</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242900</td>\n",
       "      <td>0.239739</td>\n",
       "      <td>0.078043</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.663644</td>\n",
       "      <td>0.645675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.971117</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962855</td>\n",
       "      <td>0.964037</td>\n",
       "      <td>0.930658</td>\n",
       "      <td>0.615046</td>\n",
       "      <td>0.894403</td>\n",
       "      <td>0.971216</td>\n",
       "      <td>0.984493</td>\n",
       "      <td>0.953736</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.963041</td>\n",
       "      <td>0.914867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.231798</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4093</td>\n",
       "      <td>429</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.023450</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.009752</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230039</td>\n",
       "      <td>0.207812</td>\n",
       "      <td>0.060583</td>\n",
       "      <td>0.071850</td>\n",
       "      <td>0.658377</td>\n",
       "      <td>0.538078</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.962787</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.316218</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.972792</td>\n",
       "      <td>0.977618</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.976298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.210140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.332235</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   from    to  combined__wikipedia  combined__pubmed  combined__plants  combined__tokenization_wikipedia  combined__tokenization_pubmed  combined__tokenization_plants  baseline__identity  baseline__tokenization_identity  bert__base_layers_3_concatenated  bert__tokenization_base_layers_4_summed  biobert__pubmed_pmc_layers_2_concatenated  biobert__tokenization_pubmed_pmc_layers_4_summed  word2vec__pubmed_size_200_mean  word2vec__tokenization_pubmed_size_200_mean  noble_coder__precise_tfidf  noble_coder__partial_tfidf  noble_coder__tokenization_precise_tfidf  noble_coder__tokenization_partial_tfidf  topic_modeling__nmf_full_topics_50  topic_modeling__nmf_full_topics_100  topic_modeling__tokenization_nmf_full_topics_50  topic_modeling__tokenization_nmf_full_topics_100  topic_modeling__lda_full_topics_50  topic_modeling__lda_full_topics_100  topic_modeling__tokenization_lda_full_topics_50  topic_modeling__tokenization_lda_full_topics_100  \\\n",
       "0  4093  5757             0.021132          0.028582          0.008181                          0.013326                       0.014158                       0.004033                 1.0                              1.0                          0.286487                                 0.215179                                   0.073477                                          0.090136                        0.730418                                     0.710201                         1.0                    0.987947                                      1.0                                 0.951162                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.971875                             0.978262                                         0.948471                                          0.972212   \n",
       "1  4093   849             0.026466          0.038164          0.016437                          0.010039                       0.011830                       0.003232                 1.0                              1.0                          0.220043                                 0.203800                                   0.063275                                          0.071212                        0.596440                                     0.480467                         1.0                    0.872203                                      1.0                                 0.682345                            1.000000                             1.000000                                         0.780742                                          0.899824                            0.964242                             0.984735                                         0.764925                                          0.000572   \n",
       "2  4093  5017             0.015389          0.022860          0.008063                          0.011845                       0.015611                       0.005041                 1.0                              1.0                          0.307953                                 0.240655                                   0.078245                                          0.086225                        0.564886                                     0.557720                         1.0                    0.920055                                      1.0                                 0.925978                            0.924027                             0.943172                                         0.189937                                          0.591749                            0.972625                             0.982359                                         0.966798                                          0.983229   \n",
       "3  4093  3731             0.015332          0.021885          0.004555                          0.015332                       0.021885                       0.004555                 1.0                              1.0                          0.324839                                 0.316868                                   0.104704                                          0.084486                        0.691245                                     0.691245                         1.0                    1.000000                                      1.0                                 0.991407                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966900                             0.983170                                         0.966964                                          0.983203   \n",
       "4  4093  1402             0.010233          0.016574          0.005421                          0.010233                       0.016574                       0.005421                 1.0                              1.0                          0.358581                                 0.219964                                   0.119103                                          0.108093                        0.672983                                     0.672983                         1.0                    0.949656                                      1.0                                 0.944617                            1.000000                             1.000000                                         1.000000                                          1.000000                            0.966020                             0.982716                                         0.956615                                          0.975953   \n",
       "5  4093  2326             0.017643          0.024863          0.006968                          0.012682                       0.013521                       0.004642                 1.0                              1.0                          0.234614                                 0.236156                                   0.076470                                          0.092160                        0.663932                                     0.606153                         1.0                    0.971710                                      1.0                                 0.945347                            1.000000                             1.000000                                         1.000000                                          0.959371                            0.965316                             0.982731                                         0.956947                                          0.971609   \n",
       "6  4093  1222             0.026355          0.043397          0.017755                          0.012693                       0.015659                       0.004525                 1.0                              1.0                          0.297935                                 0.274033                                   0.071276                                          0.090697                        0.697745                                     0.692346                         1.0                    0.967403                                      1.0                                 0.902780                            0.994719                             0.985315                                         0.049944                                          0.596927                            0.976384                             0.989671                                         0.053443                                          0.969937   \n",
       "7  4093  3194             0.019583          0.027005          0.008353                          0.010146                       0.011903                       0.004738                 1.0                              1.0                          0.225783                                 0.249967                                   0.063061                                          0.086642                        0.617088                                     0.597901                         1.0                    0.974629                                      1.0                                 0.959209                            1.000000                             1.000000                                         0.984949                                          0.986308                            0.971549                             0.983987                                         0.948541                                          0.973554   \n",
       "8  4093  1085             0.016977          0.027115          0.010153                          0.014969                       0.012784                       0.004931                 1.0                              1.0                          0.242900                                 0.239739                                   0.078043                                          0.088600                        0.663644                                     0.645675                         1.0                    0.971117                                      1.0                                 0.962855                            0.964037                             0.930658                                         0.615046                                          0.894403                            0.971216                             0.984493                                         0.953736                                          0.971820   \n",
       "9  4093   429             0.011838          0.023450          0.006482                          0.009752                       0.009688                       0.004477                 1.0                              1.0                          0.230039                                 0.207812                                   0.060583                                          0.071850                        0.658377                                     0.538078                         1.0                    0.975251                                      1.0                                 0.962787                            1.000000                             1.000000                                         0.316218                                          1.000000                            0.972792                             0.977618                                         0.000168                                          0.976298   \n",
       "\n",
       "   n_grams__full_plant_overrepresented_tokens_1_grams  n_grams__tokenization_full_plant_overrepresented_tokens_1_grams  go__union  po__union  go__minimum  po__minimum  go_cc__union  go_bp__union  go_mf__union  go_cc__minimum  go_bp__minimum  go_mf__minimum  go__ic    po__ic  eqs__no_hyperparams   same  pathways  kegg_only  pmn_only  subsets  known  predicted  orthologs  curated  \n",
       "0                                           1.000000                                            1.000000                      1.0   0.661101          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.765446                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  \n",
       "1                                           1.000000                                            1.000000                      1.0   0.468493          1.0     0.000000           1.0      0.361739           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  \n",
       "2                                           0.939204                                            0.877296                      1.0   0.329672          1.0     0.000000           1.0      0.381671           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  \n",
       "3                                           1.000000                                            1.000000                      1.0   1.000000          1.0     1.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0    False  \n",
       "4                                           1.000000                                            1.000000                      1.0   0.314650          1.0     0.000000           1.0      0.437913           1.0             1.0             1.0             1.0     1.0  0.400000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  \n",
       "5                                           1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0    False  \n",
       "6                                           0.979949                                            0.880080                      0.0   0.898544          0.0     0.484201           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  0.880323                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0    False  \n",
       "7                                           1.000000                                            1.000000                      0.0   1.000000          0.0     1.000000           0.0      0.000000           0.0             0.0             0.0             0.0     1.0  1.000000                 -1.0  False        -1         -1        -1       -1   -1.0       -1.0       -1.0    False  \n",
       "8                                           0.963041                                            0.914867                      1.0   0.231798          1.0     0.000000           1.0      1.000000           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  \n",
       "9                                           1.000000                                            1.000000                      1.0   0.210140          1.0     0.000000           1.0      0.332235           1.0             1.0             1.0             1.0     1.0  0.200000                 -1.0   True        -1         -1        -1       -1    0.0        0.0       -1.0    False  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a column that indicates whether or not both genes of the pair are mapped to all the curation types.\n",
    "# This is because to keep things simple for the analysis, we're keeping ones that have all three annotation types as a\n",
    "# separate datasets where we have all the curation information that we want to use as a comparison.\n",
    "relevant_ids = set(ids_with_all_annotations)\n",
    "df[\"from_is_valid\"] = df[\"from\"].map(lambda x: x in relevant_ids)\n",
    "df[\"to_is_valid\"] = df[\"to\"].map(lambda x: x in relevant_ids)\n",
    "df[\"curated\"] = df[\"from_is_valid\"]*df[\"to_is_valid\"]\n",
    "df.drop(labels=[\"from_is_valid\",\"to_is_valid\"], axis=\"columns\", inplace=True)\n",
    "assert df.shape[0] == expected_number_of_rows\n",
    "df.head(10)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking to make sure that the number of genes and pairs matches what is expected at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the columns in this dataframe that were generated in the previous cells, what are all the variables \n",
    "# by which we want to be able to split up the data, so that we can calculate metrics on different subsets of it?\n",
    "curated = [True,False]\n",
    "question = [\"subsets\", \"known\", \"predicted\", \"pathways\", \"orthologs\"]\n",
    "species = [\"intra\",\"inter\",\"both\"]\n",
    "\n",
    "\n",
    "# Not all possible combinations of these variables makes logical sense or are of interest. \n",
    "# For example, the dataset for interspecies genes pairs and protein associations will have only negatives in it.\n",
    "# The dataset for intraspecies gene pairs that are orthologous will also have only negatives in it.\n",
    "# Including both intraspecies and interspecies only really applies to looking at biochemical pathways currently.\n",
    "# For now, just manually specify which of the combinations make sense. If adding more ways to split up the data in\n",
    "# the future, this might have to be done in a better way.\n",
    "variable_combinations = [\n",
    "    (True,\"subsets\",\"intra\"),\n",
    "    (True,\"known\",\"intra\"),\n",
    "    (True,\"predicted\",\"intra\"),\n",
    "    (True,\"pathways\",\"intra\"),\n",
    "    (True,\"pathways\",\"inter\"),\n",
    "    (True,\"pathways\",\"both\"),\n",
    "    (True,\"orthologs\",\"inter\"),\n",
    "    (False,\"subsets\",\"intra\"),\n",
    "    (False,\"known\",\"intra\"),\n",
    "    (False,\"predicted\",\"intra\"),\n",
    "    (False,\"pathways\",\"intra\"),\n",
    "    (False,\"pathways\",\"inter\"),\n",
    "    (False,\"pathways\",\"both\"),\n",
    "    (False,\"orthologs\",\"inter\"),\n",
    "]\n",
    "\n",
    "# Create an infinitely nested dictionary to put results in for each of these different subsets of the data.\n",
    "# This does not have a limited shape, but it should be used in this case as:\n",
    "# dict[curated][question][species][approach][metric] --> value.\n",
    "infinite_defaultdict = lambda: defaultdict(infinite_defaultdict)\n",
    "tables = infinite_defaultdict()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"n_values\"></a>\n",
    "### What are the values of *n* for each type of iteration through a subset of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>curated</th>\n",
       "      <th>species</th>\n",
       "      <th>num_genes</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>num_pairs</th>\n",
       "      <th>positive_fraction</th>\n",
       "      <th>negative_fraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>265</td>\n",
       "      <td>3873</td>\n",
       "      <td>31107</td>\n",
       "      <td>34980</td>\n",
       "      <td>0.110720</td>\n",
       "      <td>0.889280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>known</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>278</td>\n",
       "      <td>919</td>\n",
       "      <td>33075</td>\n",
       "      <td>33994</td>\n",
       "      <td>0.027034</td>\n",
       "      <td>0.972966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>predicted</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>278</td>\n",
       "      <td>538</td>\n",
       "      <td>33456</td>\n",
       "      <td>33994</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.984174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pathways</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>122</td>\n",
       "      <td>275</td>\n",
       "      <td>6291</td>\n",
       "      <td>6566</td>\n",
       "      <td>0.041882</td>\n",
       "      <td>0.958118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pathways</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>123</td>\n",
       "      <td>26</td>\n",
       "      <td>911</td>\n",
       "      <td>937</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.972252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pathways</td>\n",
       "      <td>true</td>\n",
       "      <td>both</td>\n",
       "      <td>123</td>\n",
       "      <td>301</td>\n",
       "      <td>7202</td>\n",
       "      <td>7503</td>\n",
       "      <td>0.040117</td>\n",
       "      <td>0.959883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>orthologs</td>\n",
       "      <td>true</td>\n",
       "      <td>inter</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>767</td>\n",
       "      <td>767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subsets</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>273</td>\n",
       "      <td>3998</td>\n",
       "      <td>33130</td>\n",
       "      <td>37128</td>\n",
       "      <td>0.107682</td>\n",
       "      <td>0.892318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>known</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>552</td>\n",
       "      <td>2658</td>\n",
       "      <td>129625</td>\n",
       "      <td>132283</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.979907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>predicted</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>552</td>\n",
       "      <td>1339</td>\n",
       "      <td>130944</td>\n",
       "      <td>132283</td>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.989878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>pathways</td>\n",
       "      <td>false</td>\n",
       "      <td>intra</td>\n",
       "      <td>224</td>\n",
       "      <td>693</td>\n",
       "      <td>18914</td>\n",
       "      <td>19607</td>\n",
       "      <td>0.035345</td>\n",
       "      <td>0.964655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pathways</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>225</td>\n",
       "      <td>108</td>\n",
       "      <td>5485</td>\n",
       "      <td>5593</td>\n",
       "      <td>0.019310</td>\n",
       "      <td>0.980690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pathways</td>\n",
       "      <td>false</td>\n",
       "      <td>both</td>\n",
       "      <td>225</td>\n",
       "      <td>801</td>\n",
       "      <td>24399</td>\n",
       "      <td>25200</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.968214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>orthologs</td>\n",
       "      <td>false</td>\n",
       "      <td>inter</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>4107</td>\n",
       "      <td>4109</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.999513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     question curated species  num_genes  positive  negative  num_pairs  positive_fraction  negative_fraction\n",
       "0     subsets    true   intra        265      3873     31107      34980           0.110720           0.889280\n",
       "1       known    true   intra        278       919     33075      33994           0.027034           0.972966\n",
       "2   predicted    true   intra        278       538     33456      33994           0.015826           0.984174\n",
       "3    pathways    true   intra        122       275      6291       6566           0.041882           0.958118\n",
       "4    pathways    true   inter        123        26       911        937           0.027748           0.972252\n",
       "5    pathways    true    both        123       301      7202       7503           0.040117           0.959883\n",
       "6   orthologs    true   inter         48         0       767        767           0.000000           1.000000\n",
       "7     subsets   false   intra        273      3998     33130      37128           0.107682           0.892318\n",
       "8       known   false   intra        552      2658    129625     132283           0.020093           0.979907\n",
       "9   predicted   false   intra        552      1339    130944     132283           0.010122           0.989878\n",
       "10   pathways   false   intra        224       693     18914      19607           0.035345           0.964655\n",
       "11   pathways   false   inter        225       108      5485       5593           0.019310           0.980690\n",
       "12   pathways   false    both        225       801     24399      25200           0.031786           0.968214\n",
       "13  orthologs   false   inter        118         2      4107       4109           0.000487           0.999513"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_idx_lists = []\n",
    "subset_properties = []\n",
    "table_lists = defaultdict(list)\n",
    "for (c,q,s) in variable_combinations: \n",
    "\n",
    "    # Remembering what the properties for this particular subset are.\n",
    "    subset_properties.append((c,q,s))\n",
    "    \n",
    "    # Subsetting the dataframe to the rows (gene pairs) that are relevant for this particular biological question.\n",
    "    subset = df[df[q] != -1]\n",
    "    if c:\n",
    "        subset = subset[subset[\"curated\"] == True]\n",
    "        \n",
    "        \n",
    "    # Subsetting the dataframe to the rows (gene pairs) where both genes are from the same or different species.\n",
    "    if s == \"intra\":\n",
    "        subset = subset[subset[\"same\"] == True]\n",
    "    elif s == \"inter\":\n",
    "        subset = subset[subset[\"same\"] == False]\n",
    "        \n",
    "    \n",
    "    # Remember which indices in the dataframe correspond to the subset for that combination of variables.\n",
    "    if args.ratio != None:\n",
    "        np.random.seed(seed=293874)\n",
    "        class_ratio = args.ratio\n",
    "        positive_idxs = subset[subset[q]==1].index.to_list()\n",
    "        negative_idxs = subset[subset[q]==0].index.to_list()\n",
    "        num_to_retain = math.ceil(len(positive_idxs)*class_ratio)\n",
    "        negative_idxs = np.random.choice(negative_idxs, num_to_retain).tolist()\n",
    "        idxs = positive_idxs + negative_idxs\n",
    "        subset_idx_lists.append(idxs)\n",
    "        subset = df.loc[idxs]\n",
    "        \n",
    "    else:   \n",
    "        subset_idx_lists.append(subset.index.to_list())\n",
    "    \n",
    "    \n",
    "    # Adding values to the table that are specific to this biological question.\n",
    "    counts = Counter(subset[q].values)\n",
    "    table_lists[\"question\"].append(q.lower())\n",
    "    table_lists[\"curated\"].append(str(c).lower())\n",
    "    table_lists[\"species\"].append(s.lower())\n",
    "    table_lists[\"num_genes\"].append(len(set(subset[\"to\"].values).union(set(subset[\"from\"].values))))\n",
    "    table_lists[\"positive\"].append(counts[1])\n",
    "    table_lists[\"negative\"].append(counts[0])\n",
    "    \n",
    "# Adding the additional columns that are functions of the ones already created.\n",
    "pairs_table = pd.DataFrame(table_lists)  \n",
    "pairs_table[\"num_pairs\"] = pairs_table[\"positive\"]+pairs_table[\"negative\"]\n",
    "pairs_table[\"positive_fraction\"] = pairs_table[\"positive\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table[\"negative_fraction\"] = pairs_table[\"negative\"] / pairs_table[\"num_pairs\"]\n",
    "pairs_table.to_csv(os.path.join(OUTPUT_DIR, QUESTIONS_DIR, \"value_of_n_for_each_question.csv\"), index=False)\n",
    "\n",
    "# The number of pairs for a given task should always be less than the total possible combinations(genes,2).\n",
    "pairs_table[\"max_num_pairs_expected\"] = pairs_table[\"num_genes\"].map(lambda x: ((x**2)-x)/2)\n",
    "bool_list = [(x>=0) for x in pairs_table[\"max_num_pairs_expected\"]-pairs_table[\"num_pairs\"]]\n",
    "assert sum(bool_list) == len(bool_list)\n",
    "pairs_table.drop(columns=[\"max_num_pairs_expected\"], inplace=True)\n",
    "\n",
    "pairs_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"objective_similarities\"></a>\n",
    "### How similar are the different biological objectives to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>num_pairs_1</th>\n",
       "      <th>num_pairs_2</th>\n",
       "      <th>jacsim_genes</th>\n",
       "      <th>num_overlap</th>\n",
       "      <th>jacsim_truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>known</td>\n",
       "      <td>pathways</td>\n",
       "      <td>132283</td>\n",
       "      <td>25200</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>18223</td>\n",
       "      <td>0.272889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>predicted</td>\n",
       "      <td>pathways</td>\n",
       "      <td>132283</td>\n",
       "      <td>25200</td>\n",
       "      <td>0.130856</td>\n",
       "      <td>18223</td>\n",
       "      <td>0.143204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subsets</td>\n",
       "      <td>pathways</td>\n",
       "      <td>37128</td>\n",
       "      <td>25200</td>\n",
       "      <td>0.124547</td>\n",
       "      <td>6903</td>\n",
       "      <td>0.133464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>known</td>\n",
       "      <td>predicted</td>\n",
       "      <td>132283</td>\n",
       "      <td>132283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>132283</td>\n",
       "      <td>0.118981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subsets</td>\n",
       "      <td>known</td>\n",
       "      <td>37128</td>\n",
       "      <td>132283</td>\n",
       "      <td>0.270281</td>\n",
       "      <td>36046</td>\n",
       "      <td>0.065051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pathways</td>\n",
       "      <td>orthologs</td>\n",
       "      <td>25200</td>\n",
       "      <td>4109</td>\n",
       "      <td>0.015206</td>\n",
       "      <td>439</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>subsets</td>\n",
       "      <td>predicted</td>\n",
       "      <td>37128</td>\n",
       "      <td>132283</td>\n",
       "      <td>0.270281</td>\n",
       "      <td>36046</td>\n",
       "      <td>0.042034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>subsets</td>\n",
       "      <td>orthologs</td>\n",
       "      <td>37128</td>\n",
       "      <td>4109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>known</td>\n",
       "      <td>orthologs</td>\n",
       "      <td>132283</td>\n",
       "      <td>4109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>predicted</td>\n",
       "      <td>orthologs</td>\n",
       "      <td>132283</td>\n",
       "      <td>4109</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  question_1 question_2  num_pairs_1  num_pairs_2  jacsim_genes  num_overlap  jacsim_truths\n",
       "0      known   pathways       132283        25200      0.130856        18223       0.272889\n",
       "1  predicted   pathways       132283        25200      0.130856        18223       0.143204\n",
       "2    subsets   pathways        37128        25200      0.124547         6903       0.133464\n",
       "3      known  predicted       132283       132283      1.000000       132283       0.118981\n",
       "4    subsets      known        37128       132283      0.270281        36046       0.065051\n",
       "5   pathways  orthologs        25200         4109      0.015206          439       0.052632\n",
       "6    subsets  predicted        37128       132283      0.270281        36046       0.042034\n",
       "7    subsets  orthologs        37128         4109      0.000000            0      -1.000000\n",
       "8      known  orthologs       132283         4109      0.000000            0      -1.000000\n",
       "9  predicted  orthologs       132283         4109      0.000000            0      -1.000000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Looking more at the distributions of target values for each of the biological questions.\n",
    "from scipy.spatial.distance import jaccard\n",
    "row_tuples = []\n",
    "for q1,q2 in itertools.combinations(question, 2):\n",
    "    \n",
    "    # How similar are these two questions in terms of which gene pairs apply to them?\n",
    "    q1_subset = df[df[q1] != -1]\n",
    "    q2_subset = df[df[q2] != -1]\n",
    "    overlap_subset  = q1_subset[q1_subset[q2] != -1]\n",
    "    union_subset = df[(df[q1] != -1) | (df[q2] != -1)]\n",
    "    if len(union_subset) != 0:\n",
    "        overlaps_sim = len(overlap_subset)/len(union_subset)\n",
    "    else:\n",
    "        overlaps_sim = -1\n",
    "    \n",
    "    # How big is that overlap in gene pairs that apply to both questions?\n",
    "    q1_num_pairs = q1_subset.shape[0]\n",
    "    q2_num_pairs = q2_subset.shape[0]\n",
    "    overlap_size = overlap_subset.shape[0]\n",
    "    \n",
    "    # How similar are the truth values between those two questions for the gene pairs that apply to both?\n",
    "    assert len(overlap_subset[q1].values) == len(overlap_subset[q2].values)\n",
    "    if len(overlap_subset[q1].values) == 0:\n",
    "        overlap_sim = -1\n",
    "    else:\n",
    "        overlap_sim = 1-jaccard(overlap_subset[q1].values, overlap_subset[q2].values)\n",
    "    row_tuples.append((q1, q2, q1_num_pairs, q2_num_pairs, overlaps_sim, overlap_size, overlap_sim))\n",
    "    \n",
    "# Putting together the dataframe for all possible pairs of questions.\n",
    "question_overlaps_table = pd.DataFrame(row_tuples)\n",
    "question_overlaps_table.columns = [\"question_1\", \"question_2\", \"num_pairs_1\", \"num_pairs_2\", \"jacsim_genes\", \"num_overlap\", \"jacsim_truths\"]\n",
    "question_overlaps_table.sort_values(by=\"jacsim_truths\", ascending=False, inplace=True)\n",
    "question_overlaps_table.reset_index(inplace=True, drop=True)\n",
    "question_overlaps_table.to_csv(os.path.join(OUTPUT_DIR, QUESTIONS_DIR, \"sizes_of_overlaps_between_questions.csv\"), index=False)\n",
    "question_overlaps_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_6\"></a>\n",
    "# Part 6. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ks\"></a>\n",
    "### Do the edges joining genes that share a group, pathway, or interaction come from a different distribution?\n",
    "The purpose of this section is to visualize kernel estimates for the distributions of distance or similarity scores generated by each of the methods tested for measuring semantic similarity or generating vector representations of the phenotype descriptions. Ideally, better methods should show better separation between the distributions for distance values between two genes involved in a common specified group or two genes that are not. Additionally, a statistical test is used to check whether these two distributions are significantly different from each other or not, although this is a less informative measure than the other tests used in subsequent sections, because it does not address how useful these differences in the distributions actually are for making predictions about group membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For creating the frequency and density dataframe.\n",
    "dist_rows = []\n",
    "\n",
    "\n",
    "\n",
    "for properties,idxs in zip(subset_properties, subset_idx_lists):\n",
    "    \n",
    "    # Remember the properties for this subset being looked at, and subset the dataframe accordingly.\n",
    "    c,q,s = properties\n",
    "    \n",
    "    \n",
    "    # Only look at gene pairs where both are relevant to the given biological question.\n",
    "    subset = df.loc[idxs]\n",
    "        \n",
    "    # Check that this subsetting leaves a valid dataset with both positive and negatives samples.\n",
    "    class_values = pd.unique(subset[q].values)\n",
    "    if not (len(class_values)==2 and 0 in class_values and 1 in class_values):\n",
    "        continue\n",
    "    \n",
    "    # Use Kolmogorov-Smirnov test to see if edges between genes that share a group come from a distinct distribution.\n",
    "    ppi_pos_dict = {method.name_with_hyperparameters:(subset[subset[q] > 0.00][method.name_with_hyperparameters].values) for method in methods}\n",
    "    ppi_neg_dict = {method.name_with_hyperparameters:(subset[subset[q] == 0.00][method.name_with_hyperparameters].values) for method in methods}\n",
    "    for method in methods:\n",
    "        name = method.name_with_hyperparameters\n",
    "        stat,p = ks_2samp(ppi_pos_dict[name],ppi_neg_dict[name])\n",
    "        pos_mean = np.average(ppi_pos_dict[name])\n",
    "        neg_mean = np.average(ppi_neg_dict[name])\n",
    "        pos_n = len(ppi_pos_dict[name])\n",
    "        neg_n = len(ppi_neg_dict[name])\n",
    "        \n",
    "        tables[c][q][s][name].update({\"mean_1\":pos_mean, \"mean_0\":neg_mean, \"n_1\":pos_n, \"n_0\":neg_n})\n",
    "        tables[c][q][s][name].update({\"ks\":stat, \"ks_pval\":p})\n",
    "\n",
    "        \n",
    "        \n",
    "        # Adding the histogram creation part.\n",
    "        num_bins_to_try = [20, 50, 100]\n",
    "        for num_bins in num_bins_to_try:\n",
    "        \n",
    "            range_ = (0,1)\n",
    "            bin_width = (range_[1]-range_[0])/num_bins\n",
    "            positive_dist = ppi_pos_dict[name]\n",
    "            negative_dist = ppi_neg_dict[name]\n",
    "            positive_hist_frequency = np.histogram(positive_dist, bins=num_bins, range=range_, density=False)\n",
    "            negative_hist_frequency = np.histogram(negative_dist, bins=num_bins, range=range_, density=False)\n",
    "            positive_hist_density = np.histogram(positive_dist, bins=num_bins, range=range_, density=True)\n",
    "            negative_hist_density = np.histogram(negative_dist, bins=num_bins, range=range_, density=True)\n",
    "\n",
    "            # All those should have identical sets of bin edges.\n",
    "            #assert positive_hist_frequency[1] == negative_hist_frequency[1]\n",
    "            #assert positive_hist_frequency[1] == positive_hist_density[1]\n",
    "            #assert positive_hist_frequency[1] == negative_hist_density[1]\n",
    "            bin_centers = [x+(bin_width/2) for x in positive_hist_frequency[1][:num_bins]]\n",
    "\n",
    "            for i,bin_center in enumerate(bin_centers):\n",
    "                p_freq = positive_hist_frequency[0][i]\n",
    "                n_freq = negative_hist_frequency[0][i]\n",
    "                p_dens = positive_hist_density[0][i]\n",
    "                n_dens = negative_hist_density[0][i]\n",
    "                dist_rows.append((name, str(c).lower(),str(q).lower(),str(s).lower(),\"positive\",bin_center,p_freq,p_dens,num_bins))\n",
    "                dist_rows.append((name, str(c).lower(),str(q).lower(),str(s).lower(),\"negative\",bin_center,n_freq,n_dens,num_bins))\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Show the kernel estimates for each distribution of weights for each method.\n",
    "    num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "    fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "    for method,ax in zip(methods,axs.flatten()):\n",
    "        name = method.name_with_hyperparameters\n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel(\"value\")\n",
    "        ax.set_ylabel(\"density\")\n",
    "        sns.kdeplot(ppi_pos_dict[name], color=\"black\", shade=False, alpha=1.0, ax=ax)\n",
    "        sns.kdeplot(ppi_neg_dict[name], color=\"black\", shade=True, alpha=0.1, ax=ax) \n",
    "    fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Need to name the files depending on the variables being looked at.\n",
    "    curated_str = \"curated_{}\".format(str(c).lower())\n",
    "    question_str = \"question_{}\".format(str(q).lower())\n",
    "    species_str = \"species_{}\".format(str(s).lower())\n",
    "    variables_strs = [curated_str, question_str, species_str]\n",
    "    \n",
    "    # Save those plots in a new image file.\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR, PLOTS_DIR, \"kernel_densities_{}_{}_{}.png\".format(*variables_strs)),dpi=400)\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "\n",
    "dists_df = pd.DataFrame(dist_rows, columns=[\"approach\",\"curated\",\"objective\",\"species\",\"distribution\",\"bin_center\",\"frequency\",\"density\",\"num_bins\"])\n",
    "dists_df[\"name_key\"] = dists_df[\"approach\"]\n",
    "dists_df.to_csv(os.path.join(OUTPUT_DIR, PLOTS_DIR, \"histograms.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"within\"></a>\n",
    "### Looking at within-group or within-pathway distances in each graph\n",
    "The purpose of this section is to determine which methods generated graphs which tightly group genes which share common pathways or group membership with one another. In order to compare across different methods where the distance value distributions are different, the mean distance values for each group for each method are converted to percentile scores. Lower percentile scores indicate that the average distance value between any two genes that belong to that group is lower than most of the distance values in the entire distribution for that method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that structures created above look how we expect them to.\n",
    "assert len(ids) == len(id_to_array_index)\n",
    "assert len(ids) == len(array_index_to_id)\n",
    "for method in methods:\n",
    "    name = method.name_with_hyperparameters\n",
    "    assert name_to_array[name].shape == (len(ids),len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished finding sample means for this type of grouping\n",
      "finished finding sample means for this type of grouping\n",
      "finished finding sample means for this type of grouping\n",
      "finished finding sample means for this type of grouping\n",
      "finished finding sample means for this type of grouping\n",
      "finished finding sample means for this type of grouping\n"
     ]
    }
   ],
   "source": [
    "# What are the different groupings we are interested in for these mean within-group distance tables?\n",
    "grouping_objects = [kegg_groups, pmn_groups, phe_subsets_groups]\n",
    "grouping_names = [\"kegg_only\",\"pmn_only\",\"subsets\"]\n",
    "\n",
    "\n",
    "for (groups,q) in zip(grouping_objects,grouping_names):\n",
    "    for curated_genes_only in [True,False]:\n",
    "\n",
    "        # Only look at gene pairs where both are relevant to the given biological question.\n",
    "        subset = df[df[q] != -1]\n",
    "        if curated_genes_only:\n",
    "            subset = subset[subset[\"curated\"]==True]\n",
    "            \n",
    "        # Check that this subsetting leaves a valid dataset with both positive and negatives samples.\n",
    "        class_values = pd.unique(subset[q].values)\n",
    "        if not (len(class_values)==2 and 0 in class_values and 1 in class_values):\n",
    "            continue\n",
    "\n",
    "        # The grouping dictionaries for this particular biological question.    \n",
    "        id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(dataset)\n",
    "\n",
    "        # Get all the average within-group distance values for each approach.\n",
    "        group_ids = list(group_id_to_ids.keys())\n",
    "        within_percentiles_dict = defaultdict(lambda: defaultdict(list))\n",
    "        within_weights_dict = defaultdict(lambda: defaultdict(list))\n",
    "        group_id_to_n = {}\n",
    "        all_weights_dict = {}\n",
    "        for method in methods:\n",
    "            name = method.name_with_hyperparameters\n",
    "            for group in group_ids:\n",
    "                within_ids = group_id_to_ids[group]    \n",
    "                if curated_genes_only:\n",
    "                    within_ids = [i for i in within_ids if i in ids_with_all_annotations]\n",
    "                group_id_to_n[group] = len(within_ids)\n",
    "                \n",
    "                mean_weight = np.mean([name_to_array[name][id_to_array_index[i],id_to_array_index[j]] for i,j in combinations(within_ids,2)])\n",
    "                # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.percentileofscore.html\n",
    "                # Check this for documentation and specifically what the kind argument is for.\n",
    "                # With kind=\"mean\", the value of the weak and strict percentiles are averaged.\n",
    "                # Question for this part, is the percentile of the mean the same as the mean of the percentiles?\n",
    "                # I think we want mean of the percentiles, but we're calculating the other one here...\n",
    "                within_percentiles_dict[name][group] = stats.percentileofscore(subset[name].values, mean_weight, kind=\"mean\")\n",
    "                within_weights_dict[name][group] = mean_weight\n",
    "\n",
    "        # Generating a dataframe of percentiles of the mean in-group distance scores.\n",
    "        within_dist_data = pd.DataFrame(within_percentiles_dict)\n",
    "        within_dist_data = within_dist_data.dropna(axis=0, inplace=False)\n",
    "        within_dist_data = within_dist_data.round(4)\n",
    "\n",
    "        # Adding relevant information to this dataframe and saving.\n",
    "        # Defining mean_group_rank: the average of the individual rank given to this pathway by each approach.\n",
    "        # Defining mean_avg_pair_percentile: the average across all approaches of the average distance percentile for each gene pair.\n",
    "        within_dist_data[\"mean_group_rank\"] = within_dist_data.rank().mean(axis=1)\n",
    "        within_dist_data[\"mean_avg_pair_percentile\"] = within_dist_data.mean(axis=1)\n",
    "        within_dist_data.sort_values(by=\"mean_avg_pair_percentile\", inplace=True)\n",
    "        within_dist_data.reset_index(inplace=True)\n",
    "        within_dist_data[\"group_id\"] = within_dist_data[\"index\"]\n",
    "        within_dist_data[\"full_name\"] = within_dist_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "        #within_dist_data[\"n\"] = within_dist_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "        within_dist_data[\"n\"] = within_dist_data[\"group_id\"].map(group_id_to_n)\n",
    "        method_col_names = [method.name_with_hyperparameters for method in methods]\n",
    "        within_dist_data = within_dist_data[flatten([\"group_id\",\"full_name\",\"n\",\"mean_avg_pair_percentile\",\"mean_group_rank\",method_col_names])]\n",
    "        curated_string = {True:\"curated\",False:\"all\"}[curated_genes_only]\n",
    "        within_dist_data.to_csv(os.path.join(OUTPUT_DIR, GROUP_DISTS_DIR, \"{}_{}_within_distances.csv\".format(curated_string,q)), index=False)\n",
    "        num_groups = within_dist_data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "        # Making a melted version that calculates p-values for each row.\n",
    "        within_dist_data = pd.DataFrame(within_percentiles_dict)\n",
    "        within_dist_data = within_dist_data.dropna(axis=0, inplace=False)\n",
    "        within_dist_data = within_dist_data.round(4)\n",
    "        within_dist_data.reset_index(inplace=True)\n",
    "        within_dist_data[\"group_id\"] = within_dist_data[\"index\"]\n",
    "        method_col_names = [method.name_with_hyperparameters for method in methods]\n",
    "        within_dist_data = pd.melt(within_dist_data, id_vars=[\"group_id\"], value_vars=method_col_names, var_name=\"approach\", value_name=\"percentile\")\n",
    "        within_dist_data[\"full_name\"] = within_dist_data[\"group_id\"].apply(lambda x: groups.get_long_name(x))\n",
    "        within_dist_data[\"n\"] = within_dist_data[\"group_id\"].apply(lambda x: len(group_id_to_ids[x]))\n",
    "        within_dist_data = within_dist_data[[\"group_id\",\"full_name\", \"approach\", \"n\", \"percentile\"]]\n",
    "        within_dist_data[\"mean_value\"] = within_dist_data.apply(lambda row: within_weights_dict[row[\"approach\"]][row[\"group_id\"]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # Some setup for random sampling for finding p-values.\n",
    "        num_sampling_iterations = 1000\n",
    "        n_max = within_dist_data[\"n\"].max()\n",
    "        n_to_n_choose_two = {}\n",
    "        n = 2\n",
    "        while n <= n_max:\n",
    "            n_to_n_choose_two[n] = comb(n, k=2, exact=True)\n",
    "            n = n+1\n",
    "\n",
    "\n",
    "        # Which IDs should be considered for random sampling?\n",
    "        id_set_1 = pd.unique(subset[\"to\"].values)\n",
    "        id_set_2 = pd.unique(subset[\"from\"].values)\n",
    "        ids_that_are_relevant = list(set(id_set_1).union(id_set_2))\n",
    "\n",
    "        # Given that subset of IDs that are relevant to this task, create a 3D array as (iteration, pairs, IDs)\n",
    "        sampled_ids = np.array([np.random.choice(a=ids_that_are_relevant, size=n_max, replace=False) for i in range(num_sampling_iterations)])\n",
    "        sampled_id_pairs = np.array([list(combinations(id_list,2)) for id_list in sampled_ids])\n",
    "\n",
    "\n",
    "        # Create a mapping between method names, value of n, and a list of the means generated with each random sampling.\n",
    "        name_to_n_to_means = defaultdict(dict)\n",
    "        for method in methods:\n",
    "\n",
    "            # Build an array where the rows are sampling iterations and the columns have edge values.\n",
    "            name = method.name_with_hyperparameters\n",
    "            sampled_values = []\n",
    "            for pairs_of_ids in sampled_id_pairs:\n",
    "                sampled_values.append([name_to_array[name][id_to_array_index[i],id_to_array_index[j]] for (i,j) in pairs_of_ids])\n",
    "            sampled_values = np.array(sampled_values)\n",
    "\n",
    "            # Retain just the random means information we need. \n",
    "            n_to_means = {}\n",
    "            length = sampled_values.shape[1]\n",
    "            for n,n_choose_two in n_to_n_choose_two.items():\n",
    "                num_values_to_take = n_choose_two\n",
    "                means = np.mean(sampled_values[:, length-num_values_to_take:length], axis=1)\n",
    "                name_to_n_to_means[name][n] = means\n",
    "\n",
    "        # The length of all the arrays of means in that dictionary should be the same as the number of samplings.\n",
    "        # Values of n between 2 and the maximum value of n should be supported as keys in the dictionary.\n",
    "        assert name_to_n_to_means[methods[0].name_with_hyperparameters][2].shape[0] == num_sampling_iterations\n",
    "        assert name_to_n_to_means[methods[0].name_with_hyperparameters][n_max].shape[0] == num_sampling_iterations\n",
    "        print(\"finished finding sample means for this type of grouping\")\n",
    "\n",
    "\n",
    "\n",
    "        # Assigning p-values to each mean value assigned to each group by each algorithm, using the random sampled.\n",
    "        def calculate_p_value(sampled_means, actual_value):\n",
    "            atleast_as_small_as_actual_value = actual_value>=sampled_means\n",
    "            p_value = atleast_as_small_as_actual_value.sum()/len(sampled_means)\n",
    "            return(p_value)\n",
    "\n",
    "\n",
    "        within_dist_data[\"p_value\"] = within_dist_data.apply(lambda row: calculate_p_value(name_to_n_to_means[row[\"approach\"]][row[\"n\"]], row[\"mean_value\"]), axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        def benjamini_hochberg(unadjusted_p_values, alpha):\n",
    "            m = len(unadjusted_p_values)\n",
    "            bhdf = pd.DataFrame({\"p_value\":unadjusted_p_values})\n",
    "            bhdf[\"rank\"] = bhdf.rank(axis=\"rows\", method=\"first\", ascending=True)\n",
    "            assert bhdf[\"rank\"].min() == 1\n",
    "            assert bhdf[\"rank\"].max() == m\n",
    "            assert len(pd.unique(bhdf[\"rank\"])) == m\n",
    "            bhdf[\"total\"] = m\n",
    "            bhdf[\"fraction\"] = bhdf[\"rank\"]/bhdf[\"total\"]\n",
    "            bhdf[\"threshold\"] = alpha*bhdf[\"fraction\"]\n",
    "            bhdf[\"significant\"] = (bhdf[\"p_value\"]<=bhdf[\"threshold\"])\n",
    "            return(bhdf[\"significant\"].values)\n",
    "        \n",
    "        # Figuring out what proportion of the groups were assigned cohesive values that are considered significant.\n",
    "        within_dist_data[\"benjamini_hochberg\"] = within_dist_data.groupby(\"approach\")[\"p_value\"].transform(benjamini_hochberg, alpha=0.05)\n",
    "        within_dist_data[\"fraction_significant\"] = within_dist_data.groupby(\"approach\")[\"benjamini_hochberg\"].transform(lambda x: int(x.sum())/x.count())\n",
    "        within_dist_data[\"number_of_groups\"] = within_dist_data.groupby(\"approach\")[\"benjamini_hochberg\"].transform(lambda x: x.count())\n",
    "        \n",
    "        \n",
    "        #tdf = pd.DataFrame(within_dist_data.groupby(\"approach\")[\"p_value\",\"p_adjusted\"].agg(lambda x: sum(x<=significance_threshold)))\n",
    "        #tdf = tdf.reset_index(drop=False)\n",
    "        #tdf.columns = [\"approach\",\"num_significant\",\"num_adjusted\"]\n",
    "        #tdf[\"total_groups\"] = num_groups\n",
    "        #tdf[\"fraction_significant\"] = tdf[\"num_significant\"]/tdf[\"total_groups\"]\n",
    "        #tdf[\"fraction_adjusted\"] = tdf[\"num_adjusted\"]/tdf[\"total_groups\"]\n",
    "        #num_rows_before_merge = within_dist_data.shape[0]\n",
    "        #within_dist_data = within_dist_data.merge(right=tdf, how=\"left\", on=[\"approach\"])\n",
    "        #assert within_dist_data.shape[0] == num_rows_before_merge\n",
    "        #within_dist_data[[\"mean_value\",\"fraction_significant\",\"fraction_adjusted\"]] = within_dist_data[[\"mean_value\",\"fraction_significant\",\"fraction_adjusted\"]].round(4)\n",
    "        curated_string = {True:\"curated\",False:\"all\"}[curated_genes_only]\n",
    "        within_dist_data[\"name_key\"] = within_dist_data[\"approach\"]\n",
    "        within_dist_data.to_csv(os.path.join(OUTPUT_DIR, GROUP_DISTS_DIR, \"{}_{}_within_distances_melted.csv\".format(curated_string,q)), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"auc\"></a>\n",
    "### Predicting whether two genes belong to the same group, pathway, or share an interaction\n",
    "The purpose of this section is to see if whether or not two genes share at least one common pathway can be predicted from the distance scores assigned using analysis of text similarity. The evaluation of predictability is done by reporting a precision and recall curve for each method, as well as remembering the area under the curve, and ratio between the area under the curve and the baseline (expected area when guessing randomly) for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bootstrap(fraction, num_iterations, y_true, y_prob):\n",
    "#     # Run the desired number of bootstrap iterations over the full population of predictions and return st devs.\n",
    "#     scores = pd.DataFrame([bootstrap_iteration(fraction, y_true, y_prob) for i in range(num_iterations)])\n",
    "#     standard_deviations = {\n",
    "#         \"f_1_max_std\": np.std(scores[\"f_1_max\"].values),\n",
    "#         \"f_2_max_std\": np.std(scores[\"f_2_max\"].values),\n",
    "#         \"f_point5_max_std\": np.std(scores[\"f_point5_max\"].values)}\n",
    "#     return(standard_deviations)\n",
    "\n",
    "\n",
    "# def bootstrap_iteration(fraction, y_true, y_prob):\n",
    "#     assert len(y_true) == len(y_prob)\n",
    "#     # Subset the total population of predictions using the provided fraction.\n",
    "#     num_predictions = len(y_true)\n",
    "#     bootstrapping_fraction = fraction\n",
    "#     num_to_retain = int(np.ceil(num_predictions*bootstrapping_fraction))\n",
    "#     idx = np.random.choice(np.arange(num_predictions), num_to_retain, replace=False)\n",
    "#     y_true_sample = y_true[idx]\n",
    "#     y_prob_sample = y_prob[idx]\n",
    "    \n",
    "#     # Calculate any desired metrics using just that subset.\n",
    "#     n_pos, n_neg = Counter(y_true_sample)[1], Counter(y_true_sample)[0]\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true_sample, y_prob_sample)\n",
    "#     baseline = Counter(y_true_sample)[1]/len(y_true_sample) \n",
    "#     area = auc(recall, precision)\n",
    "#     auc_to_baseline_auc_ratio = area/baseline\n",
    "    \n",
    "#     # Find the maximum F score for different values of .  \n",
    "#     f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "#     f_1_scores = f_beta(precision,recall,beta=1)\n",
    "#     f_2_scores = f_beta(precision,recall,beta=2)\n",
    "#     f_point5_scores = f_beta(precision,recall,beta=0.5)\n",
    "    \n",
    "#     # Create a dictionary of those metric values to return.\n",
    "#     scores={\"f_1_max\":np.nanmax(f_1_scores),\"f_2_max\":np.nanmax(f_2_scores),\"f_point5_max\":np.nanmax(f_point5_scores)}\n",
    "#     return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>approach</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>group</th>\n",
       "      <th>task</th>\n",
       "      <th>curated</th>\n",
       "      <th>species</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>basline_auc</th>\n",
       "      <th>name_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110695</td>\n",
       "      <td>0.999742</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110670</td>\n",
       "      <td>0.999484</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110644</td>\n",
       "      <td>0.999225</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>0.998967</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110568</td>\n",
       "      <td>0.998451</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110441</td>\n",
       "      <td>0.997160</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.110364</td>\n",
       "      <td>0.996385</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109967</td>\n",
       "      <td>0.992254</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109868</td>\n",
       "      <td>0.991221</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109817</td>\n",
       "      <td>0.990705</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109740</td>\n",
       "      <td>0.989930</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109689</td>\n",
       "      <td>0.989414</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109616</td>\n",
       "      <td>0.988639</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109543</td>\n",
       "      <td>0.987865</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109444</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.109240</td>\n",
       "      <td>0.984766</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.108883</td>\n",
       "      <td>0.981152</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.108736</td>\n",
       "      <td>0.979602</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>intra</td>\n",
       "      <td>0.108659</td>\n",
       "      <td>0.978828</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  approach hyperparameters group     task curated species  precision    recall  basline_auc             name_key\n",
       "0   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110720  1.000000      0.11072  combined__wikipedia\n",
       "1   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110695  0.999742      0.11072  combined__wikipedia\n",
       "2   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110670  0.999484      0.11072  combined__wikipedia\n",
       "3   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110644  0.999225      0.11072  combined__wikipedia\n",
       "4   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110619  0.998967      0.11072  combined__wikipedia\n",
       "5   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110568  0.998451      0.11072  combined__wikipedia\n",
       "6   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110441  0.997160      0.11072  combined__wikipedia\n",
       "7   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.110364  0.996385      0.11072  combined__wikipedia\n",
       "8   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109967  0.992254      0.11072  combined__wikipedia\n",
       "9   combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109868  0.991221      0.11072  combined__wikipedia\n",
       "10  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109817  0.990705      0.11072  combined__wikipedia\n",
       "11  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109740  0.989930      0.11072  combined__wikipedia\n",
       "12  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109689  0.989414      0.11072  combined__wikipedia\n",
       "13  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109616  0.988639      0.11072  combined__wikipedia\n",
       "14  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109543  0.987865      0.11072  combined__wikipedia\n",
       "15  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109444  0.986832      0.11072  combined__wikipedia\n",
       "16  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.109240  0.984766      0.11072  combined__wikipedia\n",
       "17  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.108883  0.981152      0.11072  combined__wikipedia\n",
       "18  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.108736  0.979602      0.11072  combined__wikipedia\n",
       "19  combined__wikipedia  combined       wikipedia   nlp  subsets    true   intra   0.108659  0.978828      0.11072  combined__wikipedia"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_df_rows = []\n",
    "for properties,idxs in zip(subset_properties, subset_idx_lists):\n",
    "    \n",
    "    # Remember the properties for this subset being looked at, and subset the dataframe accordingly.\n",
    "    c,q,s = properties\n",
    "    \n",
    "    # Don't look at the inter-species and intra-species edges except for pathways, otherwise irrelevant.\n",
    "    #if (s != \"both\") and (q != \"pathways\"):\n",
    "    #    continue\n",
    "    \n",
    "    # Create a subset of the dataframe that contains only the gene pairs for this question.\n",
    "    subset = df.loc[idxs]\n",
    "\n",
    "    # Check that this subsetting leaves a valid dataset with both positive and negatives samples.\n",
    "    class_values = pd.unique(subset[q].values)\n",
    "    if not (len(class_values)==2 and 0 in class_values and 1 in class_values):\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Get all the probabilities and all the ones for positives samples in this case.\n",
    "    y_true_dict = {method.name_with_hyperparameters:subset[q].values for method in methods} \n",
    "    y_prob_dict = {method.name_with_hyperparameters:(1 - subset[method.name_with_hyperparameters].values) for method in methods}\n",
    "    num_plots, plots_per_row, row_width, row_height = (len(methods), 4, 14, 3)\n",
    "    fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "    for method,ax in zip(methods, axs.flatten()):\n",
    "        \n",
    "        # What is the name to use for this method, which represents a column in the dataframe.\n",
    "        name = method.name_with_hyperparameters\n",
    "        \n",
    "\n",
    "        # Obtaining the values and metrics.\n",
    "        y_true, y_prob = y_true_dict[name], y_prob_dict[name]\n",
    "        n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "        baseline_auc = Counter(y_true)[1]/len(y_true) \n",
    "        area = auc(recall, precision)\n",
    "        auc_to_baseline_auc_ratio = area/baseline_auc\n",
    "        \n",
    "        \n",
    "        # The baseline F1 max has a precision of the ratio of positives to all samples and a recall of 1.\n",
    "        # This is because a random classifier achieves that precision at all recall values, so recall is maximized to\n",
    "        # find the maximum F1 value that can be expected due to random chance.\n",
    "        baseline_f1_max = (2*baseline_auc*1)/(baseline_auc+1)\n",
    "        tables[c][q][s][name].update({\"auc\":area,\"auc_baseline\":baseline_auc, \"f1_max_baseline\":baseline_f1_max, })\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Add a row to the dataframe that specifically keeps track of the precision and recall distributions.\n",
    "        # We don't want to remember all the precision and recall values that sklearn generates, that file would\n",
    "        # get enormous. Instead, round the thresholds to 3 decimal places and then just remember the indices\n",
    "        # where we jumpt to the next thresholds (subsets to <= 1000 precision and recall value pairs). \n",
    "        # Note this only works when the threshold values are sorted in increasing order, which they currently are\n",
    "        # for this sklearn function. If that changes this will section will break.\n",
    "        pr_indices_to_use = []\n",
    "        last_threshold = -0.001 # A value smaller than any real threshold in this case.\n",
    "        for idx,threshold in enumerate(thresholds):\n",
    "            this_threshold = round(threshold,3)\n",
    "            if this_threshold != last_threshold:\n",
    "                pr_indices_to_use.append(idx)\n",
    "                last_threshold = this_threshold\n",
    "        pr_indices_to_use.append(len(thresholds)) # Add the index of the '1' and '0' that cap the P and R arrays.    \n",
    "        p_values_to_use = precision[pr_indices_to_use]\n",
    "        r_values_to_use = recall[pr_indices_to_use]\n",
    "\n",
    "        \n",
    "        # Use those 1000 or fewer precision and recall pairs to build a file from which curves can be plotted. \n",
    "        for p,r in zip(p_values_to_use,r_values_to_use):\n",
    "            pr_df_rows.append((name, method.name, method.hyperparameters, method.group, q.lower(), str(c).lower(), s.lower(), p, r, baseline_auc))\n",
    "        \n",
    "        \n",
    "        # Find the maximum F score for different values of .  \n",
    "        f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "        f_1_scores = f_beta(precision,recall,beta=1)\n",
    "        f_2_scores = f_beta(precision,recall,beta=2)\n",
    "        f_point5_scores = f_beta(precision,recall,beta=0.5)\n",
    "        f_1_max, f_1_std = np.nanmax(f_1_scores), np.std(f_1_scores)\n",
    "        f_2_max, f_2_std = np.nanmax(f_2_scores), np.std(f_2_scores)\n",
    "        f_point5_max, f_point5_std = np.nanmax(f_point5_scores), np.std(f_point5_scores)\n",
    "        tables[c][q][s][name].update({\"f1_max\":f_1_max, \"f5_max\":f_point5_max, \"f2_max\":f_2_max})\n",
    "        \n",
    "        \n",
    "        # Find the standard deviation of each metric when subsampling the dataset of predictions for each method.\n",
    "        #bootstrap_fraction = 0.9\n",
    "        #bootstrap_iterations = 100\n",
    "        #bootstrapped_std_dict = bootstrap(bootstrap_fraction, bootstrap_iterations, y_true, y_prob)\n",
    "        #tables[c][q][s][name].update({\"f1_std\":bootstrapped_std_dict[\"f_1_max_std\"], \"f5_std\":bootstrapped_std_dict[\"f_point5_max_std\"], \"f2_std\":bootstrapped_std_dict[\"f_2_max_std\"]}) \n",
    "\n",
    "        # Producing the precision recall curve.\n",
    "        step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "        ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "        ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "        ax.axhline(baseline_auc, linestyle=\"--\", color=\"lightgray\")\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name, baseline_auc))\n",
    "        \n",
    "    \n",
    "    # Creating the image file name.\n",
    "    curated_str = \"curated_{}\".format(str(c).lower())\n",
    "    question_str = \"question_{}\".format(str(q).lower())\n",
    "    species_str = \"species_{}\".format(str(s).lower())\n",
    "    variables_strs = [curated_str, question_str, species_str]\n",
    "    image_filename = \"{}_{}_{}.png\".format(curated_str, question_str, species_str)\n",
    "    \n",
    "    fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(OUTPUT_DIR,PLOTS_DIR,image_filename),dpi=400)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Create a CSV file for the precision recall curves for each different approach. \n",
    "precision_recall_curves_df = pd.DataFrame(pr_df_rows, columns=[\"name\", \"approach\", \"hyperparameters\", \"group\", \"task\", \"curated\", \"species\", \"precision\", \"recall\", \"basline_auc\"])\n",
    "precision_recall_curves_df[\"name_key\"] = precision_recall_curves_df[\"name\"]\n",
    "precision_recall_curves_df.to_csv(os.path.join(OUTPUT_DIR, METRICS_DIR, \"precision_recall_curves.csv\"), index=False) \n",
    "precision_recall_curves_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"y\"></a>\n",
    "### Are genes in the same group or pathway ranked higher with respect to individual nodes?\n",
    "This is a way of statistically seeing if for some value k, the graph ranks more edges from some particular gene to any other gene that it has a true protein-protein interaction with higher or equal to rank k, than we would expect due to random chance. This way of looking at the problem helps to be less ambiguous than the previous methods, because it gets at the core of how this would actually be used. In other words, we don't really care how much true information we're missing as long as we're still able to pick up some new useful information by building these networks, so even though we could be missing a lot, what's going on at the very top of the results? These results should be comparable to very strictly thresholding the network and saying that the remaining edges are our guesses at interactions. This is comparable to just looking at the far left-hand side of the precision recall curves, but just quantifies it slightly differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SKIPPED = True\n",
    "if not SKIPPED:\n",
    "    \n",
    "    # When the edgelist is generated above, only the lower triangle of the pairwise matrix is retained for edges in the \n",
    "    # graph. This means that in terms of the indices of each node, only the (i,j) node is listed in the edge list where\n",
    "    # i is less than j. This makes sense because the graph that's specified is assumed to already be undirected. However\n",
    "    # in order to be able to easily subset the edgelist by a single column to obtain rows that correspond to all edges\n",
    "    # connected to a particular node, this method will double the number of rows to include both (i,j) and (j,i) edges.\n",
    "    df = make_undirected(df)\n",
    "\n",
    "    # What's the number of functional partners ranked k or higher in terms of phenotypic description similarity for \n",
    "    # each gene? Also figure out the maximum possible number of functional partners that could be theoretically\n",
    "    # recovered in this dataset if recovered means being ranked as k or higher here.\n",
    "    k = 10      # The threshold of interest for gene ranks.\n",
    "    n = 100     # Number of Monte Carlo simulation iterations to complete.\n",
    "    df[list(names)] = df.groupby(\"from\")[list(names)].rank()\n",
    "    ys = df[df[\"shared\"]==1][list(names)].apply(lambda s: len([x for x in s if x<=k]))\n",
    "    ymax = sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: min(len([x for x in s if x==1]),k)))\n",
    "\n",
    "    # Monte Carlo simulation to see what the probability is of achieving each y-value by just randomly pulling k \n",
    "    # edges for each gene rather than taking the top k ones that the similarity methods specifies when ranking.\n",
    "    ysims = [sum(df.groupby(\"from\")[\"shared\"].apply(lambda s: len([x for x in s.sample(k) if x>0.00]))) for i in range(n)]\n",
    "    for name in names:\n",
    "        pvalue = len([ysim for ysim in ysims if ysim>=ys[name]])/float(n)\n",
    "        TABLE[name].update({\"y\":ys[name], \"y_max\":ymax, \"y_ratio\":ys[name]/ymax, \"y_pval\":pvalue})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mean\"></a>\n",
    "### Predicting biochemical pathway or group membership based on mean vectors\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the similarity between the vector representation of the phenotype descriptions for that gene and the average vector for all the vector representations of phenotypes associated with genes that belong to that particular pathway. In calculating the average vector for a given biochemical pathway, the vector corresponding to the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have at least two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the list of methods to look at, and a mapping between each method and the correct similarity metric to apply.\n",
    "# vector_dicts = {k:v.vector_dictionary for k,v in graphs.items()}\n",
    "# names = list(vector_dicts.keys())\n",
    "# group_id_to_ids = groups.get_group_id_to_ids_dict(dataset.get_gene_dictionary())\n",
    "# valid_group_ids = [group for group,id_list in group_id_to_ids.items() if len(id_list)>1]\n",
    "# valid_ids = [i for i in dataset.get_ids() if len(set(valid_group_ids).intersection(set(id_to_group_ids[i])))>0]\n",
    "# pred_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# true_dict = defaultdict(lambda: defaultdict(dict))\n",
    "# for name in names:\n",
    "#     for group in valid_group_ids:\n",
    "#         ids = group_id_to_ids[group]\n",
    "#         for identifier in valid_ids:\n",
    "#             # What's the mean vector of this group, without this particular one that we're trying to classify.\n",
    "#             vectors = np.array([vector_dicts[name][some_id] for some_id in ids if not some_id==identifier])\n",
    "#             mean_vector = vectors.mean(axis=0)\n",
    "#             this_vector = vector_dicts[name][identifier]\n",
    "#             pred_dict[name][identifier][group] = 1-metric_dict[name](mean_vector, this_vector)\n",
    "#             true_dict[name][identifier][group] = (identifier in group_id_to_ids[group])*1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_plots, plots_per_row, row_width, row_height = (len(names), 4, 14, 3)\n",
    "# fig,axs = plt.subplots(math.ceil(num_plots/plots_per_row), plots_per_row, squeeze=False)\n",
    "# for name,ax in zip(names, axs.flatten()):\n",
    "#     \n",
    "#     # Obtaining the values and metrics.\n",
    "#     y_true = pd.DataFrame(true_dict[name]).as_matrix().flatten()\n",
    "#     y_prob = pd.DataFrame(pred_dict[name]).as_matrix().flatten()\n",
    "#     n_pos, n_neg = Counter(y_true)[1], Counter(y_true)[0]\n",
    "#     precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "#     baseline = Counter(y_true)[1]/len(y_true) \n",
    "#     area = auc(recall, precision)\n",
    "#     auc_to_baseline_auc_ratio = area/baseline\n",
    "#     TABLE[name].update({\"mean_auc\":area, \"mean_baseline\":baseline, \"mean_ratio\":auc_to_baseline_auc_ratio})\n",
    "# \n",
    "#     # Producing the precision recall curve.\n",
    "#     step_kwargs = ({'step': 'post'} if 'step' in signature(plt.fill_between).parameters else {})\n",
    "#     ax.step(recall, precision, color='black', alpha=0.2, where='post')\n",
    "#     ax.fill_between(recall, precision, alpha=0.7, color='black', **step_kwargs)\n",
    "#     ax.axhline(baseline, linestyle=\"--\", color=\"lightgray\")\n",
    "#     ax.set_xlabel('Recall')\n",
    "#     ax.set_ylabel('Precision')\n",
    "#     ax.set_ylim([0.0, 1.05])\n",
    "#     ax.set_xlim([0.0, 1.0])\n",
    "#     ax.set_title(\"PR {0} (Baseline={1:0.3f})\".format(name[:10], baseline))\n",
    "#     \n",
    "# fig.set_size_inches(row_width, row_height*math.ceil(num_plots/plots_per_row))\n",
    "# fig.tight_layout()\n",
    "# fig.savefig(os.path.join(OUTPUT_DIR,\"part_6_prcurve_mean_classifier.png\"),dpi=400)\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway membership based on mean similarity values\n",
    "This section looks at how well the biochemical pathways that a particular gene is a member of can be predicted based on the average similarity between the vector representation of the phenotype descriptions for that gene and each of the vector representations for other phenotypes associated with genes that belong to that particular pathway. In calculating the average similarity to other genes from a given biochemical pathway, the gene that is currently being classified is not accounted for, to avoid overestimating the performance by including information about the ground truth during classification. This leads to missing information in the case of biochemical pathways that have only one member. This can be accounted for by only limiting the overall dataset to only include genes that belong to pathways that have at least two genes mapped to them, and only including those pathways, or by removing the missing values before calculating the performance metrics below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting biochemical pathway or group membership with KNN classifier\n",
    "This section looks at how well the group(s) or biochemical pathway(s) that a particular gene belongs to can be predicted based on a KNN classifier generated using every other gene. For this section, only the groups or pathways which contain more than one gene, and the genes mapped to those groups or pathways, are of interest. This is because for other genes, if we consider them then it will be true that that gene belongs to that group in the target vector, but the KNN classifier could never predict this because when that gene is held out, nothing could provide a vote for that group, because there are zero genes available to be members of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"output\"></a>\n",
    "### Summarizing the results for this notebook\n",
    "Write a large table of results to an output file. Columns are generally metrics and rows are generally methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>species</th>\n",
       "      <th>objective</th>\n",
       "      <th>curated</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>name_key</th>\n",
       "      <th>group</th>\n",
       "      <th>order</th>\n",
       "      <th>mean_1</th>\n",
       "      <th>mean_0</th>\n",
       "      <th>n_1</th>\n",
       "      <th>n_0</th>\n",
       "      <th>ks</th>\n",
       "      <th>ks_pval</th>\n",
       "      <th>auc</th>\n",
       "      <th>auc_baseline</th>\n",
       "      <th>f1_max_baseline</th>\n",
       "      <th>f1_max</th>\n",
       "      <th>f5_max</th>\n",
       "      <th>f2_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>43</td>\n",
       "      <td>0.027138</td>\n",
       "      <td>0.021791</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>3.071691e-42</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.214050</td>\n",
       "      <td>0.266987</td>\n",
       "      <td>0.383678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>combined__pubmed</td>\n",
       "      <td>nlp</td>\n",
       "      <td>44</td>\n",
       "      <td>0.034417</td>\n",
       "      <td>0.029150</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>2.472802e-44</td>\n",
       "      <td>0.232249</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.216985</td>\n",
       "      <td>0.337382</td>\n",
       "      <td>0.383686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>plants</td>\n",
       "      <td>combined__plants</td>\n",
       "      <td>nlp</td>\n",
       "      <td>45</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.015126</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.153515</td>\n",
       "      <td>2.446620e-71</td>\n",
       "      <td>0.248417</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.251704</td>\n",
       "      <td>0.355119</td>\n",
       "      <td>0.383686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_wikipedia</td>\n",
       "      <td>combined__tokenization_wikipedia</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1043</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.007671</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.541996</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.561104</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.558750</td>\n",
       "      <td>0.617838</td>\n",
       "      <td>0.589076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_pubmed</td>\n",
       "      <td>combined__tokenization_pubmed</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1044</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.573447</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.568852</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.545013</td>\n",
       "      <td>0.609342</td>\n",
       "      <td>0.612176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_plants</td>\n",
       "      <td>combined__tokenization_plants</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1045</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.004608</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.596620</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.579195</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.559486</td>\n",
       "      <td>0.607507</td>\n",
       "      <td>0.621240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baseline</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>identity</td>\n",
       "      <td>baseline__identity</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0</td>\n",
       "      <td>0.986832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>5.843054e-01</td>\n",
       "      <td>0.561215</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.134673</td>\n",
       "      <td>0.383678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>baseline</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_identity</td>\n",
       "      <td>baseline__tokenization_identity</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.616318</td>\n",
       "      <td>0.978461</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.362143</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.570580</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.492951</td>\n",
       "      <td>0.594543</td>\n",
       "      <td>0.421011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>base_layers_3_concatenated</td>\n",
       "      <td>bert__base_layers_3_concatenated</td>\n",
       "      <td>nlp</td>\n",
       "      <td>23</td>\n",
       "      <td>0.127327</td>\n",
       "      <td>0.177421</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.300795</td>\n",
       "      <td>7.097094e-277</td>\n",
       "      <td>0.238626</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.294041</td>\n",
       "      <td>0.258336</td>\n",
       "      <td>0.446483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_base_layers_4_summed</td>\n",
       "      <td>bert__tokenization_base_layers_4_summed</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1027</td>\n",
       "      <td>0.097800</td>\n",
       "      <td>0.207539</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.501808</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.536112</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.517703</td>\n",
       "      <td>0.598137</td>\n",
       "      <td>0.556112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>biobert</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>pubmed_pmc_layers_2_concatenated</td>\n",
       "      <td>biobert__pubmed_pmc_layers_2_concatenated</td>\n",
       "      <td>nlp</td>\n",
       "      <td>28</td>\n",
       "      <td>0.031524</td>\n",
       "      <td>0.047971</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.288202</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.306377</td>\n",
       "      <td>0.335391</td>\n",
       "      <td>0.466431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>biobert</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_pubmed_pmc_layers_4_summed</td>\n",
       "      <td>biobert__tokenization_pubmed_pmc_layers_4_summed</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1033</td>\n",
       "      <td>0.031025</td>\n",
       "      <td>0.067846</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.518954</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.543807</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.533731</td>\n",
       "      <td>0.595135</td>\n",
       "      <td>0.568398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>pubmed_size_200_mean</td>\n",
       "      <td>word2vec__pubmed_size_200_mean</td>\n",
       "      <td>nlp</td>\n",
       "      <td>16</td>\n",
       "      <td>0.346436</td>\n",
       "      <td>0.529709</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.338611</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.309695</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.316802</td>\n",
       "      <td>0.342218</td>\n",
       "      <td>0.466386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_pubmed_size_200_mean</td>\n",
       "      <td>word2vec__tokenization_pubmed_size_200_mean</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1016</td>\n",
       "      <td>0.186738</td>\n",
       "      <td>0.495524</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.528297</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.559544</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.547448</td>\n",
       "      <td>0.612267</td>\n",
       "      <td>0.574331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>precise_tfidf</td>\n",
       "      <td>noble_coder__precise_tfidf</td>\n",
       "      <td>nlp</td>\n",
       "      <td>36</td>\n",
       "      <td>0.728142</td>\n",
       "      <td>0.872270</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.352608</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.347872</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.365333</td>\n",
       "      <td>0.400674</td>\n",
       "      <td>0.465948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>partial_tfidf</td>\n",
       "      <td>noble_coder__partial_tfidf</td>\n",
       "      <td>nlp</td>\n",
       "      <td>37</td>\n",
       "      <td>0.748837</td>\n",
       "      <td>0.891791</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.413604</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.361004</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.366906</td>\n",
       "      <td>0.392974</td>\n",
       "      <td>0.502574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_precise_tfidf</td>\n",
       "      <td>noble_coder__tokenization_precise_tfidf</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1036</td>\n",
       "      <td>0.430514</td>\n",
       "      <td>0.718796</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.383492</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.412064</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.440150</td>\n",
       "      <td>0.456916</td>\n",
       "      <td>0.461108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>tokenization_partial_tfidf</td>\n",
       "      <td>noble_coder__tokenization_partial_tfidf</td>\n",
       "      <td>nlp</td>\n",
       "      <td>1037</td>\n",
       "      <td>0.361517</td>\n",
       "      <td>0.748520</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.483513</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.481239</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.479947</td>\n",
       "      <td>0.521733</td>\n",
       "      <td>0.543047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>nmf_full_topics_50</td>\n",
       "      <td>topic_modeling__nmf_full_topics_50</td>\n",
       "      <td>nlp</td>\n",
       "      <td>9</td>\n",
       "      <td>0.675492</td>\n",
       "      <td>0.920268</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.516834</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.442182</td>\n",
       "      <td>0.443661</td>\n",
       "      <td>0.565334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>intra</td>\n",
       "      <td>subsets</td>\n",
       "      <td>true</td>\n",
       "      <td>nmf_full_topics_100</td>\n",
       "      <td>topic_modeling__nmf_full_topics_100</td>\n",
       "      <td>nlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.721009</td>\n",
       "      <td>0.932674</td>\n",
       "      <td>3873.0</td>\n",
       "      <td>31107.0</td>\n",
       "      <td>0.475715</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.393846</td>\n",
       "      <td>0.11072</td>\n",
       "      <td>0.199367</td>\n",
       "      <td>0.417537</td>\n",
       "      <td>0.410260</td>\n",
       "      <td>0.539605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            method species objective curated                          hyperparameters                                          name_key group order    mean_1    mean_0     n_1      n_0        ks        ks_pval       auc  auc_baseline  f1_max_baseline    f1_max    f5_max    f2_max\n",
       "0         combined   intra   subsets    true                                wikipedia                               combined__wikipedia   nlp    43  0.027138  0.021791  3873.0  31107.0  0.118000   3.071691e-42  0.208909       0.11072         0.199367  0.214050  0.266987  0.383678\n",
       "1         combined   intra   subsets    true                                   pubmed                                  combined__pubmed   nlp    44  0.034417  0.029150  3873.0  31107.0  0.120910   2.472802e-44  0.232249       0.11072         0.199367  0.216985  0.337382  0.383686\n",
       "2         combined   intra   subsets    true                                   plants                                  combined__plants   nlp    45  0.022046  0.015126  3873.0  31107.0  0.153515   2.446620e-71  0.248417       0.11072         0.199367  0.251704  0.355119  0.383686\n",
       "3         combined   intra   subsets    true                   tokenization_wikipedia                  combined__tokenization_wikipedia   nlp  1043  0.003000  0.007671  3873.0  31107.0  0.541996   0.000000e+00  0.561104       0.11072         0.199367  0.558750  0.617838  0.589076\n",
       "4         combined   intra   subsets    true                      tokenization_pubmed                     combined__tokenization_pubmed   nlp  1044  0.003741  0.009745  3873.0  31107.0  0.573447   0.000000e+00  0.568852       0.11072         0.199367  0.545013  0.609342  0.612176\n",
       "5         combined   intra   subsets    true                      tokenization_plants                     combined__tokenization_plants   nlp  1045  0.001801  0.004608  3873.0  31107.0  0.596620   0.000000e+00  0.579195       0.11072         0.199367  0.559486  0.607507  0.621240\n",
       "6         baseline   intra   subsets    true                                 identity                                baseline__identity   nlp     0  0.986832  1.000000  3873.0  31107.0  0.013168   5.843054e-01  0.561215       0.11072         0.199367  0.199367  0.134673  0.383678\n",
       "7         baseline   intra   subsets    true                    tokenization_identity                   baseline__tokenization_identity   nlp  1000  0.616318  0.978461  3873.0  31107.0  0.362143   0.000000e+00  0.570580       0.11072         0.199367  0.492951  0.594543  0.421011\n",
       "8             bert   intra   subsets    true               base_layers_3_concatenated                  bert__base_layers_3_concatenated   nlp    23  0.127327  0.177421  3873.0  31107.0  0.300795  7.097094e-277  0.238626       0.11072         0.199367  0.294041  0.258336  0.446483\n",
       "9             bert   intra   subsets    true        tokenization_base_layers_4_summed           bert__tokenization_base_layers_4_summed   nlp  1027  0.097800  0.207539  3873.0  31107.0  0.501808   0.000000e+00  0.536112       0.11072         0.199367  0.517703  0.598137  0.556112\n",
       "10         biobert   intra   subsets    true         pubmed_pmc_layers_2_concatenated         biobert__pubmed_pmc_layers_2_concatenated   nlp    28  0.031524  0.047971  3873.0  31107.0  0.336416   0.000000e+00  0.288202       0.11072         0.199367  0.306377  0.335391  0.466431\n",
       "11         biobert   intra   subsets    true  tokenization_pubmed_pmc_layers_4_summed  biobert__tokenization_pubmed_pmc_layers_4_summed   nlp  1033  0.031025  0.067846  3873.0  31107.0  0.518954   0.000000e+00  0.543807       0.11072         0.199367  0.533731  0.595135  0.568398\n",
       "12        word2vec   intra   subsets    true                     pubmed_size_200_mean                    word2vec__pubmed_size_200_mean   nlp    16  0.346436  0.529709  3873.0  31107.0  0.338611   0.000000e+00  0.309695       0.11072         0.199367  0.316802  0.342218  0.466386\n",
       "13        word2vec   intra   subsets    true        tokenization_pubmed_size_200_mean       word2vec__tokenization_pubmed_size_200_mean   nlp  1016  0.186738  0.495524  3873.0  31107.0  0.528297   0.000000e+00  0.559544       0.11072         0.199367  0.547448  0.612267  0.574331\n",
       "14     noble_coder   intra   subsets    true                            precise_tfidf                        noble_coder__precise_tfidf   nlp    36  0.728142  0.872270  3873.0  31107.0  0.352608   0.000000e+00  0.347872       0.11072         0.199367  0.365333  0.400674  0.465948\n",
       "15     noble_coder   intra   subsets    true                            partial_tfidf                        noble_coder__partial_tfidf   nlp    37  0.748837  0.891791  3873.0  31107.0  0.413604   0.000000e+00  0.361004       0.11072         0.199367  0.366906  0.392974  0.502574\n",
       "16     noble_coder   intra   subsets    true               tokenization_precise_tfidf           noble_coder__tokenization_precise_tfidf   nlp  1036  0.430514  0.718796  3873.0  31107.0  0.383492   0.000000e+00  0.412064       0.11072         0.199367  0.440150  0.456916  0.461108\n",
       "17     noble_coder   intra   subsets    true               tokenization_partial_tfidf           noble_coder__tokenization_partial_tfidf   nlp  1037  0.361517  0.748520  3873.0  31107.0  0.483513   0.000000e+00  0.481239       0.11072         0.199367  0.479947  0.521733  0.543047\n",
       "18  topic_modeling   intra   subsets    true                       nmf_full_topics_50                topic_modeling__nmf_full_topics_50   nlp     9  0.675492  0.920268  3873.0  31107.0  0.516834   0.000000e+00  0.424342       0.11072         0.199367  0.442182  0.443661  0.565334\n",
       "19  topic_modeling   intra   subsets    true                      nmf_full_topics_100               topic_modeling__nmf_full_topics_100   nlp    10  0.721009  0.932674  3873.0  31107.0  0.475715   0.000000e+00  0.393846       0.11072         0.199367  0.417537  0.410260  0.539605"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_name_to_method_obj = {method.name_with_hyperparameters:method for method in methods}\n",
    "result_dfs = []\n",
    "for (c,q,s) in variable_combinations:\n",
    "    TABLE = tables[c][q][s]\n",
    "    results = pd.DataFrame(TABLE).transpose()\n",
    "    columns = flatten([\"species\", \"objective\",\"curated\",\"hyperparameters\",\"name_key\",\"group\",\"order\",results.columns])\n",
    "    results[\"hyperparameters\"] = \"\"\n",
    "    results[\"name_key\"] = \"\"\n",
    "    results[\"group\"] = \"\"\n",
    "    results[\"order\"] = \"\"\n",
    "    results[\"species\"] = s.lower()\n",
    "    results[\"objective\"] = q.lower()\n",
    "    results[\"curated\"] = str(c).lower()\n",
    "    results = results[columns]\n",
    "    results.reset_index(inplace=True)\n",
    "    results = results.rename({\"index\":\"method\"}, axis=\"columns\")\n",
    "    results[\"order\"] = results[\"method\"].map(lambda x: method_name_to_method_obj[x].number)\n",
    "    results[\"group\"] = results[\"method\"].map(lambda x: method_name_to_method_obj[x].group)\n",
    "    results[\"hyperparameters\"] = results[\"method\"].map(lambda x: method_name_to_method_obj[x].hyperparameters)\n",
    "    results[\"name_key\"] = results[\"method\"].map(lambda x: method_name_to_method_obj[x].name_with_hyperparameters)\n",
    "    results[\"method\"] = results[\"method\"].map(lambda x: method_name_to_method_obj[x].name)\n",
    "    result_dfs.append(results)\n",
    "\n",
    "results = pd.concat(result_dfs)\n",
    "results.reset_index(inplace=True, drop=True)\n",
    "results.to_csv(os.path.join(OUTPUT_DIR, METRICS_DIR, \"full_table_with_all_metrics.csv\"), index=False)\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>order</th>\n",
       "      <th>group</th>\n",
       "      <th>intra_curated_subsets</th>\n",
       "      <th>intra_curated_known</th>\n",
       "      <th>intra_curated_predicted</th>\n",
       "      <th>intra_curated_pathways</th>\n",
       "      <th>inter_curated_pathways</th>\n",
       "      <th>both_curated_pathways</th>\n",
       "      <th>intra_all_subsets</th>\n",
       "      <th>intra_all_known</th>\n",
       "      <th>intra_all_predicted</th>\n",
       "      <th>intra_all_pathways</th>\n",
       "      <th>inter_all_pathways</th>\n",
       "      <th>both_all_pathways</th>\n",
       "      <th>inter_all_orthologs</th>\n",
       "      <th>name_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>combined</td>\n",
       "      <td>wikipedia</td>\n",
       "      <td>43</td>\n",
       "      <td>nlp</td>\n",
       "      <td>3.071691e-42</td>\n",
       "      <td>2.217024e-39</td>\n",
       "      <td>4.219615e-20</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.015235</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>2.063195e-42</td>\n",
       "      <td>1.540070e-59</td>\n",
       "      <td>4.280187e-28</td>\n",
       "      <td>2.930960e-35</td>\n",
       "      <td>3.031289e-04</td>\n",
       "      <td>1.647536e-37</td>\n",
       "      <td>0.507396</td>\n",
       "      <td>combined__wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>combined</td>\n",
       "      <td>pubmed</td>\n",
       "      <td>44</td>\n",
       "      <td>nlp</td>\n",
       "      <td>2.472802e-44</td>\n",
       "      <td>3.113419e-41</td>\n",
       "      <td>4.276006e-20</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.020322</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>6.160369e-44</td>\n",
       "      <td>2.477582e-64</td>\n",
       "      <td>4.307350e-28</td>\n",
       "      <td>5.390920e-36</td>\n",
       "      <td>1.006435e-04</td>\n",
       "      <td>1.150935e-38</td>\n",
       "      <td>0.526534</td>\n",
       "      <td>combined__pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combined</td>\n",
       "      <td>plants</td>\n",
       "      <td>45</td>\n",
       "      <td>nlp</td>\n",
       "      <td>2.446620e-71</td>\n",
       "      <td>4.316521e-47</td>\n",
       "      <td>4.219615e-20</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.076596</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>4.306702e-71</td>\n",
       "      <td>2.827311e-77</td>\n",
       "      <td>4.280187e-28</td>\n",
       "      <td>1.537895e-39</td>\n",
       "      <td>2.582333e-04</td>\n",
       "      <td>7.259977e-41</td>\n",
       "      <td>0.566396</td>\n",
       "      <td>combined__plants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>combined</td>\n",
       "      <td>tokenization_wikipedia</td>\n",
       "      <td>1043</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.313176e-52</td>\n",
       "      <td>1.348028e-36</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.061679</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.575712e-72</td>\n",
       "      <td>1.142106e-46</td>\n",
       "      <td>1.476695e-47</td>\n",
       "      <td>8.332976e-07</td>\n",
       "      <td>1.583397e-51</td>\n",
       "      <td>0.594047</td>\n",
       "      <td>combined__tokenization_wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>combined</td>\n",
       "      <td>tokenization_pubmed</td>\n",
       "      <td>1044</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.645315e-54</td>\n",
       "      <td>3.411237e-30</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.004557</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.176081e-74</td>\n",
       "      <td>2.306859e-34</td>\n",
       "      <td>2.331866e-46</td>\n",
       "      <td>2.044249e-10</td>\n",
       "      <td>1.007350e-53</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>combined__tokenization_pubmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>combined</td>\n",
       "      <td>tokenization_plants</td>\n",
       "      <td>1045</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.213946e-55</td>\n",
       "      <td>1.367748e-30</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.063781</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.344043e-78</td>\n",
       "      <td>9.854973e-32</td>\n",
       "      <td>6.601566e-53</td>\n",
       "      <td>2.727460e-09</td>\n",
       "      <td>4.507603e-63</td>\n",
       "      <td>0.581672</td>\n",
       "      <td>combined__tokenization_plants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baseline</td>\n",
       "      <td>identity</td>\n",
       "      <td>0</td>\n",
       "      <td>nlp</td>\n",
       "      <td>5.843054e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.025896e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>baseline__identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>baseline</td>\n",
       "      <td>tokenization_identity</td>\n",
       "      <td>1000</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.265296e-23</td>\n",
       "      <td>6.498810e-20</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.437695e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.004228e-20</td>\n",
       "      <td>3.192631e-15</td>\n",
       "      <td>3.835213e-15</td>\n",
       "      <td>9.999987e-01</td>\n",
       "      <td>2.440914e-14</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>baseline__tokenization_identity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bert</td>\n",
       "      <td>base_layers_3_concatenated</td>\n",
       "      <td>23</td>\n",
       "      <td>nlp</td>\n",
       "      <td>7.097094e-277</td>\n",
       "      <td>2.688059e-23</td>\n",
       "      <td>2.030203e-04</td>\n",
       "      <td>4.662937e-15</td>\n",
       "      <td>0.009706</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>7.137668e-281</td>\n",
       "      <td>1.339110e-53</td>\n",
       "      <td>3.226486e-13</td>\n",
       "      <td>1.099770e-21</td>\n",
       "      <td>2.833836e-04</td>\n",
       "      <td>1.958883e-26</td>\n",
       "      <td>0.149824</td>\n",
       "      <td>bert__base_layers_3_concatenated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bert</td>\n",
       "      <td>tokenization_base_layers_4_summed</td>\n",
       "      <td>1027</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.076200e-50</td>\n",
       "      <td>5.365084e-24</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.633794e-93</td>\n",
       "      <td>7.705227e-26</td>\n",
       "      <td>1.633460e-51</td>\n",
       "      <td>5.796573e-08</td>\n",
       "      <td>1.304063e-61</td>\n",
       "      <td>0.926427</td>\n",
       "      <td>bert__tokenization_base_layers_4_summed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>biobert</td>\n",
       "      <td>pubmed_pmc_layers_2_concatenated</td>\n",
       "      <td>28</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.069413e-25</td>\n",
       "      <td>3.419700e-04</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.622916e-56</td>\n",
       "      <td>9.117187e-11</td>\n",
       "      <td>7.024576e-23</td>\n",
       "      <td>4.960189e-04</td>\n",
       "      <td>3.354231e-27</td>\n",
       "      <td>0.305411</td>\n",
       "      <td>biobert__pubmed_pmc_layers_2_concatenated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>biobert</td>\n",
       "      <td>tokenization_pubmed_pmc_layers_4_summed</td>\n",
       "      <td>1033</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.471003e-56</td>\n",
       "      <td>8.500625e-27</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.612367e-100</td>\n",
       "      <td>1.501182e-25</td>\n",
       "      <td>1.253968e-51</td>\n",
       "      <td>6.086538e-09</td>\n",
       "      <td>2.565772e-63</td>\n",
       "      <td>0.611402</td>\n",
       "      <td>biobert__tokenization_pubmed_pmc_layers_4_summed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>pubmed_size_200_mean</td>\n",
       "      <td>16</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>9.346964e-29</td>\n",
       "      <td>5.799676e-02</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.172047</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.596858e-47</td>\n",
       "      <td>1.350495e-04</td>\n",
       "      <td>2.778546e-19</td>\n",
       "      <td>5.335344e-06</td>\n",
       "      <td>6.610643e-23</td>\n",
       "      <td>0.288530</td>\n",
       "      <td>word2vec__pubmed_size_200_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>word2vec</td>\n",
       "      <td>tokenization_pubmed_size_200_mean</td>\n",
       "      <td>1016</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.470708e-58</td>\n",
       "      <td>9.431940e-31</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.056279</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.034910e-89</td>\n",
       "      <td>7.094744e-30</td>\n",
       "      <td>2.262837e-53</td>\n",
       "      <td>8.377017e-06</td>\n",
       "      <td>1.800589e-58</td>\n",
       "      <td>0.565489</td>\n",
       "      <td>word2vec__tokenization_pubmed_size_200_mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>precise_tfidf</td>\n",
       "      <td>36</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.038335e-09</td>\n",
       "      <td>1.996973e-03</td>\n",
       "      <td>5.655237e-08</td>\n",
       "      <td>0.148041</td>\n",
       "      <td>3.568604e-09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.241053e-20</td>\n",
       "      <td>4.163729e-12</td>\n",
       "      <td>4.410970e-04</td>\n",
       "      <td>1.123544e-03</td>\n",
       "      <td>8.781892e-07</td>\n",
       "      <td>0.365145</td>\n",
       "      <td>noble_coder__precise_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>partial_tfidf</td>\n",
       "      <td>37</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.634635e-19</td>\n",
       "      <td>5.037695e-05</td>\n",
       "      <td>1.157963e-13</td>\n",
       "      <td>0.137083</td>\n",
       "      <td>8.437695e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.744578e-45</td>\n",
       "      <td>1.401193e-10</td>\n",
       "      <td>1.147776e-18</td>\n",
       "      <td>1.267651e-03</td>\n",
       "      <td>1.261273e-23</td>\n",
       "      <td>0.495265</td>\n",
       "      <td>noble_coder__partial_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>tokenization_precise_tfidf</td>\n",
       "      <td>1036</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.547997e-39</td>\n",
       "      <td>8.025839e-28</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.769419e-62</td>\n",
       "      <td>1.128746e-24</td>\n",
       "      <td>4.244423e-39</td>\n",
       "      <td>1.126028e-05</td>\n",
       "      <td>5.403471e-44</td>\n",
       "      <td>0.628379</td>\n",
       "      <td>noble_coder__tokenization_precise_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>noble_coder</td>\n",
       "      <td>tokenization_partial_tfidf</td>\n",
       "      <td>1037</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.870125e-45</td>\n",
       "      <td>9.738222e-28</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.356051e-90</td>\n",
       "      <td>3.055846e-30</td>\n",
       "      <td>2.537990e-51</td>\n",
       "      <td>7.355894e-11</td>\n",
       "      <td>1.180290e-62</td>\n",
       "      <td>0.626698</td>\n",
       "      <td>noble_coder__tokenization_partial_tfidf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>nmf_full_topics_50</td>\n",
       "      <td>9</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.039495e-38</td>\n",
       "      <td>5.885438e-15</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.009442</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.770440e-63</td>\n",
       "      <td>2.156932e-15</td>\n",
       "      <td>7.366966e-44</td>\n",
       "      <td>9.721834e-11</td>\n",
       "      <td>2.466238e-57</td>\n",
       "      <td>0.677425</td>\n",
       "      <td>topic_modeling__nmf_full_topics_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>nmf_full_topics_100</td>\n",
       "      <td>10</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.050250e-37</td>\n",
       "      <td>7.285534e-22</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.571394e-63</td>\n",
       "      <td>4.848064e-21</td>\n",
       "      <td>1.065621e-47</td>\n",
       "      <td>9.944523e-11</td>\n",
       "      <td>7.319690e-61</td>\n",
       "      <td>0.783741</td>\n",
       "      <td>topic_modeling__nmf_full_topics_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>tokenization_nmf_full_topics_50</td>\n",
       "      <td>1009</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.393218e-49</td>\n",
       "      <td>2.066784e-27</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.683506e-94</td>\n",
       "      <td>1.036380e-28</td>\n",
       "      <td>7.757654e-67</td>\n",
       "      <td>4.173395e-09</td>\n",
       "      <td>9.654942e-76</td>\n",
       "      <td>0.369735</td>\n",
       "      <td>topic_modeling__tokenization_nmf_full_topics_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>tokenization_nmf_full_topics_100</td>\n",
       "      <td>1010</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.313958e-48</td>\n",
       "      <td>9.794571e-33</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.491295e-84</td>\n",
       "      <td>5.633450e-38</td>\n",
       "      <td>3.321058e-66</td>\n",
       "      <td>3.658692e-10</td>\n",
       "      <td>1.019210e-75</td>\n",
       "      <td>0.444782</td>\n",
       "      <td>topic_modeling__tokenization_nmf_full_topics_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>lda_full_topics_50</td>\n",
       "      <td>7</td>\n",
       "      <td>nlp</td>\n",
       "      <td>7.264515e-310</td>\n",
       "      <td>7.861499e-22</td>\n",
       "      <td>2.402848e-08</td>\n",
       "      <td>2.678748e-08</td>\n",
       "      <td>0.338703</td>\n",
       "      <td>5.858999e-09</td>\n",
       "      <td>2.855699e-321</td>\n",
       "      <td>5.825443e-46</td>\n",
       "      <td>7.576491e-03</td>\n",
       "      <td>3.690161e-12</td>\n",
       "      <td>2.701001e-03</td>\n",
       "      <td>6.517960e-18</td>\n",
       "      <td>0.675074</td>\n",
       "      <td>topic_modeling__lda_full_topics_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>lda_full_topics_100</td>\n",
       "      <td>8</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.723058e-41</td>\n",
       "      <td>4.403486e-06</td>\n",
       "      <td>1.409983e-14</td>\n",
       "      <td>0.300891</td>\n",
       "      <td>8.437695e-15</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.001787e-72</td>\n",
       "      <td>1.555457e-07</td>\n",
       "      <td>5.055922e-28</td>\n",
       "      <td>3.735414e-02</td>\n",
       "      <td>1.563478e-32</td>\n",
       "      <td>0.671138</td>\n",
       "      <td>topic_modeling__lda_full_topics_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>tokenization_lda_full_topics_50</td>\n",
       "      <td>1007</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.188552e-43</td>\n",
       "      <td>5.183887e-27</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.071152</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.153135e-80</td>\n",
       "      <td>2.181252e-38</td>\n",
       "      <td>3.536013e-38</td>\n",
       "      <td>7.000894e-07</td>\n",
       "      <td>2.520290e-42</td>\n",
       "      <td>0.616535</td>\n",
       "      <td>topic_modeling__tokenization_lda_full_topics_50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>topic_modeling</td>\n",
       "      <td>tokenization_lda_full_topics_100</td>\n",
       "      <td>1008</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.959148e-50</td>\n",
       "      <td>7.860625e-33</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.026623</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.270398e-98</td>\n",
       "      <td>3.938234e-35</td>\n",
       "      <td>7.225983e-45</td>\n",
       "      <td>6.914219e-08</td>\n",
       "      <td>4.527760e-49</td>\n",
       "      <td>0.560939</td>\n",
       "      <td>topic_modeling__tokenization_lda_full_topics_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>n_grams</td>\n",
       "      <td>full_plant_overrepresented_tokens_1_grams</td>\n",
       "      <td>4</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.025156e-45</td>\n",
       "      <td>1.458820e-22</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.005775</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.913393e-73</td>\n",
       "      <td>4.187884e-21</td>\n",
       "      <td>9.868489e-50</td>\n",
       "      <td>3.823767e-09</td>\n",
       "      <td>3.775395e-64</td>\n",
       "      <td>0.697815</td>\n",
       "      <td>n_grams__full_plant_overrepresented_tokens_1_g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n_grams</td>\n",
       "      <td>tokenization_full_plant_overrepresented_tokens...</td>\n",
       "      <td>1004</td>\n",
       "      <td>nlp</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.996442e-53</td>\n",
       "      <td>4.001137e-30</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.009138</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.748903e-90</td>\n",
       "      <td>1.444100e-36</td>\n",
       "      <td>2.595236e-53</td>\n",
       "      <td>5.108612e-09</td>\n",
       "      <td>2.142301e-62</td>\n",
       "      <td>0.586113</td>\n",
       "      <td>n_grams__tokenization_full_plant_overrepresent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>go</td>\n",
       "      <td>union</td>\n",
       "      <td>2001</td>\n",
       "      <td>curated</td>\n",
       "      <td>9.227039e-24</td>\n",
       "      <td>1.833493e-57</td>\n",
       "      <td>2.191446e-21</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.564933</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>7.033942e-23</td>\n",
       "      <td>1.367389e-126</td>\n",
       "      <td>6.221490e-59</td>\n",
       "      <td>8.987091e-49</td>\n",
       "      <td>4.146190e-02</td>\n",
       "      <td>1.199989e-51</td>\n",
       "      <td>0.151964</td>\n",
       "      <td>go__union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>po</td>\n",
       "      <td>union</td>\n",
       "      <td>2002</td>\n",
       "      <td>curated</td>\n",
       "      <td>1.866681e-04</td>\n",
       "      <td>2.486458e-06</td>\n",
       "      <td>3.542093e-12</td>\n",
       "      <td>5.705325e-12</td>\n",
       "      <td>0.086189</td>\n",
       "      <td>5.443381e-08</td>\n",
       "      <td>2.490199e-05</td>\n",
       "      <td>2.882980e-10</td>\n",
       "      <td>3.827575e-18</td>\n",
       "      <td>5.608304e-11</td>\n",
       "      <td>3.401737e-01</td>\n",
       "      <td>6.501214e-05</td>\n",
       "      <td>0.836973</td>\n",
       "      <td>po__union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>go</td>\n",
       "      <td>minimum</td>\n",
       "      <td>2003</td>\n",
       "      <td>curated</td>\n",
       "      <td>1.721542e-77</td>\n",
       "      <td>3.753702e-135</td>\n",
       "      <td>1.536014e-67</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.629939</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>1.749502e-78</td>\n",
       "      <td>1.871107e-306</td>\n",
       "      <td>5.321252e-93</td>\n",
       "      <td>2.528107e-107</td>\n",
       "      <td>1.549691e-02</td>\n",
       "      <td>1.387588e-112</td>\n",
       "      <td>0.080742</td>\n",
       "      <td>go__minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>po</td>\n",
       "      <td>minimum</td>\n",
       "      <td>2004</td>\n",
       "      <td>curated</td>\n",
       "      <td>2.956883e-01</td>\n",
       "      <td>9.628084e-01</td>\n",
       "      <td>9.824561e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.101980</td>\n",
       "      <td>9.850687e-01</td>\n",
       "      <td>2.037302e-01</td>\n",
       "      <td>4.158459e-03</td>\n",
       "      <td>2.960259e-02</td>\n",
       "      <td>9.998287e-01</td>\n",
       "      <td>9.800452e-01</td>\n",
       "      <td>7.792412e-04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>po__minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>go_cc</td>\n",
       "      <td>union</td>\n",
       "      <td>5001</td>\n",
       "      <td>curated</td>\n",
       "      <td>7.860054e-01</td>\n",
       "      <td>6.884641e-02</td>\n",
       "      <td>8.433272e-01</td>\n",
       "      <td>2.649773e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.477749e-01</td>\n",
       "      <td>8.504410e-01</td>\n",
       "      <td>4.273616e-01</td>\n",
       "      <td>9.002213e-01</td>\n",
       "      <td>3.754934e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.865823e-01</td>\n",
       "      <td>0.945051</td>\n",
       "      <td>go_cc__union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>go_bp</td>\n",
       "      <td>union</td>\n",
       "      <td>5002</td>\n",
       "      <td>curated</td>\n",
       "      <td>5.944360e-10</td>\n",
       "      <td>1.474245e-04</td>\n",
       "      <td>3.628864e-01</td>\n",
       "      <td>8.407955e-07</td>\n",
       "      <td>0.346947</td>\n",
       "      <td>7.683509e-07</td>\n",
       "      <td>8.984123e-08</td>\n",
       "      <td>8.261486e-06</td>\n",
       "      <td>1.576549e-04</td>\n",
       "      <td>3.444633e-08</td>\n",
       "      <td>4.608682e-01</td>\n",
       "      <td>2.010359e-07</td>\n",
       "      <td>0.945051</td>\n",
       "      <td>go_bp__union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>go_mf</td>\n",
       "      <td>union</td>\n",
       "      <td>5003</td>\n",
       "      <td>curated</td>\n",
       "      <td>6.044919e-01</td>\n",
       "      <td>9.621455e-01</td>\n",
       "      <td>9.999996e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.577341e-01</td>\n",
       "      <td>9.831171e-01</td>\n",
       "      <td>9.471786e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.945051</td>\n",
       "      <td>go_mf__union</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>go_cc</td>\n",
       "      <td>minimum</td>\n",
       "      <td>6001</td>\n",
       "      <td>curated</td>\n",
       "      <td>2.787058e-01</td>\n",
       "      <td>9.862947e-01</td>\n",
       "      <td>1.008342e-04</td>\n",
       "      <td>3.943394e-01</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.853207e-01</td>\n",
       "      <td>2.193002e-01</td>\n",
       "      <td>8.951896e-02</td>\n",
       "      <td>4.947979e-04</td>\n",
       "      <td>3.822481e-01</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.582246e-01</td>\n",
       "      <td>0.945695</td>\n",
       "      <td>go_cc__minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>go_bp</td>\n",
       "      <td>minimum</td>\n",
       "      <td>6002</td>\n",
       "      <td>curated</td>\n",
       "      <td>1.442218e-17</td>\n",
       "      <td>4.285756e-47</td>\n",
       "      <td>5.987907e-07</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>2.583254e-17</td>\n",
       "      <td>4.958183e-80</td>\n",
       "      <td>9.645656e-11</td>\n",
       "      <td>9.578902e-26</td>\n",
       "      <td>9.952852e-01</td>\n",
       "      <td>1.124580e-24</td>\n",
       "      <td>0.945695</td>\n",
       "      <td>go_bp__minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>go_mf</td>\n",
       "      <td>minimum</td>\n",
       "      <td>6003</td>\n",
       "      <td>curated</td>\n",
       "      <td>4.693472e-01</td>\n",
       "      <td>9.959064e-01</td>\n",
       "      <td>2.666921e-01</td>\n",
       "      <td>2.666697e-01</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>3.398829e-01</td>\n",
       "      <td>4.243062e-01</td>\n",
       "      <td>1.630761e-01</td>\n",
       "      <td>8.648712e-02</td>\n",
       "      <td>3.985367e-02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.132373e-02</td>\n",
       "      <td>0.945695</td>\n",
       "      <td>go_mf__minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>go</td>\n",
       "      <td>ic</td>\n",
       "      <td>9001</td>\n",
       "      <td>curated</td>\n",
       "      <td>9.518065e-105</td>\n",
       "      <td>1.544990e-99</td>\n",
       "      <td>4.106318e-111</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.958036</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>1.940728e-103</td>\n",
       "      <td>1.030223e-233</td>\n",
       "      <td>5.434529e-135</td>\n",
       "      <td>7.252334e-91</td>\n",
       "      <td>3.058398e-02</td>\n",
       "      <td>3.064635e-92</td>\n",
       "      <td>0.644155</td>\n",
       "      <td>go__ic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>po</td>\n",
       "      <td>ic</td>\n",
       "      <td>9005</td>\n",
       "      <td>curated</td>\n",
       "      <td>6.570620e-12</td>\n",
       "      <td>7.923836e-13</td>\n",
       "      <td>5.300711e-02</td>\n",
       "      <td>3.268258e-04</td>\n",
       "      <td>0.030643</td>\n",
       "      <td>2.125880e-05</td>\n",
       "      <td>2.897397e-11</td>\n",
       "      <td>2.407962e-28</td>\n",
       "      <td>1.021575e-22</td>\n",
       "      <td>1.159958e-04</td>\n",
       "      <td>8.785199e-01</td>\n",
       "      <td>1.649145e-10</td>\n",
       "      <td>0.906990</td>\n",
       "      <td>po__ic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>eqs</td>\n",
       "      <td>no_hyperparams</td>\n",
       "      <td>2005</td>\n",
       "      <td>curated</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.463720e-31</td>\n",
       "      <td>4.145582e-35</td>\n",
       "      <td>2.997602e-15</td>\n",
       "      <td>0.923920</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.717858e-19</td>\n",
       "      <td>3.258780e-25</td>\n",
       "      <td>2.562795e-03</td>\n",
       "      <td>5.701466e-01</td>\n",
       "      <td>7.823332e-05</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>eqs__no_hyperparams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            method                                    hyperparameters order    group  intra_curated_subsets  intra_curated_known  intra_curated_predicted  intra_curated_pathways  inter_curated_pathways  both_curated_pathways  intra_all_subsets  intra_all_known  intra_all_predicted  intra_all_pathways  inter_all_pathways  both_all_pathways  inter_all_orthologs                                           name_key\n",
       "0         combined                                          wikipedia    43      nlp           3.071691e-42         2.217024e-39             4.219615e-20            2.997602e-15                0.015235           3.330669e-16       2.063195e-42     1.540070e-59         4.280187e-28        2.930960e-35        3.031289e-04       1.647536e-37             0.507396                                combined__wikipedia\n",
       "1         combined                                             pubmed    44      nlp           2.472802e-44         3.113419e-41             4.276006e-20            2.997602e-15                0.020322           3.330669e-16       6.160369e-44     2.477582e-64         4.307350e-28        5.390920e-36        1.006435e-04       1.150935e-38             0.526534                                   combined__pubmed\n",
       "2         combined                                             plants    45      nlp           2.446620e-71         4.316521e-47             4.219615e-20            2.997602e-15                0.076596           3.330669e-16       4.306702e-71     2.827311e-77         4.280187e-28        1.537895e-39        2.582333e-04       7.259977e-41             0.566396                                   combined__plants\n",
       "3         combined                             tokenization_wikipedia  1043      nlp           0.000000e+00         3.313176e-52             1.348028e-36            2.997602e-15                0.061679           3.330669e-16       0.000000e+00     3.575712e-72         1.142106e-46        1.476695e-47        8.332976e-07       1.583397e-51             0.594047                   combined__tokenization_wikipedia\n",
       "4         combined                                tokenization_pubmed  1044      nlp           0.000000e+00         6.645315e-54             3.411237e-30            2.997602e-15                0.004557           3.330669e-16       0.000000e+00     2.176081e-74         2.306859e-34        2.331866e-46        2.044249e-10       1.007350e-53             0.167364                      combined__tokenization_pubmed\n",
       "5         combined                                tokenization_plants  1045      nlp           0.000000e+00         8.213946e-55             1.367748e-30            2.997602e-15                0.063781           3.330669e-16       0.000000e+00     3.344043e-78         9.854973e-32        6.601566e-53        2.727460e-09       4.507603e-63             0.581672                      combined__tokenization_plants\n",
       "6         baseline                                           identity     0      nlp           5.843054e-01         1.000000e+00             1.000000e+00            1.000000e+00                1.000000           1.000000e+00       6.025896e-01     1.000000e+00         1.000000e+00        1.000000e+00        1.000000e+00       1.000000e+00             1.000000                                 baseline__identity\n",
       "7         baseline                              tokenization_identity  1000      nlp           0.000000e+00         6.265296e-23             6.498810e-20            2.997602e-15                1.000000           8.437695e-15       0.000000e+00     2.004228e-20         3.192631e-15        3.835213e-15        9.999987e-01       2.440914e-14             1.000000                    baseline__tokenization_identity\n",
       "8             bert                         base_layers_3_concatenated    23      nlp          7.097094e-277         2.688059e-23             2.030203e-04            4.662937e-15                0.009706           3.330669e-16      7.137668e-281     1.339110e-53         3.226486e-13        1.099770e-21        2.833836e-04       1.958883e-26             0.149824                   bert__base_layers_3_concatenated\n",
       "9             bert                  tokenization_base_layers_4_summed  1027      nlp           0.000000e+00         4.076200e-50             5.365084e-24            2.997602e-15                0.041834           3.330669e-16       0.000000e+00     8.633794e-93         7.705227e-26        1.633460e-51        5.796573e-08       1.304063e-61             0.926427            bert__tokenization_base_layers_4_summed\n",
       "10         biobert                   pubmed_pmc_layers_2_concatenated    28      nlp           0.000000e+00         4.069413e-25             3.419700e-04            2.997602e-15                0.005306           3.330669e-16       0.000000e+00     7.622916e-56         9.117187e-11        7.024576e-23        4.960189e-04       3.354231e-27             0.305411          biobert__pubmed_pmc_layers_2_concatenated\n",
       "11         biobert            tokenization_pubmed_pmc_layers_4_summed  1033      nlp           0.000000e+00         2.471003e-56             8.500625e-27            2.997602e-15                0.000939           3.330669e-16       0.000000e+00    6.612367e-100         1.501182e-25        1.253968e-51        6.086538e-09       2.565772e-63             0.611402   biobert__tokenization_pubmed_pmc_layers_4_summed\n",
       "12        word2vec                               pubmed_size_200_mean    16      nlp           0.000000e+00         9.346964e-29             5.799676e-02            2.997602e-15                0.172047           3.330669e-16       0.000000e+00     5.596858e-47         1.350495e-04        2.778546e-19        5.335344e-06       6.610643e-23             0.288530                     word2vec__pubmed_size_200_mean\n",
       "13        word2vec                  tokenization_pubmed_size_200_mean  1016      nlp           0.000000e+00         1.470708e-58             9.431940e-31            2.997602e-15                0.056279           3.330669e-16       0.000000e+00     1.034910e-89         7.094744e-30        2.262837e-53        8.377017e-06       1.800589e-58             0.565489        word2vec__tokenization_pubmed_size_200_mean\n",
       "14     noble_coder                                      precise_tfidf    36      nlp           0.000000e+00         7.038335e-09             1.996973e-03            5.655237e-08                0.148041           3.568604e-09       0.000000e+00     1.241053e-20         4.163729e-12        4.410970e-04        1.123544e-03       8.781892e-07             0.365145                         noble_coder__precise_tfidf\n",
       "15     noble_coder                                      partial_tfidf    37      nlp           0.000000e+00         1.634635e-19             5.037695e-05            1.157963e-13                0.137083           8.437695e-15       0.000000e+00     2.744578e-45         1.401193e-10        1.147776e-18        1.267651e-03       1.261273e-23             0.495265                         noble_coder__partial_tfidf\n",
       "16     noble_coder                         tokenization_precise_tfidf  1036      nlp           0.000000e+00         1.547997e-39             8.025839e-28            2.997602e-15                0.016726           3.330669e-16       0.000000e+00     7.769419e-62         1.128746e-24        4.244423e-39        1.126028e-05       5.403471e-44             0.628379            noble_coder__tokenization_precise_tfidf\n",
       "17     noble_coder                         tokenization_partial_tfidf  1037      nlp           0.000000e+00         1.870125e-45             9.738222e-28            2.997602e-15                0.000351           3.330669e-16       0.000000e+00     2.356051e-90         3.055846e-30        2.537990e-51        7.355894e-11       1.180290e-62             0.626698            noble_coder__tokenization_partial_tfidf\n",
       "18  topic_modeling                                 nmf_full_topics_50     9      nlp           0.000000e+00         1.039495e-38             5.885438e-15            2.997602e-15                0.009442           3.330669e-16       0.000000e+00     1.770440e-63         2.156932e-15        7.366966e-44        9.721834e-11       2.466238e-57             0.677425                 topic_modeling__nmf_full_topics_50\n",
       "19  topic_modeling                                nmf_full_topics_100    10      nlp           0.000000e+00         3.050250e-37             7.285534e-22            2.997602e-15                0.002953           3.330669e-16       0.000000e+00     1.571394e-63         4.848064e-21        1.065621e-47        9.944523e-11       7.319690e-61             0.783741                topic_modeling__nmf_full_topics_100\n",
       "20  topic_modeling                    tokenization_nmf_full_topics_50  1009      nlp           0.000000e+00         7.393218e-49             2.066784e-27            2.997602e-15                0.001820           3.330669e-16       0.000000e+00     4.683506e-94         1.036380e-28        7.757654e-67        4.173395e-09       9.654942e-76             0.369735    topic_modeling__tokenization_nmf_full_topics_50\n",
       "21  topic_modeling                   tokenization_nmf_full_topics_100  1010      nlp           0.000000e+00         2.313958e-48             9.794571e-33            2.997602e-15                0.000352           3.330669e-16       0.000000e+00     2.491295e-84         5.633450e-38        3.321058e-66        3.658692e-10       1.019210e-75             0.444782   topic_modeling__tokenization_nmf_full_topics_100\n",
       "22  topic_modeling                                 lda_full_topics_50     7      nlp          7.264515e-310         7.861499e-22             2.402848e-08            2.678748e-08                0.338703           5.858999e-09      2.855699e-321     5.825443e-46         7.576491e-03        3.690161e-12        2.701001e-03       6.517960e-18             0.675074                 topic_modeling__lda_full_topics_50\n",
       "23  topic_modeling                                lda_full_topics_100     8      nlp           0.000000e+00         2.723058e-41             4.403486e-06            1.409983e-14                0.300891           8.437695e-15       0.000000e+00     4.001787e-72         1.555457e-07        5.055922e-28        3.735414e-02       1.563478e-32             0.671138                topic_modeling__lda_full_topics_100\n",
       "24  topic_modeling                    tokenization_lda_full_topics_50  1007      nlp           0.000000e+00         2.188552e-43             5.183887e-27            2.997602e-15                0.071152           3.330669e-16       0.000000e+00     1.153135e-80         2.181252e-38        3.536013e-38        7.000894e-07       2.520290e-42             0.616535    topic_modeling__tokenization_lda_full_topics_50\n",
       "25  topic_modeling                   tokenization_lda_full_topics_100  1008      nlp           0.000000e+00         1.959148e-50             7.860625e-33            2.997602e-15                0.026623           3.330669e-16       0.000000e+00     5.270398e-98         3.938234e-35        7.225983e-45        6.914219e-08       4.527760e-49             0.560939   topic_modeling__tokenization_lda_full_topics_100\n",
       "26         n_grams          full_plant_overrepresented_tokens_1_grams     4      nlp           0.000000e+00         2.025156e-45             1.458820e-22            2.997602e-15                0.005775           3.330669e-16       0.000000e+00     1.913393e-73         4.187884e-21        9.868489e-50        3.823767e-09       3.775395e-64             0.697815  n_grams__full_plant_overrepresented_tokens_1_g...\n",
       "27         n_grams  tokenization_full_plant_overrepresented_tokens...  1004      nlp           0.000000e+00         3.996442e-53             4.001137e-30            2.997602e-15                0.009138           3.330669e-16       0.000000e+00     5.748903e-90         1.444100e-36        2.595236e-53        5.108612e-09       2.142301e-62             0.586113  n_grams__tokenization_full_plant_overrepresent...\n",
       "28              go                                              union  2001  curated           9.227039e-24         1.833493e-57             2.191446e-21            2.997602e-15                0.564933           3.330669e-16       7.033942e-23    1.367389e-126         6.221490e-59        8.987091e-49        4.146190e-02       1.199989e-51             0.151964                                          go__union\n",
       "29              po                                              union  2002  curated           1.866681e-04         2.486458e-06             3.542093e-12            5.705325e-12                0.086189           5.443381e-08       2.490199e-05     2.882980e-10         3.827575e-18        5.608304e-11        3.401737e-01       6.501214e-05             0.836973                                          po__union\n",
       "30              go                                            minimum  2003  curated           1.721542e-77        3.753702e-135             1.536014e-67            2.997602e-15                0.629939           3.330669e-16       1.749502e-78    1.871107e-306         5.321252e-93       2.528107e-107        1.549691e-02      1.387588e-112             0.080742                                        go__minimum\n",
       "31              po                                            minimum  2004  curated           2.956883e-01         9.628084e-01             9.824561e-01            1.000000e+00                0.101980           9.850687e-01       2.037302e-01     4.158459e-03         2.960259e-02        9.998287e-01        9.800452e-01       7.792412e-04             1.000000                                        po__minimum\n",
       "32           go_cc                                              union  5001  curated           7.860054e-01         6.884641e-02             8.433272e-01            2.649773e-01                1.000000           3.477749e-01       8.504410e-01     4.273616e-01         9.002213e-01        3.754934e-01        1.000000e+00       4.865823e-01             0.945051                                       go_cc__union\n",
       "33           go_bp                                              union  5002  curated           5.944360e-10         1.474245e-04             3.628864e-01            8.407955e-07                0.346947           7.683509e-07       8.984123e-08     8.261486e-06         1.576549e-04        3.444633e-08        4.608682e-01       2.010359e-07             0.945051                                       go_bp__union\n",
       "34           go_mf                                              union  5003  curated           6.044919e-01         9.621455e-01             9.999996e-01            1.000000e+00                1.000000           1.000000e+00       6.577341e-01     9.831171e-01         9.471786e-01        1.000000e+00        1.000000e+00       1.000000e+00             0.945051                                       go_mf__union\n",
       "35           go_cc                                            minimum  6001  curated           2.787058e-01         9.862947e-01             1.008342e-04            3.943394e-01                1.000000           4.853207e-01       2.193002e-01     8.951896e-02         4.947979e-04        3.822481e-01        1.000000e+00       4.582246e-01             0.945695                                     go_cc__minimum\n",
       "36           go_bp                                            minimum  6002  curated           1.442218e-17         4.285756e-47             5.987907e-07            2.997602e-15                1.000000           3.330669e-16       2.583254e-17     4.958183e-80         9.645656e-11        9.578902e-26        9.952852e-01       1.124580e-24             0.945695                                     go_bp__minimum\n",
       "37           go_mf                                            minimum  6003  curated           4.693472e-01         9.959064e-01             2.666921e-01            2.666697e-01                0.999999           3.398829e-01       4.243062e-01     1.630761e-01         8.648712e-02        3.985367e-02        1.000000e+00       6.132373e-02             0.945695                                     go_mf__minimum\n",
       "38              go                                                 ic  9001  curated          9.518065e-105         1.544990e-99            4.106318e-111            2.997602e-15                0.958036           3.330669e-16      1.940728e-103    1.030223e-233        5.434529e-135        7.252334e-91        3.058398e-02       3.064635e-92             0.644155                                             go__ic\n",
       "39              po                                                 ic  9005  curated           6.570620e-12         7.923836e-13             5.300711e-02            3.268258e-04                0.030643           2.125880e-05       2.897397e-11     2.407962e-28         1.021575e-22        1.159958e-04        8.785199e-01       1.649145e-10             0.906990                                             po__ic\n",
       "40             eqs                                     no_hyperparams  2005  curated           0.000000e+00         3.463720e-31             4.145582e-35            2.997602e-15                0.923920           3.330669e-16       0.000000e+00     4.717858e-19         3.258780e-25        2.562795e-03        5.701466e-01       7.823332e-05             1.000000                                eqs__no_hyperparams"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make another version of the table that is more useful for looking at one particular metric or value.\n",
    "metrics_of_interest = [\"f1_max\", \"auc\", \"f2_max\", \"ks\", \"ks_pval\"]\n",
    "for metric_of_interest in metrics_of_interest:\n",
    "    reshaped_results = results[[\"method\",\"hyperparameters\",\"order\",\"group\"]].drop_duplicates()\n",
    "    for (c,q,s) in variable_combinations:\n",
    "\n",
    "        # Construct a column name that corresponds to a particular subset.\n",
    "        c_str = {True:\"curated\",False:\"all\"}[c]\n",
    "        q_str = str(q).lower()\n",
    "        s_str = str(s).lower()\n",
    "        col_name =  \"{}_{}_{}\".format(s_str, c_str, q_str)\n",
    "\n",
    "        # Pull data out of the the full metrics dataframe.\n",
    "        reshaped_results[col_name] = reshaped_results[\"order\"].map(lambda x: results.loc[(results[\"order\"]==x) & (results[\"curated\"]==str(c).lower()) & (results[\"objective\"]==q.lower()) & (results[\"species\"]==s.lower()), metric_of_interest])\n",
    "        reshaped_results[col_name] = reshaped_results[col_name].map(lambda x: None if len(x)==0 else x.values[0])\n",
    "    \n",
    "    # Remove columns that have all NA values, these are for questions that weren't applicable to these metrics.\n",
    "    reshaped_results.dropna(axis=\"columns\", how=\"all\", inplace=True)\n",
    "    reshaped_results[\"name_key\"] = reshaped_results.apply(lambda x: \"{}__{}\".format(x[\"method\"],x[\"hyperparameters\"]),axis=1)\n",
    "    reshaped_results.to_csv(os.path.join(OUTPUT_DIR, METRICS_DIR, \"{}.csv\").format(metric_of_interest), index=False)\n",
    "reshaped_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "corn",
   "language": "python",
   "name": "corn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
