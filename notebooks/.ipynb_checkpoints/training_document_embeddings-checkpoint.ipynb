{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from scipy.spatial.distance import cosine\n",
    "import warnings\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from itertools import product\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.utils.utils import flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating datasets of sentences to train word embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input paths to text datasets.\n",
    "plant_abstracts_corpus_path = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\"\n",
    "plant_phenotype_descriptions_path = \"../../plant-data/genes_texts_annots.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset that combines the dataset of plant phenotype descriptions and scrapped abstracts.\n",
    "corpus = open(plant_abstracts_corpus_path, 'r').read()\n",
    "sentences_from_corpus = sent_tokenize(corpus)\n",
    "phenotype_descriptions = \" \".join(pd.read_csv(plant_phenotype_descriptions_path)[\"descriptions\"].values)\n",
    "times_to_duplicate_phenotype_dataset = 5\n",
    "sentences_from_descriptions = sent_tokenize(phenotype_descriptions)\n",
    "sentences_from_descriptions_duplicated = list(np.repeat(sentences_from_descriptions, times_to_duplicate_phenotype_dataset))\n",
    "sentences_from_corpus_and_descriptions = sentences_from_corpus+sentences_from_descriptions_duplicated\n",
    "random.shuffle(sentences_from_corpus_and_descriptions)\n",
    "random.shuffle(sentences_from_corpus)\n",
    "random.shuffle(sentences_from_descriptions)\n",
    "sentences_from_corpus_and_descriptions = [preprocess_string(s) for s in sentences_from_corpus_and_descriptions]\n",
    "sentences_from_corpus = [preprocess_string(s) for s in sentences_from_corpus]\n",
    "sentences_from_descriptions = [preprocess_string(s) for s in sentences_from_descriptions]\n",
    "assert len(sentences_from_corpus_and_descriptions) == len(sentences_from_corpus)+(times_to_duplicate_phenotype_dataset*len(sentences_from_descriptions))\n",
    "print(len(sentences_from_corpus_and_descriptions))\n",
    "print(len(sentences_from_corpus))\n",
    "print(len(sentences_from_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training and saving models with hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining grid search parameters for training word embedding models.\n",
    "training_sentence_sets =  [(sentences_from_corpus_and_descriptions,\"both\"), (sentences_from_corpus,\"abstracts\"), (sentences_from_descriptions,\"dataset\")]\n",
    "training_sentence_sets =  [(sentences_from_corpus_and_descriptions,\"both\")]\n",
    "dimensions = [(x,\"dim{}\".format(str(x))) for x in [200, 300]]\n",
    "num_epochs = [(x,\"ep{}\".format(str(x))) for x in [200]]\n",
    "min_alpha = [(0.0001,\"a\")]\n",
    "alpha = [(0.025,\"s\")]\n",
    "min_count = [(x,\"min{}\".format(str(x))) for x in [3,5]]\n",
    "hyperparameter_sets = list(product(\n",
    "    num_epochs,\n",
    "    training_sentence_sets, \n",
    "    dimensions, \n",
    "    min_alpha, \n",
    "    alpha, \n",
    "    min_count))\n",
    "print(len(hyperparameter_sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_document_embedding_model(output_dir, hyperparameters):\n",
    "    \n",
    "    # Producing the path to the output file named according to hyperparameters.\n",
    "    model_filename = \"doc2vec_{}.model\".format(\"_\".join([x[1] for x in hyperparameters]))\n",
    "    output_path = os.path.join(output_dir, model_filename)    \n",
    "    \n",
    "    # Get the hyperparamter values.\n",
    "    epochs, sentences, vector_size, min_a, a, min_count = [x[0] for x in hyperparameters]  \n",
    "    print(epochs, sentences[0], vector_size, min_a, a, min_count)\n",
    "    \n",
    "    # Fitting a vocabulary and training the model.\n",
    "    tagged_sentences = [TaggedDocument(words=s,tags=[i]) for i,s in enumerate(sentences)]\n",
    "    workers = 4\n",
    "    model = gensim.models.Doc2Vec(vector_size=vector_size, min_count=min_count, dm=0, workers=workers, alpha=a, min_alpha=min_a, dbow_words=0)\n",
    "    model.build_vocab(tagged_sentences)\n",
    "    model.train(tagged_sentences, epochs=epochs, total_examples=model.corpus_count)   \n",
    "    \n",
    "    # Saving the model to a file.\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=False, keep_inference=True)\n",
    "    model.save(output_path)\n",
    "    print(\"saving {}\".format(model_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the model creation function iteratively through the hyperparameter grid search.\n",
    "output_models_directory = \"../models/plants_dbow\"\n",
    "for h in hyperparameter_sets:\n",
    "    train_and_save_document_embedding_model(output_models_directory, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluating word embedding models on the validation dataset of related concepts (sentence version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"../models/plants_dbow\"\n",
    "\n",
    "output_path_for_results = \"../models/plants_dbow/{}_validation.csv\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "output_path_for_results_summary = \"../models/plants_dbow/{}_validation_summary.csv\".format(datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'))\n",
    "\n",
    "rows = []\n",
    "     \n",
    "validation_df = pd.read_csv(\"../data/corpus_related_files/closely_related/concepts_multi_word.csv\")\n",
    "concepts_1 = list(validation_df[\"concept_1\"].values)\n",
    "concepts_2 = list(validation_df[\"concept_2\"].values)\n",
    "random.shuffle(concepts_2)\n",
    "validation_df_shuffled = pd.DataFrame({\"concept_1\":concepts_1,\"concept_2\":concepts_2})\n",
    "validation_df[\"class\"] = 1\n",
    "validation_df_shuffled[\"class\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.concat([validation_df,validation_df_shuffled])\n",
    "\n",
    "\n",
    "for path in glob.glob(os.path.join(models_dir,\"*.model\")): \n",
    "    model = gensim.models.Doc2Vec.load(path)\n",
    "    model_name = os.path.basename(path)    \n",
    "    get_similarity = lambda s1,s2: 1-cosine(model.infer_vector(preprocess_string(s1)),model.infer_vector(preprocess_string(s2)))\n",
    "    df[model_name] = df.apply(lambda x: get_similarity(x[\"concept_1\"],x[\"concept_2\"]),axis=1)\n",
    "    \n",
    "    \n",
    "    y_true = list(df[\"class\"].values)\n",
    "    y_prob = list(df[model_name].values)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    f_beta = lambda pr,re,beta: [((1+beta**2)*p*r)/((((beta**2)*p)+r)) for p,r in zip(pr,re)]\n",
    "    f_1_scores = f_beta(precision,recall,beta=1)\n",
    "    f_1_max = np.nanmax(f_1_scores)\n",
    "          \n",
    "    rows.append((model_name, f_1_max))\n",
    "\n",
    "    \n",
    "    \n",
    "          \n",
    "# Constructing and saving the results dataframe.        \n",
    "df.to_csv(output_path_for_results, index=False)\n",
    "header = [\"model\",\"f1_max\"]\n",
    "pd.DataFrame(rows, columns=header).to_csv(output_path_for_results_summary, index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
