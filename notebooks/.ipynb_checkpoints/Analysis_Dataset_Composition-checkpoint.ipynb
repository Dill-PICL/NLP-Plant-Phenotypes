{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Dataset\n",
    "The purpose of this notebook is to look closer at the dataset of genes, natural language descriptions, and ontology term annotations that are used in this work. As included in the preprocessing notebooks, these data are drawn from files from either publications supplements like Oellrich, Walls et al. (2015) or model species databases such as TAIR, MaizeGDB, and SGN. The datasets are already loaded and merged using classes available through the oats package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 4.438685417175293 secs.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string, remove_stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, flatten, to_hms\n",
    "from oats.utils.utils import function_wrapper_with_duration, remove_duplicates_retain_order\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import annotate_using_noble_coder, term_enrichment\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocab_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocab_connected_components, reduce_vocab_linares_pontes, token_enrichment\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the files that are used for this notebook.\n",
    "plant_dataset_path = \"../../plant-data/genes_texts_annots.csv\"\n",
    "\n",
    "# Paths to files with mappings to groups.\n",
    "kegg_pathways_path = \"../../plant-data/reshaped_data/kegg_pathways.csv\" \n",
    "plantcyc_pathways_path = \"../../plant-data/reshaped_data/plantcyc_pathways.csv\" \n",
    "lloyd_meinke_subsets_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets.csv\" \n",
    "lloyd_meinke_classes_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes.csv\" \n",
    "\n",
    "# Paths to files that map group identifers to longer group names.\n",
    "kegg_pathways_names_path = \"../../plant-data/reshaped_data/kegg_pathways_name_map.csv\"\n",
    "plantcyc_pathways_names_path = \"../../plant-data/reshaped_data/plantcyc_pathways_name_map.csv\"\n",
    "lloyd_meinke_subsets_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets_name_map.csv\"\n",
    "lloyd_meinke_classes_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes_name_map.csv\"\n",
    "\n",
    "# Path to file with plant ortholog mappings.\n",
    "ortholog_file_path = \"../../plant-data/databases/panther/PlantGenomeOrthologs_IRB_Modified.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and name an output directory according to when the notebooks was run.\n",
    "OUTPUT_NAME = \"composition\"\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",\"{}_{}_{}\".format(OUTPUT_NAME,datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'),random.randrange(1000,9999)))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6274</td>\n",
       "      <td>3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7907</td>\n",
       "      <td>4841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions\n",
       "0     ath                     6274                 3818\n",
       "1     gmx                       30                   23\n",
       "2     mtr                       37                   36\n",
       "3     osa                       92                   85\n",
       "4     sly                       69                   69\n",
       "5     zma                     1405                  810\n",
       "6   total                     7907                 4841"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in and describing the dataset of plant genes.\n",
    "plant_dataset = Dataset(plant_dataset_path)\n",
    "plant_dataset.filter_has_description()\n",
    "plant_dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's there for each species?\n",
    "The previously loaded dataset contains all of the genes that across six plant species that have natural language description data for phenotype(s) related to that gene. Each gene can have multiple descriptions annotated to it, which were combined or concatenated when the datasets from multiple sources were merged in creating the pickled datasets. Arabidopsis has the highest number of genes that satisfy this criteria, followed by maize, and then followed by the other four species which have a relatively low number of genes that satisfy this criteria, at least given the sources used for this work. Note that the number of unique descriptions is lower than the number of genes in call cases, because multiple genes can have the same phenotype description associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>unique_gene_identifiers</th>\n",
       "      <th>unique_descriptions</th>\n",
       "      <th>total_sents</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_stems</th>\n",
       "      <th>total_lemmas</th>\n",
       "      <th>unique_lemmas</th>\n",
       "      <th>unique_lemmas_to_species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>6274</td>\n",
       "      <td>3818</td>\n",
       "      <td>30121</td>\n",
       "      <td>261441</td>\n",
       "      <td>7331</td>\n",
       "      <td>5305</td>\n",
       "      <td>261441</td>\n",
       "      <td>6792</td>\n",
       "      <td>5084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zma</td>\n",
       "      <td>1405</td>\n",
       "      <td>810</td>\n",
       "      <td>7512</td>\n",
       "      <td>47139</td>\n",
       "      <td>1846</td>\n",
       "      <td>1317</td>\n",
       "      <td>47139</td>\n",
       "      <td>1722</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osa</td>\n",
       "      <td>92</td>\n",
       "      <td>85</td>\n",
       "      <td>478</td>\n",
       "      <td>3689</td>\n",
       "      <td>826</td>\n",
       "      <td>586</td>\n",
       "      <td>3689</td>\n",
       "      <td>760</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sly</td>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>359</td>\n",
       "      <td>1678</td>\n",
       "      <td>577</td>\n",
       "      <td>438</td>\n",
       "      <td>1678</td>\n",
       "      <td>552</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mtr</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>263</td>\n",
       "      <td>2447</td>\n",
       "      <td>718</td>\n",
       "      <td>516</td>\n",
       "      <td>2447</td>\n",
       "      <td>671</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gmx</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>222</td>\n",
       "      <td>81</td>\n",
       "      <td>68</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>total</td>\n",
       "      <td>7907</td>\n",
       "      <td>4841</td>\n",
       "      <td>38795</td>\n",
       "      <td>316616</td>\n",
       "      <td>8277</td>\n",
       "      <td>5984</td>\n",
       "      <td>316616</td>\n",
       "      <td>7663</td>\n",
       "      <td>5913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  species  unique_gene_identifiers  unique_descriptions  total_sents  total_words  unique_words  unique_stems  total_lemmas  unique_lemmas  unique_lemmas_to_species\n",
       "0     ath                     6274                 3818        30121       261441          7331          5305        261441           6792                      5084\n",
       "5     zma                     1405                  810         7512        47139          1846          1317         47139           1722                       498\n",
       "3     osa                       92                   85          478         3689           826           586          3689            760                        97\n",
       "4     sly                       69                   69          359         1678           577           438          1678            552                        99\n",
       "2     mtr                       37                   36          263         2447           718           516          2447            671                       123\n",
       "1     gmx                       30                   23           62          222            81            68           222             78                        12\n",
       "6   total                     7907                 4841        38795       316616          8277          5984        316616           7663                      5913"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = plant_dataset\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatize_doc = lambda d: [wnl.lemmatize(x) for x in simple_preprocess(d)]\n",
    "\n",
    "dists = defaultdict(list)\n",
    "\n",
    "sent_lists = {}\n",
    "token_lists = {}\n",
    "stems_lists = {}\n",
    "lemma_lists = {}\n",
    "\n",
    "\n",
    "# For each individual species.\n",
    "for species in data.get_species():\n",
    "    df = data.to_pandas()\n",
    "    subset = df[df[\"species\"]==species]\n",
    "    sentences = [sent_tokenize(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_stemmed = [preprocess_string(d) for d in subset[\"descriptions\"].values]\n",
    "    descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"descriptions\"].values]\n",
    "    sent_lists[species] = flatten(sentences)\n",
    "    token_lists[species] = flatten(descriptions_not_stemmed)\n",
    "    stems_lists[species] = flatten(descriptions_stemmed)    \n",
    "    lemma_lists[species] = flatten(descriptions_lemmatized)\n",
    "    \n",
    "    # What about the distributions of words per gene and sentences per gene?\n",
    "    dists[\"species\"].extend([species]*subset.shape[0])\n",
    "    dists[\"num_words\"].extend([len(word_tokenize(x)) for x in subset[\"descriptions\"].values])\n",
    "    dists[\"num_sents\"].extend([len(sent_tokenize(x)) for x in subset[\"descriptions\"].values])\n",
    "    \n",
    "# For the entire dataset including all of the species.\n",
    "df = data.to_pandas()\n",
    "subset = df\n",
    "sentences = [sent_tokenize(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_not_stemmed = [simple_preprocess(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_stemmed = [preprocess_string(d) for d in subset[\"descriptions\"].values]\n",
    "descriptions_lemmatized = [lemmatize_doc(d) for d in subset[\"descriptions\"].values]\n",
    "sent_lists[\"total\"] = flatten(sentences)\n",
    "token_lists[\"total\"] = flatten(descriptions_not_stemmed)\n",
    "stems_lists[\"total\"] = flatten(descriptions_stemmed)    \n",
    "lemma_lists[\"total\"] = flatten(descriptions_lemmatized)\n",
    "\n",
    "# What about lemmas that are uniquely used for a particular species?\n",
    "lemma_sets_unique_to_species = {}\n",
    "for species in data.get_species():\n",
    "    other_species = [s for s in data.get_species() if s != species]\n",
    "    lemmas_used_in_other_species = set(flatten([lemma_lists[s] for s in other_species]))\n",
    "    unique_lemmas = set(lemma_lists[species]).difference(lemmas_used_in_other_species)\n",
    "    lemma_sets_unique_to_species[species] = unique_lemmas\n",
    "lemma_sets_unique_to_species[\"total\"] = flatten([list(s) for s in lemma_sets_unique_to_species.values()])\n",
    "\n",
    "    \n",
    "# Create a dataframe to contain the summarizing information about this dataset, and sort it by number of genes.\n",
    "# Unique gene identifiers is just the total number of genes, this column name should be changed in the class...\n",
    "df = data.describe() \n",
    "condition = (df.species==\"total\")\n",
    "excluded = df[condition]\n",
    "included = df[~condition]\n",
    "df_sorted = included.sort_values(by=\"unique_gene_identifiers\", ascending=False)\n",
    "df = pd.concat([df_sorted,excluded])\n",
    "\n",
    "# Add columns summarizing information about the text descriptions in the dataset.\n",
    "df[\"total_sents\"] = df[\"species\"].map(lambda x: len(sent_lists[x]))\n",
    "df[\"total_words\"] = df[\"species\"].map(lambda x: len(token_lists[x]))\n",
    "df[\"unique_words\"] = df[\"species\"].map(lambda x: len(set(token_lists[x])))\n",
    "df[\"unique_stems\"] = df[\"species\"].map(lambda x: len(set(stems_lists[x])))\n",
    "df[\"total_lemmas\"] = df[\"species\"].map(lambda x: len(lemma_lists[x]))\n",
    "df[\"unique_lemmas\"] = df[\"species\"].map(lambda x: len(set(lemma_lists[x])))\n",
    "df[\"unique_lemmas_to_species\"] = df[\"species\"].map(lambda x: len(lemma_sets_unique_to_species[x]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ath</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ath</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ath</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ath</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ath</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ath</td>\n",
       "      <td>167</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ath</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ath</td>\n",
       "      <td>182</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ath</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ath</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ath</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ath</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ath</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ath</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ath</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ath</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ath</td>\n",
       "      <td>101</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ath</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ath</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ath</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  num_words  num_sents\n",
       "0      ath          8          2\n",
       "1      ath         47          6\n",
       "2      ath         75          9\n",
       "3      ath         12          3\n",
       "4      ath         36          3\n",
       "5      ath        167         11\n",
       "6      ath         45          3\n",
       "7      ath        182         21\n",
       "8      ath         62          3\n",
       "9      ath          5          1\n",
       "10     ath         45          6\n",
       "11     ath          5          1\n",
       "12     ath         44          8\n",
       "13     ath         32          4\n",
       "14     ath         72          9\n",
       "15     ath         10          3\n",
       "16     ath        101          8\n",
       "17     ath         57          6\n",
       "18     ath         24          5\n",
       "19     ath          7          1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_distributions = pd.DataFrame(dists)\n",
    "text_distributions.to_csv(os.path.join(OUTPUT_DIR, \"word_sent_distributions.csv\"), index=False)\n",
    "text_distributions.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "species_to_num_to_quantity = {}\n",
    "for species in data.get_species():\n",
    "    how_many_species = lambda token: sum([(token in stems_lists[s]) for s in data.get_species()])\n",
    "    this_vocab = [token for token in stems_lists[species]]\n",
    "    distribution = [how_many_species(token) for token in this_vocab]\n",
    "    species_to_num_to_quantity[species] = dict(Counter(distribution))\n",
    "table = pd.DataFrame(species_to_num_to_quantity).transpose()\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame(species_to_num_to_quantity).transpose().reset_index()\n",
    "table.rename({\"index\":\"species\"}, axis=\"columns\", inplace=True)\n",
    "table.to_csv(os.path.join(OUTPUT_DIR, \"words_shared_by_species.csv\"), index=False)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueness_df = table.melt(id_vars=[\"species\"], var_name=\"others\", value_name=\"quantity\")\n",
    "uniqueness_df[\"others\"] = uniqueness_df[\"others\"]-1\n",
    "uniqueness_df.sort_values(by=\"others\", inplace=True, ascending=False)\n",
    "uniqueness_df.to_csv(os.path.join(OUTPUT_DIR, \"words_shared_by_species_melted.csv\"), index=False)\n",
    "uniqueness_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the ontology term annotations for each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many of the genes in this dataset for each species are mapped to at least one term from a given ontology?\n",
    "num_mapped_go = {}\n",
    "num_mapped_po = {}\n",
    "for species in data.get_species():\n",
    "    d = data.to_pandas()\n",
    "    subset = d[d[\"species\"]==species]    \n",
    "    num_mapped_po[species] = len([t for t in subset[\"annotations\"].values if \"PO\" in t])\n",
    "    num_mapped_go[species] = len([t for t in subset[\"annotations\"].values if \"GO\" in t])\n",
    "num_mapped_go[\"total\"] = sum(list(num_mapped_go.values()))   \n",
    "num_mapped_po[\"total\"] = sum(list(num_mapped_po.values()))\n",
    "df[\"go\"] = df[\"species\"].map(lambda x: num_mapped_go[x])\n",
    "df[\"po\"] = df[\"species\"].map(lambda x: num_mapped_po[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the biologically relevant groups like biochemical pathways and phenotypes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the groupings that we're interested in mapping to? Uses the paths defined at the top of the notebook.\n",
    "groupings_dict = {\n",
    "    \"kegg\":(kegg_pathways_path, kegg_pathways_names_path),\n",
    "    \"plantcyc\":(plantcyc_pathways_path, plantcyc_pathways_names_path),\n",
    "    \"lloyd_meinke\":(lloyd_meinke_subsets_path, lloyd_meinke_subsets_names_path)\n",
    "}\n",
    "\n",
    "\n",
    "for name,(filename,mapfile) in groupings_dict.items():\n",
    "    groups = Groupings(filename, {row.group_id:row.group_name for row in pd.read_csv(mapfile).itertuples()})\n",
    "    id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(data)\n",
    "    group_mapped_ids = [k for (k,v) in id_to_group_ids.items() if len(v)>0]\n",
    "    species_dict = data.get_species_dictionary()\n",
    "    num_mapped = {}\n",
    "    for species in data.get_species():\n",
    "        num_mapped[species] = len([x for x in group_mapped_ids if species_dict[x]==species])\n",
    "    num_mapped[\"total\"] = sum(list(num_mapped.values()))    \n",
    "    df[name] = df[\"species\"].map(lambda x: num_mapped[x])  \n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the other biologically relevant information like orthologous genes and protein interactions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PantherDB for plant orthologs.\n",
    "ortholog_edgelist = AnyInteractions(data.get_name_to_id_dictionary(), ortholog_file_path)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in ortholog_edgelist.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"panther\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRING DB for protein-protein interactions.\n",
    "naming_file = \"../../plant-data/databases/string/all_organisms.name_2_string.tsv\"\n",
    "interaction_files = [\n",
    "    \"../../plant-data/databases/string/3702.protein.links.detailed.v11.0.txt\", # Arabidopsis\n",
    "    \"../../plant-data/databases/string/4577.protein.links.detailed.v11.0.txt\", # Maize\n",
    "    \"../../plant-data/databases/string/4530.protein.links.detailed.v11.0.txt\", # Tomato \n",
    "    \"../../plant-data/databases/string/4081.protein.links.detailed.v11.0.txt\", # Medicago\n",
    "    \"../../plant-data/databases/string/3880.protein.links.detailed.v11.0.txt\", # Rice \n",
    "    \"../../plant-data/databases/string/3847.protein.links.detailed.v11.0.txt\", # Soybean\n",
    "    \"../../plant-data/databases/string/9606.protein.links.detailed.v11.0.txt\", # Human\n",
    "]\n",
    "genes = data.get_gene_dictionary()\n",
    "string_data = ProteinInteractions(genes, naming_file, *interaction_files)\n",
    "species_dict = data.get_species_dictionary()\n",
    "num_mapped = {}\n",
    "for species in data.get_species():\n",
    "    num_mapped[species] = len([x for x in string_data.ids if species_dict[x]==species])\n",
    "num_mapped[\"total\"] = sum(list(num_mapped.values()))\n",
    "df[\"stringdb\"] = df[\"species\"].map(lambda x: num_mapped[x])    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write that dataframe with all the information about dataset to a file.\n",
    "df.to_csv(os.path.join(OUTPUT_DIR,\"full_dataset_composition.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the vocabularies used for different species compare?\n",
    "One of the things we are interested in is discovering or recovering phenotype similarity between different species in order to identify phenologs (phenotypes between species that share some underlying genetic cause). For this reason, we are interested in how the vocabularies used to describe phenotypes between different species vary, because this will impact how feasible it is to use a dataset like this to identify phenologs. Because the Arabidopsis and maize datasets are the largest in this case, we will compare the vocabularies used in describing the phenotypes associated with the genes from these species in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lemmas as the vocabulary components.\n",
    "vocabs = {s:set(lemma_list) for s,lemma_list in lemma_lists.items()}\n",
    "fdist_zma = FreqDist(lemma_lists[\"zma\"])\n",
    "fdist_ath = FreqDist(lemma_lists[\"ath\"])\n",
    "\n",
    "# Using word stems as the vocabulary components.\n",
    "#vocabs = {s:set(stems_list) for s,stems_list in stems_lists.items()}\n",
    "#fdist_zma = FreqDist(stems_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(stems_lists[\"ath\"])\n",
    "\n",
    "# Using tokens (full words) as the vocabulary components.\n",
    "#vocabs = {s:set(token_list) for s,token_list in token_lists.items()}\n",
    "#fdist_zma = FreqDist(token_lists[\"zma\"])\n",
    "#fdist_ath = FreqDist(token_lists[\"ath\"])\n",
    "\n",
    "union_vocab = vocabs[\"zma\"].union(vocabs[\"ath\"])\n",
    "table = pd.DataFrame({\"token\":list(union_vocab)})\n",
    "stops = set(stopwords.words('english'))\n",
    "table = table[~table.token.isin(stops)]\n",
    "table[\"part_of_speech\"] = table[\"token\"].map(lambda x: nltk.pos_tag([x])[0][1][:2])\n",
    "table[\"ath_freq\"] = table[\"token\"].map(lambda x: fdist_ath[x])\n",
    "table[\"ath_rate\"] = table[\"ath_freq\"]*100/len(token_lists[\"ath\"])\n",
    "table[\"zma_freq\"] = table[\"token\"].map(lambda x: fdist_zma[x])\n",
    "table[\"zma_rate\"] = table[\"zma_freq\"]*100/len(token_lists[\"zma\"])\n",
    "table[\"diff\"] = table[\"ath_rate\"]-table[\"zma_rate\"]\n",
    "table.to_csv(os.path.join(OUTPUT_DIR,\"token_frequencies.csv\"), index=False)\n",
    "table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for Arabidopsis than maize descriptions in this dataset?\n",
    "table.sort_values(by=\"diff\", ascending=False, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the tokens more frequently used for maize than Arabidopsis descriptions in this dataset?\n",
    "table.sort_values(by=\"diff\", ascending=True, inplace=True)\n",
    "table.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the mean absolute value of the rate differences between the different parts of speech?\n",
    "table[\"abs_diff\"] = abs(table[\"diff\"])\n",
    "pos_table = table.groupby(\"part_of_speech\").mean()\n",
    "pos_table.sort_values(by=\"abs_diff\", inplace=True, ascending=False)\n",
    "pos_table = pos_table[[\"abs_diff\"]]\n",
    "pos_table.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Working on the Venn Diagram for this part, unused currently.\n",
    "#print(table.shape)\n",
    "#zma_only = table[table[\"ath_rate\"]==0]\n",
    "#ath_only = table[table[\"zma_rate\"]==0]\n",
    "#print(zma_only.shape)\n",
    "#print(ath_only.shape)\n",
    "#print(ath_only.shape[0]+zma_only.shape[0])\n",
    "#ath_only.head(10)\n",
    "# We need to create a mapping between stems and the words that were present for them.\n",
    "# This is because what we want is the stems that are exclusive to a species.\n",
    "# but then the words that are actually there for those stems, so that we can count their parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Term and Word Enrichment for Groups of Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset of phenotype descriptions and ontology annotations.\n",
    "plant_dataset = Dataset(plant_dataset_path)\n",
    "data = plant_dataset\n",
    "data.filter_has_description()\n",
    "#data.filter_has_annotation(\"GO\")\n",
    "data.filter_has_annotation(\"PO\")\n",
    "d = data.get_description_dictionary()\n",
    "texts = {i:\" \".join(simple_preprocess(t)) for i,t in d.items()}\n",
    "len(texts)                              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ontology objects for all the biological ontologies being used.\n",
    "#go_pickle_path = \"../ontologies/go.pickle\"                                                                \n",
    "#po_pickle_path = \"../ontologies/po.pickle\"                                                             \n",
    "#pato_pickle_path = \"../ontologies/pato.pickle\"\n",
    "\n",
    "#pato = load_from_pickle(pato_pickle_path)\n",
    "#po = load_from_pickle(po_pickle_path)\n",
    "#go = load_from_pickle(go_pickle_path)\n",
    "\n",
    "go_obo_path = \"../ontologies/go.obo\"                                                                \n",
    "po_obo_path = \"../ontologies/po.obo\"                                                             \n",
    "pato_obo_path = \"../ontologies/pato.obo\"\n",
    "\n",
    "po = Ontology(po_obo_path)\n",
    "go = Ontology(go_obo_path)\n",
    "pato = Ontology(pato_obo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_go_annotations = data.get_annotations_dictionary(\"GO\")\n",
    "curated_po_annotations = data.get_annotations_dictionary(\"PO\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which GO terms are used to annotate the most genes in this dataset?\n",
    "term_id_to_ids = defaultdict(list)\n",
    "for i,term_id_list in curated_go_annotations.items():\n",
    "    for term_id in term_id_list:\n",
    "        term_id_to_ids[term_id].append(i)\n",
    "term_id_to_num_ids = {k:len(v) for k,v in term_id_to_ids.items()}\n",
    "terms_df = pd.DataFrame(term_id_to_num_ids.items(), columns=[\"term_id\", \"freq\"])\n",
    "\n",
    "def get_term_name(ont,i):\n",
    "    try:\n",
    "        return(ont[i].name)\n",
    "    except:\n",
    "        return(\"\")\n",
    "\n",
    "terms_df[\"term_name\"] = terms_df[\"term_id\"].map(lambda x: get_term_name(go,x))\n",
    "terms_df.sort_values(by=\"freq\", ascending=False, inplace=True)\n",
    "terms_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the group be ones that have that GO term annotation.\n",
    "#go_term_id_of_interest = \"GO:0009640\"\n",
    "#gene_ids_in_this_pathway = [k for k,v in curated_go_annotations.items() if go_term_id_of_interest in v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mappings from this dataset to PlantCyc information.\n",
    "#pmn_pathways_filename = \"../data/pickles/groupings_from_pmn_pathways.pickle\"                        \n",
    "#groups = load_from_pickle(pmn_pathways_filename)\n",
    "#id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(data)\n",
    "\n",
    "\n",
    "# Reading in the dataset of groupings for pathways in PlantCyc.\n",
    "plantcyc_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(plantcyc_pathways_names_path).itertuples()}\n",
    "plantcyc_grouping = Groupings(plantcyc_pathways_path, plantcyc_name_mapping)\n",
    "id_to_group_ids, group_id_to_ids = plantcyc_grouping.get_groupings_for_dataset(data)\n",
    "\n",
    "# Look at which pathways are best represented in this dataset.\n",
    "pathways_sorted = sorted(group_id_to_ids.items(), key=lambda item: len(item[1]), reverse=True)\n",
    "pathways_sorted_lengths = [(i,len(l)) for (i,l) in pathways_sorted]\n",
    "pathways_df = pd.DataFrame(pathways_sorted_lengths, columns=[\"pathway_id\",\"num_genes\"])\n",
    "pathways_df[\"pathway_name\"] = pathways_df[\"pathway_id\"].map(lambda x: plantcyc_grouping.get_long_name(x))\n",
    "pathways_df = pathways_df[[\"pathway_name\",\"pathway_id\",\"num_genes\"]]\n",
    "pathways_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some example pathway to use.\n",
    "#pathway_id = \"PWY-361\"\n",
    "pathway_id = \"PWY-581\"\n",
    "#pathway_id = \"PWY-1121\"\n",
    "pathway_id = \"PWY-695\"\n",
    "gene_ids_in_this_pathway = group_id_to_ids[pathway_id]\n",
    "gene_ids_in_this_pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = term_enrichment(curated_po_annotations, gene_ids_in_this_pathway, po).head(20)\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "results[\"info_content\"] = results[\"term_id\"].map(lambda x: po.ic(x))\n",
    "results.sort_values(by=\"info_content\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "# ns   P > 0.05\n",
    "# *    P ≤ 0.05\n",
    "# **   P ≤ 0.01\n",
    "# ***  P ≤ 0.001\n",
    "# **** P ≤ 0.0001\n",
    "\n",
    "# This lambda won't work is passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results.itertuples():\n",
    "    wordcloud[\"Weight\"].append(int(1/row.p_value_adj))\n",
    "    wordcloud[\"Word\"].append(\"{} ({})\".format(row.term_id,row.term_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = term_enrichment(curated_go_annotations, gene_ids_in_this_pathway, go).head(20)\n",
    "\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "\n",
    "\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "\n",
    "results[\"info_content\"] = results[\"term_id\"].map(lambda x: go.ic(x))\n",
    "results.sort_values(by=\"info_content\", ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "# This lambda won't work is passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = token_enrichment(texts, gene_ids_in_this_pathway).head(20)\n",
    "\n",
    "\n",
    "threshold = 0.05\n",
    "results[\"p_value_adj\"] = multipletests(results[\"p_value\"].values, method='bonferroni')[1]\n",
    "results[\"significant\"] = results[\"p_value_adj\"] < threshold\n",
    "results = results.loc[results[\"significant\"]==True]\n",
    "\n",
    "\n",
    "# This lambda won't work if passed a value greater than the minimum p-value for significance defined here.\n",
    "significance_levels = {0.05:\"*\", 0.01:\"**\", 0.001:\"***\", 0.0001:\"****\"}\n",
    "get_level = lambda x: significance_levels[min([level for level in significance_levels.keys() if x <= level])]\n",
    "results[\"significance\"] = results[\"p_value_adj\"].map(get_level)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in results.itertuples():\n",
    "    wordcloud[\"Weight\"].append(int(1/row.p_value_adj))\n",
    "    wordcloud[\"Word\"].append(row.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(wordcloud).to_csv(os.path.join(OUTPUT_DIR, \"{}_word_cloud.csv\".format(pathway_id)), index=False)\n",
    "pd.DataFrame(wordcloud)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
