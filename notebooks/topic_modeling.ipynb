{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'IndexedGraph' from '_utils' (/Users/cfy/Desktop/reorganizing-irb-scripts/plant-phenotypes-nlp/notebooks/_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-208e22bdd2d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'IndexedGraph' from '_utils' (/Users/cfy/Desktop/reorganizing-irb-scripts/plant-phenotypes-nlp/notebooks/_utils.py)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import sys\n",
    "import gensim\n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import itertools\n",
    "import argparse\n",
    "import shlex\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import signature\n",
    "from scipy.stats import ks_2samp, hypergeom, pearsonr, spearmanr\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, auc\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import spatial, stats\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum, stem_text, preprocess_string\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "sys.path.append(\"../../oats\")\n",
    "from oats.utils.utils import save_to_pickle, load_from_pickle, flatten, to_hms\n",
    "from oats.utils.utils import function_wrapper_with_duration, remove_duplicates_retain_order\n",
    "from oats.biology.dataset import Dataset\n",
    "from oats.biology.groupings import Groupings\n",
    "from oats.biology.relationships import ProteinInteractions, AnyInteractions\n",
    "from oats.annotation.ontology import Ontology\n",
    "from oats.annotation.annotation import annotate_using_noble_coder\n",
    "from oats.distances import pairwise as pw\n",
    "from oats.nlp.vocabulary import get_overrepresented_tokens, get_vocab_from_tokens\n",
    "from oats.nlp.vocabulary import reduce_vocab_connected_components, reduce_vocab_linares_pontes\n",
    "\n",
    "from _utils import Method\n",
    "from _utils import IndexedGraph\n",
    "\n",
    "\n",
    "# Some settings for how data is visualized in the notebook.\n",
    "mpl.rcParams[\"figure.dpi\"] = 400\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and name an output directory according to when the notebooks or script was run.\n",
    "name = \"topic_modeling\"\n",
    "OUTPUT_DIR = os.path.join(\"../outputs\",\"{}_{}_{}\".format(name,datetime.datetime.now().strftime('%m_%d_%Y_h%Hm%Ms%S'),random.randrange(1000,9999)))\n",
    "os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to different datasets containing gene names, text descriptions, and/or ontology term annotations.\n",
    "plant_dataset_path = \"../../plant-data/genes_texts_annots.csv\"\n",
    "clinvar_dataset_path = \"../data/clinvar/clinvar_diseases.csv\"\n",
    "snpedia_snippets_dataset_path = \"../data/snpedia/snpedia_snippets.csv\"\n",
    "snpedia_contexts_dataset_path = \"../data/snpedia/snpedia_contexts.csv\"\n",
    "\n",
    "# Paths to datasets of sentence or description pairs.\n",
    "paired_phenotypes_path = \"../data/paired_sentences/plants/scored.csv\"\n",
    "biosses_datset_path = \"../data/paired_sentences/biosses/cleaned_by_me.csv\"\n",
    "\n",
    "# Paths to files for data about how genes can be grouped into biochemical pathways, etc.\n",
    "kegg_pathways_path = \"../../plant-data/reshaped_data/kegg_pathways.csv\" \n",
    "plantcyc_pathways_path = \"../../plant-data/reshaped_data/plantcyc_pathways.csv\" \n",
    "lloyd_meinke_subsets_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets.csv\" \n",
    "lloyd_meinke_classes_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes.csv\" \n",
    "\n",
    "# Paths files that contain mappings from the identifiers used by those groups to full name strings.\n",
    "kegg_pathways_names_path = \"../../plant-data/reshaped_data/kegg_pathways_name_map.csv\"\n",
    "plantcyc_pathways_names_path = \"../../plant-data/reshaped_data/plantcyc_pathways_name_map.csv\"\n",
    "lloyd_meinke_subsets_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_subsets_name_map.csv\"\n",
    "lloyd_meinke_classes_names_path = \"../../plant-data/reshaped_data/lloyd_meinke_classes_name_map.csv\"\n",
    "\n",
    "# Paths to other files including the ortholog edgelist from Panther, and cleaned files from the two papers.\n",
    "pppn_edgelist_path = \"../../plant-data/papers/oellrich_walls_et_al_2015/supplemental_files/13007_2015_53_MOESM9_ESM.txt\"\n",
    "ortholog_file_path = \"../../plant-data/databases/panther/PlantGenomeOrthologs_IRB_Modified.txt\"\n",
    "lloyd_function_hierarchy_path = \"../../plant-data/papers/lloyd_meinke_2012/versions_cleaned_by_me/192393Table_S1_Final.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pathways to text corpora files that are used in this analysis.\n",
    "background_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/background.txt\"\n",
    "phenotypes_corpus_filename = \"../data/corpus_related_files/untagged_text_corpora/phenotypes_all.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to pretrained or saved models used for embeddings with Word2Vec or Doc2vec.\n",
    "doc2vec_plants_path = \"../models/plants_dbow/doc2vec.model\"\n",
    "doc2vec_wikipedia_path = \"../models/enwiki_dbow/doc2vec.bin\"\n",
    "word2vec_plants_path = \"../models/plants_sg/word2vec.model\"\n",
    "word2vec_wikipedia_path = \"../models/wiki_sg/word2vec.bin\"\n",
    "\n",
    "# Paths to BioBERT models.\n",
    "biobert_pmc_path = \"../models/biobert_v1.0_pmc/pytorch_model\"                                  \n",
    "biobert_pubmed_path = \"../models/biobert_v1.0_pubmed/pytorch_model\"                                 \n",
    "biobert_pubmed_pmc_path = \"../models/biobert_v1.0_pubmed_pmc/pytorch_model\"      \n",
    "\n",
    "# Word2Vec models availalbe pretrained from Pyysalo et al.\n",
    "# http://bio.nlplab.org/#doc-tools\n",
    "# http://evexdb.org/pmresources/vec-space-models/\n",
    "word2vec_bio_pmc_path = \"../models/bio_nlp_lab/PMC-w2v.bin\"\n",
    "word2vec_bio_pubmed_path = \"../models/bio_nlp_lab/PubMed-w2v.bin\"\n",
    "word2vec_bio_pubmed_and_pmc_path = \"../models/bio_nlp_lab/PubMed-and-PMC-w2v.bin\"\n",
    "word2vec_bio_wikipedia_pubmed_and_pmc_path = \"../models/bio_nlp_lab/wikipedia-pubmed-and-PMC-w2v.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"part_7\"></a>\n",
    "# Part 7. Topic Modeling\n",
    "The purpose of this section is to look at different ways that the embeddings obtained for the dataset of phenotype descriptions can be used to cluster or organize the genes to which those phenotypes are mapped into subgroups or representations. These approaches include generating topic models from the data, and doing agglomerative clustering to find clusters to which each gene belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rereading in the datasets used by this section so that it can be run independently of other notebook sections.\n",
    "dataset = Dataset(plant_dataset_path)\n",
    "dataset.filter_has_description()\n",
    "lloyd_meinke_subsets_name_mapping = {row.group_id:row.group_name for row in pd.read_csv(lloyd_meinke_subsets_names_path).itertuples()}\n",
    "groups = Groupings(lloyd_meinke_subsets_path, lloyd_meinke_subsets_name_mapping)\n",
    "id_to_group_ids, group_id_to_ids = groups.get_groupings_for_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"compare_to_subsets\"></a>\n",
    "### Comparing topics learned by topic modeling to existing categorizations\n",
    "Topic modelling learns a set of word probability distributions from the dataset of text descriptions, which represent distinct topics which are present in the dataset. Each text description can then be represented as a discrete probability distribution over the learned topics based on the probability that a given piece of text belongs to each particular topics. This is a form of data reduction because a high dimensionsal bag-of-words can be represented as a vector of *k* probabilities where *k* is the number of topics. The main advantages of topic modelling over clustering is that topic modelling provides soft classifications that can be additionally interpreted, rather than hard classifications into a single cluster. Topic models are also explainable, because the word probability distributions for that topic can be used to determine which words are most representative of any given topic. One problem with topic modelling is that is uses the n-grams embeddings to semantic similarity between different words is not accounted for. To help alleviate this, this section uses implementations of some existing algorithms to compress the vocabulary as a preprocessing step based on word distance matrices generated using word embeddings.\n",
    "\n",
    "Topic models define topics present in a dataset of texts as word or n-gram probability distributions. These models represent each instance of text then as being composed of or generated as as mixture of these topics. The vector for each text that indicates which fraction of that text is generated by a each topic is of length *n* where *n* is the number of topics, and can be used as a reduced dimensionality of the text, with a much smaller vector length than the n-grams embedding itself. Therefore we can build a topic model of the data with 100 topics for example in order to then represent each description in the dataset as a a vector of length 100. This section constructs topic models from the n-gram representations of the dataset and selects different values for the number of topics in order to find a value that works well during the grid search over the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gene IDs are used in this section, so we want to map gene IDs to fully preprocessed descriptions.\n",
    "descriptions = dataset.get_description_dictionary()\n",
    "preprocessed_descriptions = {i:\" \".join(preprocess_string(d)) for i,d in descriptions.items()}\n",
    "texts = list(preprocessed_descriptions.values())\n",
    "\n",
    "# Basic parameters for this problem that are currently used.\n",
    "number_of_topics = 42\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and fitting the topic model, either NFM or LDA or something like that.\n",
    "vectorizer = TfidfVectorizer(max_features=10000, stop_words=\"english\", max_df=0.95, min_df=2, lowercase=True)\n",
    "features = vectorizer.fit_transform(texts)\n",
    "cls = NMF(n_components=number_of_topics, random_state=seed)\n",
    "cls.fit(features)\n",
    "\n",
    "# Function for retrieving the topic vectors for a list of text descriptions.\n",
    "def get_topic_embeddings(texts, model, vectorizer):\n",
    "    ngrams_vectors = vectorizer.transform(texts).toarray()\n",
    "    topic_vectors = model.transform(ngrams_vectors)\n",
    "    return(topic_vectors)\n",
    "\n",
    "# Create the dataframe containing the average score assigned to each topic for the genes from each subset.\n",
    "group_to_topic_vector = {}\n",
    "for group_id,ids in group_id_to_ids.items():\n",
    "    texts = [preprocessed_descriptions[i] for i in ids]\n",
    "    topic_vectors = get_topic_embeddings(texts, cls, vectorizer)\n",
    "    mean_topic_vector = np.mean(topic_vectors, axis=0)\n",
    "    group_to_topic_vector[group_id] = mean_topic_vector\n",
    "    \n",
    "# Turning that matrix of weights into a dataframe so it can be worked with.\n",
    "tm_df = pd.DataFrame(group_to_topic_vector)\n",
    "\n",
    "# Changing the order of the Lloyd, Meinke phenotype subsets to match other figures and tables for consistency.\n",
    "lmtm_df = pd.read_csv(lloyd_function_hierarchy_path)    \n",
    "columns_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "columns_in_order.reverse()\n",
    "assert len(columns_in_order) == number_of_topics\n",
    "tm_df = tm_df[columns_in_order]\n",
    "    \n",
    "# Reordering so consistency with the curated subsets can be checked by looking at the diagonal.\n",
    "tm_df[\"idxmax\"] = tm_df.idxmax(axis = 1)\n",
    "tm_df[\"idxmax\"] = tm_df[\"idxmax\"].apply(lambda x: tm_df.columns.get_loc(x))\n",
    "tm_df = tm_df.sort_values(by=\"idxmax\")\n",
    "tm_df.drop(columns=[\"idxmax\"], inplace=True)\n",
    "\n",
    "# Saving a version of this dataframe this is indexed by topic integers and subset strings, before makings topics a column instead.\n",
    "topic_subset_similarity_df = tm_df\n",
    "tm_df = tm_df.reset_index(drop=False).rename({\"index\":\"topic\"},axis=1).reset_index(drop=False).rename({\"index\":\"order\"},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_subset_similarity_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describing what the most representative tokens for each topic in the model are.\n",
    "num_top_words = 5\n",
    "map_top_words = {}\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for i,topic_vec in enumerate(cls.components_):\n",
    "    top_words = []\n",
    "    print(i,end=\": \")\n",
    "    for fid in topic_vec.argsort()[-1:-num_top_words-1:-1]:\n",
    "        word = feature_names[fid]\n",
    "        # The next line is applicable if words in the topic model are actually a function of the words in the texts.\n",
    "        #word = \" \".join(unreduce[word])\n",
    "        top_words.append(word)\n",
    "        print(word, end=\" \")  \n",
    "    map_top_words[i] = top_words\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column that specifies what the top tokens for each topic are.\n",
    "tm_df[\"tokens\"] = tm_df[\"topic\"].map(lambda x: \"|\".join(map_top_words[x]))\n",
    "\n",
    "# Move that column to the left for readability before writing to the file.\n",
    "tokens_col = tm_df.pop(\"tokens\")\n",
    "tm_df.insert(2, \"tokens\", tokens_col)\n",
    "\n",
    "# Renaming the topics to be in order, to be more helpful when preparing figures that are more intuitive.\n",
    "tm_df[\"topic_renumbered\"] = tm_df[\"order\"].values[::-1]+1\n",
    "topic_renumbered_col = tm_df.pop(\"topic_renumbered\")\n",
    "tm_df.insert(2, \"topic_renumbered\", topic_renumbered_col)\n",
    "\n",
    "# Remembering a mapping between the topics, their order, and what the renumbered names are.\n",
    "topic_order_map = {t:i for t,i in zip(tm_df[\"topic\"].values, tm_df[\"order\"].values)}\n",
    "topic_renumbered_map = {t:i for t,i in zip(tm_df[\"topic\"].values, tm_df[\"topic_renumbered\"].values)}\n",
    "\n",
    "# Saving this version of the subset and topic similarity data to a file.\n",
    "tm_df.to_csv(os.path.join(OUTPUT_DIR, \"topic_subset_matrix.csv\"), index=False)\n",
    "tm_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing a version of the previous table that is useful for producing line drawings representing these results.\n",
    "tm_lines_dict = defaultdict(list)\n",
    "\n",
    "\n",
    "# Remembering the order of the Lloyd, Meinke phenotype subsets to match other figures for consistency.\n",
    "lmtm_df = pd.read_csv(lloyd_function_hierarchy_path)   \n",
    "subset_to_class_map = {s:c for s,c in zip(lmtm_df[\"Subset Symbol\"].values, lmtm_df[\"Class Name\"].values)}\n",
    "subset_to_desc_map = {s:c for s,c in zip(lmtm_df[\"Subset Symbol\"].values, lmtm_df[\"Subset Name and Description \"].values)}\n",
    "subset_abbrevs_in_order = [col for col in lmtm_df[\"Subset Symbol\"].values if col in tm_df.columns]\n",
    "subset_abbrevs_in_order.reverse()\n",
    "subset_order_map = {subset_abbrev:i for i,subset_abbrev in enumerate(subset_abbrevs_in_order)}\n",
    "\n",
    "\n",
    "\n",
    "# Producing the line entries that represent connections between the subsets and topics.\n",
    "line_number = 0\n",
    "topic_int_list = list(topic_subset_similarity_df.columns)\n",
    "subset_str_list = list(topic_subset_similarity_df.index)\n",
    "for subset_abbrev, topic_int in itertools.product(topic_int_list,subset_str_list):\n",
    "    \n",
    "    # The weight of the line, extracted from the similarity matrix between subsets and topics built previously.\n",
    "    weight = topic_subset_similarity_df.loc[topic_int,subset_abbrev]\n",
    "    \n",
    "    # The strings that should be used to represent classes, subsets, and topics in a figure or plot.\n",
    "    subset_str = \"{} ({})\".format(subset_abbrev, subset_to_desc_map[subset_abbrev].lower())\n",
    "    tm_lines_dict[\"subset_str\"].extend([subset_str,subset_str])\n",
    "    tm_lines_dict[\"class_str\"].extend([subset_to_class_map[subset_abbrev],subset_to_class_map[subset_abbrev]])\n",
    "    topic_str = \"Topic {}: ({})\".format(topic_renumbered_map[topic_int], \"|\".join(map_top_words[topic_int]))\n",
    "    tm_lines_dict[\"topic_str\"].extend([topic_str,topic_str])\n",
    "    \n",
    "    # Which line is this, they all have individual numbers so that each line can be its own group in a ggplot object.\n",
    "    tm_lines_dict[\"line_number\"].extend([line_number,line_number])\n",
    "    tm_lines_dict[\"weight\"].extend([weight,weight])\n",
    "    \n",
    "    # Where should the line start and stop? The horizontal values are arbitrary and just have to match.\n",
    "    # The vertical values are determined by which subset and topic are being connected to each other.\n",
    "    tm_lines_dict[\"x\"].extend([0,10])\n",
    "    tm_lines_dict[\"y\"].extend([subset_order_map[subset_abbrev],topic_order_map[topic_int]])\n",
    "    \n",
    "    line_number = line_number+1\n",
    "    \n",
    "tm_lines_df = pd.DataFrame(tm_lines_dict)\n",
    "tm_lines_df.to_csv(os.path.join(OUTPUT_DIR, \"topic_subset_lines.csv\"), index=False)\n",
    "tm_lines_df.head(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
